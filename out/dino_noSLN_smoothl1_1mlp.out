get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fbe3f13eb10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-11 10:43:07.710469] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-11 10:43:07.711957] # Params - G: 43378727
[2023-05-11 10:43:07.711984] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 200000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-11 10:43:07.712004] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-11 10:43:35.067030] [Steps      10] [G 0.0489129]
[2023-05-11 10:43:51.663002] [Steps      20] [G 0.0515434]
[2023-05-11 10:44:08.253911] [Steps      30] [G 0.0355804]
[2023-05-11 10:44:24.983523] [Steps      40] [G 0.0388699]
[2023-05-11 10:44:41.698759] [Steps      50] [G 0.0373737]
[2023-05-11 10:44:58.349629] [Steps      60] [G 0.0402227]
[2023-05-11 10:45:15.063915] [Steps      70] [G 0.0348206]
[2023-05-11 10:45:31.791107] [Steps      80] [G 0.0363956]
[2023-05-11 10:45:48.445353] [Steps      90] [G 0.0287111]
[2023-05-11 10:46:05.091900] [Steps     100] [G 0.0311004]
[2023-05-11 10:46:21.907680] [Steps     110] [G 0.0299055]
[2023-05-11 10:46:38.644478] [Steps     120] [G 0.0357562]
[2023-05-11 10:46:55.399400] [Steps     130] [G 0.0289173]
[2023-05-11 10:47:12.155516] [Steps     140] [G 0.0288538]
[2023-05-11 10:47:28.876928] [Steps     150] [G 0.0312279]
[2023-05-11 10:47:45.608757] [Steps     160] [G 0.0259299]
[2023-05-11 10:48:02.392915] [Steps     170] [G 0.0251010]
[2023-05-11 10:48:19.091154] [Steps     180] [G 0.0278925]
[2023-05-11 10:48:35.702138] [Steps     190] [G 0.0244948]
[2023-05-11 10:48:52.374843] [Steps     200] [G 0.0266226]
[2023-05-11 10:49:09.166501] [Steps     210] [G 0.0263584]
[2023-05-11 10:49:25.843974] [Steps     220] [G 0.0282534]
[2023-05-11 10:49:42.559365] [Steps     230] [G 0.0207537]
[2023-05-11 10:49:59.349881] [Steps     240] [G 0.0323895]
[2023-05-11 10:50:16.044118] [Steps     250] [G 0.0212658]
[2023-05-11 10:50:32.725953] [Steps     260] [G 0.0200223]
[2023-05-11 10:50:49.461228] [Steps     270] [G 0.0217080]
[2023-05-11 10:51:06.197132] [Steps     280] [G 0.0410411]
[2023-05-11 10:51:22.799979] [Steps     290] [G 0.0209391]
[2023-05-11 10:51:39.432608] [Steps     300] [G 0.0226623]
[2023-05-11 10:51:57.972726] [Steps     310] [G 0.0371021]
[2023-05-11 10:52:14.581243] [Steps     320] [G 0.0216899]
[2023-05-11 10:52:31.178109] [Steps     330] [G 0.0221416]
[2023-05-11 10:52:47.879044] [Steps     340] [G 0.0178370]
[2023-05-11 10:53:04.494129] [Steps     350] [G 0.0224675]
[2023-05-11 10:53:21.122424] [Steps     360] [G 0.0213109]
[2023-05-11 10:53:37.745990] [Steps     370] [G 0.0260168]
[2023-05-11 10:53:54.453673] [Steps     380] [G 0.0169475]
[2023-05-11 10:54:11.108719] [Steps     390] [G 0.0230377]
[2023-05-11 10:54:27.742766] [Steps     400] [G 0.0204677]
[2023-05-11 10:54:44.478531] [Steps     410] [G 0.0181096]
[2023-05-11 10:55:01.111415] [Steps     420] [G 0.0192022]
[2023-05-11 10:55:17.759373] [Steps     430] [G 0.0222374]
[2023-05-11 10:55:34.398858] [Steps     440] [G 0.0156450]
[2023-05-11 10:55:51.112201] [Steps     450] [G 0.0237880]
[2023-05-11 10:56:07.728012] [Steps     460] [G 0.0179807]
[2023-05-11 10:56:24.382177] [Steps     470] [G 0.0155983]
[2023-05-11 10:56:41.081186] [Steps     480] [G 0.0201468]
[2023-05-11 10:56:57.701776] [Steps     490] [G 0.0209088]
[2023-05-11 10:57:14.344694] [Steps     500] [G 0.0220885]
[2023-05-11 10:57:31.084863] [Steps     510] [G 0.0191241]
[2023-05-11 10:57:47.759371] [Steps     520] [G 0.0211479]
[2023-05-11 10:58:04.361897] [Steps     530] [G 0.0215369]
[2023-05-11 10:58:21.003373] [Steps     540] [G 0.0169125]
[2023-05-11 10:58:37.801792] [Steps     550] [G 0.0226657]
[2023-05-11 10:58:54.498809] [Steps     560] [G 0.0132604]
[2023-05-11 10:59:11.131932] [Steps     570] [G 0.0152329]
[2023-05-11 10:59:27.827930] [Steps     580] [G 0.0163736]
[2023-05-11 10:59:44.490654] [Steps     590] [G 0.0137435]
[2023-05-11 11:00:01.101777] [Steps     600] [G 0.0198790]
[2023-05-11 11:00:19.767663] [Steps     610] [G 0.0177745]
[2023-05-11 11:00:36.501657] [Steps     620] [G 0.0155945]
[2023-05-11 11:00:53.242450] [Steps     630] [G 0.0248778]
[2023-05-11 11:01:09.945181] [Steps     640] [G 0.0198461]
[2023-05-11 11:01:26.697388] [Steps     650] [G 0.0149668]
[2023-05-11 11:01:43.390724] [Steps     660] [G 0.0176315]
[2023-05-11 11:02:00.069321] [Steps     670] [G 0.0158808]
[2023-05-11 11:02:16.771534] [Steps     680] [G 0.0202510]
[2023-05-11 11:02:33.512805] [Steps     690] [G 0.0146296]
[2023-05-11 11:02:50.216180] [Steps     700] [G 0.0154975]
[2023-05-11 11:03:06.877675] [Steps     710] [G 0.0149485]
[2023-05-11 11:03:23.660424] [Steps     720] [G 0.0126539]
[2023-05-11 11:03:40.376366] [Steps     730] [G 0.0135442]
[2023-05-11 11:03:57.099266] [Steps     740] [G 0.0118164]
[2023-05-11 11:04:13.882782] [Steps     750] [G 0.0307871]
[2023-05-11 11:04:30.566265] [Steps     760] [G 0.0136877]
[2023-05-11 11:04:47.242069] [Steps     770] [G 0.0205332]
[2023-05-11 11:05:03.930545] [Steps     780] [G 0.0129091]
[2023-05-11 11:05:20.724642] [Steps     790] [G 0.0154999]
[2023-05-11 11:05:37.433857] [Steps     800] [G 0.0144902]
[2023-05-11 11:05:54.070858] [Steps     810] [G 0.0123575]
[2023-05-11 11:06:10.814371] [Steps     820] [G 0.0163852]
[2023-05-11 11:06:27.522997] [Steps     830] [G 0.0149509]
[2023-05-11 11:06:44.211195] [Steps     840] [G 0.0188419]
[2023-05-11 11:07:00.901836] [Steps     850] [G 0.0139579]
[2023-05-11 11:07:17.651158] [Steps     860] [G 0.0145571]
[2023-05-11 11:07:34.342755] [Steps     870] [G 0.0147401]
[2023-05-11 11:07:51.047587] [Steps     880] [G 0.0150521]
[2023-05-11 11:08:07.848645] [Steps     890] [G 0.0142984]
[2023-05-11 11:08:24.531892] [Steps     900] [G 0.0139573]
[2023-05-11 11:08:42.958971] [Steps     910] [G 0.0132805]
[2023-05-11 11:08:59.655197] [Steps     920] [G 0.0135539]
[2023-05-11 11:09:16.281637] [Steps     930] [G 0.0142354]
[2023-05-11 11:09:32.920995] [Steps     940] [G 0.0151664]
[2023-05-11 11:09:49.511431] [Steps     950] [G 0.0143007]
[2023-05-11 11:10:06.168055] [Steps     960] [G 0.0129643]
[2023-05-11 11:10:22.831356] [Steps     970] [G 0.0133871]
[2023-05-11 11:10:39.469333] [Steps     980] [G 0.0195953]
[2023-05-11 11:10:56.164288] [Steps     990] [G 0.0138074]
[2023-05-11 11:11:12.800324] [Steps    1000] [G 0.0120162]
Steps 1001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 11:11:30.528544] [Steps    1010] [G 0.0113455]
[2023-05-11 11:11:47.305876] [Steps    1020] [G 0.0146039]
[2023-05-11 11:12:03.913349] [Steps    1030] [G 0.0133820]
[2023-05-11 11:12:20.607976] [Steps    1040] [G 0.0131567]
[2023-05-11 11:12:37.242962] [Steps    1050] [G 0.0135743]
[2023-05-11 11:12:53.888703] [Steps    1060] [G 0.0146442]
[2023-05-11 11:13:10.488866] [Steps    1070] [G 0.0138332]
[2023-05-11 11:13:27.171270] [Steps    1080] [G 0.0140091]
[2023-05-11 11:13:43.845314] [Steps    1090] [G 0.0135886]
[2023-05-11 11:14:00.463435] [Steps    1100] [G 0.0156612]
[2023-05-11 11:14:17.153900] [Steps    1110] [G 0.0142362]
[2023-05-11 11:14:33.764115] [Steps    1120] [G 0.0129087]
[2023-05-11 11:14:50.431303] [Steps    1130] [G 0.0137299]
[2023-05-11 11:15:07.045032] [Steps    1140] [G 0.0147920]
[2023-05-11 11:15:23.752932] [Steps    1150] [G 0.0120676]
[2023-05-11 11:15:40.367657] [Steps    1160] [G 0.0162098]
[2023-05-11 11:15:57.021050] [Steps    1170] [G 0.0162613]
[2023-05-11 11:16:13.730731] [Steps    1180] [G 0.0133500]
[2023-05-11 11:16:30.380738] [Steps    1190] [G 0.0178541]
[2023-05-11 11:16:46.995927] [Steps    1200] [G 0.0144276]
[2023-05-11 11:17:05.808056] [Steps    1210] [G 0.0176620]
[2023-05-11 11:17:22.370471] [Steps    1220] [G 0.0124329]
[2023-05-11 11:17:39.019914] [Steps    1230] [G 0.0157611]
[2023-05-11 11:17:55.598574] [Steps    1240] [G 0.0203569]
[2023-05-11 11:18:12.253881] [Steps    1250] [G 0.0174502]
[2023-05-11 11:18:28.810889] [Steps    1260] [G 0.0135031]
[2023-05-11 11:18:45.383737] [Steps    1270] [G 0.0160358]
[2023-05-11 11:19:02.008842] [Steps    1280] [G 0.0141563]
[2023-05-11 11:19:18.573915] [Steps    1290] [G 0.0132831]
[2023-05-11 11:19:35.134490] [Steps    1300] [G 0.0138663]
[2023-05-11 11:19:51.695137] [Steps    1310] [G 0.0143629]
[2023-05-11 11:20:08.321960] [Steps    1320] [G 0.0123960]
[2023-05-11 11:20:24.880603] [Steps    1330] [G 0.0123390]
[2023-05-11 11:20:41.448780] [Steps    1340] [G 0.0134867]
[2023-05-11 11:20:58.080311] [Steps    1350] [G 0.0113505]
[2023-05-11 11:21:14.658111] [Steps    1360] [G 0.0110505]
[2023-05-11 11:21:31.219656] [Steps    1370] [G 0.0147538]
[2023-05-11 11:21:47.843122] [Steps    1380] [G 0.0118034]
[2023-05-11 11:22:04.415213] [Steps    1390] [G 0.0120951]
[2023-05-11 11:22:20.974181] [Steps    1400] [G 0.0096910]
[2023-05-11 11:22:37.543279] [Steps    1410] [G 0.0129437]
[2023-05-11 11:22:54.168997] [Steps    1420] [G 0.0165493]
[2023-05-11 11:23:10.738911] [Steps    1430] [G 0.0115841]
[2023-05-11 11:23:27.320510] [Steps    1440] [G 0.0118406]
[2023-05-11 11:23:43.943935] [Steps    1450] [G 0.0133211]
[2023-05-11 11:24:00.519089] [Steps    1460] [G 0.0147430]
[2023-05-11 11:24:17.086857] [Steps    1470] [G 0.0125282]
[2023-05-11 11:24:33.657871] [Steps    1480] [G 0.0133693]
[2023-05-11 11:24:50.299036] [Steps    1490] [G 0.0120527]
[2023-05-11 11:25:06.880827] [Steps    1500] [G 0.0092372]
[2023-05-11 11:25:23.442774] [Steps    1510] [G 0.0108026]
[2023-05-11 11:25:42.021796] [Steps    1520] [G 0.0112554]
[2023-05-11 11:25:58.649355] [Steps    1530] [G 0.0132357]
[2023-05-11 11:26:15.292419] [Steps    1540] [G 0.0136202]
[2023-05-11 11:26:32.059318] [Steps    1550] [G 0.0108778]
[2023-05-11 11:26:48.689108] [Steps    1560] [G 0.0106553]
[2023-05-11 11:27:05.362981] [Steps    1570] [G 0.0110304]
[2023-05-11 11:27:22.028989] [Steps    1580] [G 0.0141132]
[2023-05-11 11:27:38.796078] [Steps    1590] [G 0.0097600]
[2023-05-11 11:27:55.467992] [Steps    1600] [G 0.0114728]
[2023-05-11 11:28:12.120619] [Steps    1610] [G 0.0124103]
[2023-05-11 11:28:28.873198] [Steps    1620] [G 0.0179621]
[2023-05-11 11:28:45.492556] [Steps    1630] [G 0.0128130]
[2023-05-11 11:29:02.177832] [Steps    1640] [G 0.0125242]
[2023-05-11 11:29:18.797178] [Steps    1650] [G 0.0131076]
[2023-05-11 11:29:35.522736] [Steps    1660] [G 0.0131220]
[2023-05-11 11:29:52.183934] [Steps    1670] [G 0.0113099]
[2023-05-11 11:30:08.792425] [Steps    1680] [G 0.0117581]
[2023-05-11 11:30:25.518915] [Steps    1690] [G 0.0111396]
[2023-05-11 11:30:42.179896] [Steps    1700] [G 0.0102806]
[2023-05-11 11:30:58.859691] [Steps    1710] [G 0.0131953]
[2023-05-11 11:31:15.572703] [Steps    1720] [G 0.0125176]
[2023-05-11 11:31:32.264150] [Steps    1730] [G 0.0120851]
[2023-05-11 11:31:48.947237] [Steps    1740] [G 0.0134001]
[2023-05-11 11:32:05.599392] [Steps    1750] [G 0.0128649]
[2023-05-11 11:32:22.326160] [Steps    1760] [G 0.0112158]
[2023-05-11 11:32:38.995879] [Steps    1770] [G 0.0134506]
[2023-05-11 11:32:55.660234] [Steps    1780] [G 0.0166429]
[2023-05-11 11:33:12.394586] [Steps    1790] [G 0.0122889]
[2023-05-11 11:33:29.043190] [Steps    1800] [G 0.0122627]
[2023-05-11 11:33:45.686684] [Steps    1810] [G 0.0112228]
[2023-05-11 11:34:04.311196] [Steps    1820] [G 0.0115394]
[2023-05-11 11:34:21.043972] [Steps    1830] [G 0.0106286]
[2023-05-11 11:34:37.735024] [Steps    1840] [G 0.0127563]
[2023-05-11 11:34:54.392916] [Steps    1850] [G 0.0163933]
[2023-05-11 11:35:11.122284] [Steps    1860] [G 0.0145897]
[2023-05-11 11:35:27.799162] [Steps    1870] [G 0.0107391]
[2023-05-11 11:35:44.439211] [Steps    1880] [G 0.0114548]
[2023-05-11 11:36:01.144982] [Steps    1890] [G 0.0122116]
[2023-05-11 11:36:17.906654] [Steps    1900] [G 0.0106008]
[2023-05-11 11:36:34.605698] [Steps    1910] [G 0.0098656]
[2023-05-11 11:36:51.300567] [Steps    1920] [G 0.0108102]
[2023-05-11 11:37:08.130220] [Steps    1930] [G 0.0136474]
[2023-05-11 11:37:24.793848] [Steps    1940] [G 0.0111658]
[2023-05-11 11:37:41.516403] [Steps    1950] [G 0.0101497]
[2023-05-11 11:37:58.241712] [Steps    1960] [G 0.0108507]
[2023-05-11 11:38:14.883893] [Steps    1970] [G 0.0086545]
[2023-05-11 11:38:31.539954] [Steps    1980] [G 0.0109148]
[2023-05-11 11:38:48.178808] [Steps    1990] [G 0.0115022]
[2023-05-11 11:39:04.937808] [Steps    2000] [G 0.0126032]
Steps 2001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 11:39:22.963116] [Steps    2010] [G 0.0127521]
[2023-05-11 11:39:39.783178] [Steps    2020] [G 0.0098681]
[2023-05-11 11:39:56.561002] [Steps    2030] [G 0.0098086]
[2023-05-11 11:40:13.299385] [Steps    2040] [G 0.0126465]
[2023-05-11 11:40:30.076646] [Steps    2050] [G 0.0139533]
[2023-05-11 11:40:46.782050] [Steps    2060] [G 0.0095642]
[2023-05-11 11:41:03.515033] [Steps    2070] [G 0.0154921]
[2023-05-11 11:41:20.249504] [Steps    2080] [G 0.0082994]
[2023-05-11 11:41:36.951434] [Steps    2090] [G 0.0128321]
[2023-05-11 11:41:53.727514] [Steps    2100] [G 0.0105113]
[2023-05-11 11:42:10.368198] [Steps    2110] [G 0.0092120]
[2023-05-11 11:42:28.971413] [Steps    2120] [G 0.0138045]
[2023-05-11 11:42:45.587318] [Steps    2130] [G 0.0165660]
[2023-05-11 11:43:02.254464] [Steps    2140] [G 0.0100929]
[2023-05-11 11:43:19.069018] [Steps    2150] [G 0.0121834]
[2023-05-11 11:43:35.753051] [Steps    2160] [G 0.0092709]
[2023-05-11 11:43:52.435651] [Steps    2170] [G 0.0092326]
[2023-05-11 11:44:09.136556] [Steps    2180] [G 0.0102243]
[2023-05-11 11:44:25.872575] [Steps    2190] [G 0.0112996]
[2023-05-11 11:44:42.575955] [Steps    2200] [G 0.0098396]
[2023-05-11 11:44:59.262477] [Steps    2210] [G 0.0101341]
[2023-05-11 11:45:15.998847] [Steps    2220] [G 0.0093643]
[2023-05-11 11:45:32.671408] [Steps    2230] [G 0.0111453]
[2023-05-11 11:45:49.361650] [Steps    2240] [G 0.0114124]
[2023-05-11 11:46:06.176916] [Steps    2250] [G 0.0114248]
[2023-05-11 11:46:22.850212] [Steps    2260] [G 0.0118361]
[2023-05-11 11:46:39.557588] [Steps    2270] [G 0.0077787]
[2023-05-11 11:46:56.243567] [Steps    2280] [G 0.0105424]
[2023-05-11 11:47:12.984342] [Steps    2290] [G 0.0105878]
[2023-05-11 11:47:29.694000] [Steps    2300] [G 0.0112098]
[2023-05-11 11:47:46.385374] [Steps    2310] [G 0.0099639]
[2023-05-11 11:48:03.166592] [Steps    2320] [G 0.0099216]
[2023-05-11 11:48:19.827702] [Steps    2330] [G 0.0115244]
[2023-05-11 11:48:36.517921] [Steps    2340] [G 0.0085855]
[2023-05-11 11:48:53.203295] [Steps    2350] [G 0.0106388]
[2023-05-11 11:49:09.940854] [Steps    2360] [G 0.0103221]
[2023-05-11 11:49:26.681761] [Steps    2370] [G 0.0115395]
[2023-05-11 11:49:43.385728] [Steps    2380] [G 0.0110920]
[2023-05-11 11:50:00.143961] [Steps    2390] [G 0.0105325]
[2023-05-11 11:50:16.840860] [Steps    2400] [G 0.0088446]
[2023-05-11 11:50:33.517423] [Steps    2410] [G 0.0098614]
[2023-05-11 11:50:52.314487] [Steps    2420] [G 0.0094928]
[2023-05-11 11:51:08.935153] [Steps    2430] [G 0.0106774]
[2023-05-11 11:51:25.596270] [Steps    2440] [G 0.0097386]
[2023-05-11 11:51:42.236648] [Steps    2450] [G 0.0404487]
[2023-05-11 11:51:58.973710] [Steps    2460] [G 0.0100639]
[2023-05-11 11:52:15.654173] [Steps    2470] [G 0.0126561]
[2023-05-11 11:52:32.335426] [Steps    2480] [G 0.0098284]
[2023-05-11 11:52:49.077395] [Steps    2490] [G 0.0102540]
[2023-05-11 11:53:05.755968] [Steps    2500] [G 0.0086516]
[2023-05-11 11:53:22.472830] [Steps    2510] [G 0.0122739]
[2023-05-11 11:53:39.155301] [Steps    2520] [G 0.0082139]
[2023-05-11 11:53:55.911340] [Steps    2530] [G 0.0123430]
[2023-05-11 11:54:12.567344] [Steps    2540] [G 0.0097398]
[2023-05-11 11:54:29.274307] [Steps    2550] [G 0.0103994]
[2023-05-11 11:54:46.042886] [Steps    2560] [G 0.0104908]
[2023-05-11 11:55:02.758287] [Steps    2570] [G 0.0106689]
[2023-05-11 11:55:19.448097] [Steps    2580] [G 0.0101329]
[2023-05-11 11:55:36.161440] [Steps    2590] [G 0.0094089]
[2023-05-11 11:55:52.842756] [Steps    2600] [G 0.0097195]
[2023-05-11 11:56:09.528206] [Steps    2610] [G 0.0075521]
[2023-05-11 11:56:26.201287] [Steps    2620] [G 0.0117265]
[2023-05-11 11:56:42.950888] [Steps    2630] [G 0.0127977]
[2023-05-11 11:56:59.640606] [Steps    2640] [G 0.0093898]
[2023-05-11 11:57:16.348197] [Steps    2650] [G 0.0097035]
[2023-05-11 11:57:33.062407] [Steps    2660] [G 0.0112158]
[2023-05-11 11:57:49.737156] [Steps    2670] [G 0.0105700]
[2023-05-11 11:58:06.407781] [Steps    2680] [G 0.0083189]
[2023-05-11 11:58:23.128019] [Steps    2690] [G 0.0108600]
[2023-05-11 11:58:39.905315] [Steps    2700] [G 0.0087233]
[2023-05-11 11:58:56.588938] [Steps    2710] [G 0.0076090]
[2023-05-11 11:59:15.233184] [Steps    2720] [G 0.0151679]
[2023-05-11 11:59:31.890559] [Steps    2730] [G 0.0132675]
[2023-05-11 11:59:48.484688] [Steps    2740] [G 0.0095934]
[2023-05-11 12:00:05.191802] [Steps    2750] [G 0.0086165]
[2023-05-11 12:00:21.807647] [Steps    2760] [G 0.0098007]
[2023-05-11 12:00:38.477237] [Steps    2770] [G 0.0096250]
[2023-05-11 12:00:55.113188] [Steps    2780] [G 0.0096085]
[2023-05-11 12:01:11.758112] [Steps    2790] [G 0.0086509]
[2023-05-11 12:01:28.515666] [Steps    2800] [G 0.0106978]
[2023-05-11 12:01:45.130933] [Steps    2810] [G 0.0107245]
[2023-05-11 12:02:01.768186] [Steps    2820] [G 0.0103607]
[2023-05-11 12:02:18.501450] [Steps    2830] [G 0.0094598]
[2023-05-11 12:02:35.125318] [Steps    2840] [G 0.0091345]
[2023-05-11 12:02:51.756175] [Steps    2850] [G 0.0075108]
[2023-05-11 12:03:08.414357] [Steps    2860] [G 0.0096751]
[2023-05-11 12:03:25.137772] [Steps    2870] [G 0.0115728]
[2023-05-11 12:03:41.780744] [Steps    2880] [G 0.0087901]
[2023-05-11 12:03:58.406577] [Steps    2890] [G 0.0092573]
[2023-05-11 12:04:15.141622] [Steps    2900] [G 0.0092195]
[2023-05-11 12:04:31.812007] [Steps    2910] [G 0.0090132]
[2023-05-11 12:04:48.448341] [Steps    2920] [G 0.0089714]
[2023-05-11 12:05:05.075617] [Steps    2930] [G 0.0091081]
[2023-05-11 12:05:21.789185] [Steps    2940] [G 0.0109555]
[2023-05-11 12:05:38.439729] [Steps    2950] [G 0.0094804]
[2023-05-11 12:05:55.056935] [Steps    2960] [G 0.0084365]
[2023-05-11 12:06:11.845052] [Steps    2970] [G 0.0100085]
[2023-05-11 12:06:28.483642] [Steps    2980] [G 0.0100596]
[2023-05-11 12:06:45.139568] [Steps    2990] [G 0.0106643]
[2023-05-11 12:07:01.903050] [Steps    3000] [G 0.0101977]
Steps 3001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 12:07:20.005391] [Steps    3010] [G 0.0075970]
[2023-05-11 12:07:36.693849] [Steps    3020] [G 0.0107783]
[2023-05-11 12:07:55.210580] [Steps    3030] [G 0.0109209]
[2023-05-11 12:08:11.794665] [Steps    3040] [G 0.0095278]
[2023-05-11 12:08:28.396275] [Steps    3050] [G 0.0102423]
[2023-05-11 12:08:45.125416] [Steps    3060] [G 0.0083731]
[2023-05-11 12:09:01.774120] [Steps    3070] [G 0.0106375]
[2023-05-11 12:09:18.380524] [Steps    3080] [G 0.0091872]
[2023-05-11 12:09:35.074805] [Steps    3090] [G 0.0064418]
[2023-05-11 12:09:51.682741] [Steps    3100] [G 0.0095366]
[2023-05-11 12:10:08.291423] [Steps    3110] [G 0.0105277]
[2023-05-11 12:10:24.985488] [Steps    3120] [G 0.0094856]
[2023-05-11 12:10:41.608082] [Steps    3130] [G 0.0084809]
[2023-05-11 12:10:58.201922] [Steps    3140] [G 0.0113753]
[2023-05-11 12:11:14.809943] [Steps    3150] [G 0.0074200]
[2023-05-11 12:11:31.540832] [Steps    3160] [G 0.0098906]
[2023-05-11 12:11:48.115611] [Steps    3170] [G 0.0084303]
[2023-05-11 12:12:04.758820] [Steps    3180] [G 0.0086151]
[2023-05-11 12:12:21.473576] [Steps    3190] [G 0.0095183]
[2023-05-11 12:12:38.090287] [Steps    3200] [G 0.0081397]
[2023-05-11 12:12:54.721949] [Steps    3210] [G 0.0085101]
[2023-05-11 12:13:11.324822] [Steps    3220] [G 0.0079755]
[2023-05-11 12:13:28.031094] [Steps    3230] [G 0.0090655]
[2023-05-11 12:13:44.647349] [Steps    3240] [G 0.0097722]
[2023-05-11 12:14:01.237248] [Steps    3250] [G 0.0107025]
[2023-05-11 12:14:17.993138] [Steps    3260] [G 0.0207087]
[2023-05-11 12:14:34.615446] [Steps    3270] [G 0.0085855]
[2023-05-11 12:14:51.244593] [Steps    3280] [G 0.0072590]
[2023-05-11 12:15:07.955091] [Steps    3290] [G 0.0104510]
[2023-05-11 12:15:24.569458] [Steps    3300] [G 0.0082564]
[2023-05-11 12:15:41.176517] [Steps    3310] [G 0.0062069]
[2023-05-11 12:15:57.736561] [Steps    3320] [G 0.0091747]
[2023-05-11 12:16:16.265949] [Steps    3330] [G 0.0099007]
[2023-05-11 12:16:32.979165] [Steps    3340] [G 0.0081079]
[2023-05-11 12:16:49.601998] [Steps    3350] [G 0.0094766]
[2023-05-11 12:17:06.307896] [Steps    3360] [G 0.0082304]
[2023-05-11 12:17:22.939807] [Steps    3370] [G 0.0091195]
[2023-05-11 12:17:39.626279] [Steps    3380] [G 0.0100122]
[2023-05-11 12:17:56.291867] [Steps    3390] [G 0.0084936]
[2023-05-11 12:18:13.057689] [Steps    3400] [G 0.0096046]
[2023-05-11 12:18:29.721739] [Steps    3410] [G 0.0070823]
[2023-05-11 12:18:46.368216] [Steps    3420] [G 0.0067943]
[2023-05-11 12:19:03.107478] [Steps    3430] [G 0.0093543]
[2023-05-11 12:19:19.782388] [Steps    3440] [G 0.0093439]
[2023-05-11 12:19:36.422630] [Steps    3450] [G 0.0073140]
[2023-05-11 12:19:53.131902] [Steps    3460] [G 0.0083443]
[2023-05-11 12:20:09.790595] [Steps    3470] [G 0.0097078]
[2023-05-11 12:20:26.436375] [Steps    3480] [G 0.0081055]
[2023-05-11 12:20:43.082058] [Steps    3490] [G 0.0105921]
[2023-05-11 12:20:59.781250] [Steps    3500] [G 0.0059280]
[2023-05-11 12:21:16.424861] [Steps    3510] [G 0.0066073]
[2023-05-11 12:21:33.065835] [Steps    3520] [G 0.0099700]
[2023-05-11 12:21:49.808855] [Steps    3530] [G 0.0098453]
[2023-05-11 12:22:06.441196] [Steps    3540] [G 0.0076152]
[2023-05-11 12:22:23.096821] [Steps    3550] [G 0.0087741]
[2023-05-11 12:22:39.767644] [Steps    3560] [G 0.0089918]
[2023-05-11 12:22:56.472164] [Steps    3570] [G 0.0109207]
[2023-05-11 12:23:13.122518] [Steps    3580] [G 0.0076894]
[2023-05-11 12:23:29.765834] [Steps    3590] [G 0.0079618]
[2023-05-11 12:23:46.517460] [Steps    3600] [G 0.0080730]
[2023-05-11 12:24:03.177488] [Steps    3610] [G 0.0098259]
[2023-05-11 12:24:19.799758] [Steps    3620] [G 0.0079243]
[2023-05-11 12:24:38.470960] [Steps    3630] [G 0.0081399]
[2023-05-11 12:24:55.133020] [Steps    3640] [G 0.0080034]
[2023-05-11 12:25:11.820698] [Steps    3650] [G 0.0059778]
[2023-05-11 12:25:28.442060] [Steps    3660] [G 0.0109681]
[2023-05-11 12:25:45.159281] [Steps    3670] [G 0.0117914]
[2023-05-11 12:26:01.779053] [Steps    3680] [G 0.0093638]
[2023-05-11 12:26:18.454176] [Steps    3690] [G 0.0070749]
[2023-05-11 12:26:35.198044] [Steps    3700] [G 0.0088216]
[2023-05-11 12:26:51.909225] [Steps    3710] [G 0.0071672]
[2023-05-11 12:27:08.603303] [Steps    3720] [G 0.0079512]
[2023-05-11 12:27:25.284615] [Steps    3730] [G 0.0105116]
[2023-05-11 12:27:42.043595] [Steps    3740] [G 0.0076411]
[2023-05-11 12:27:58.721804] [Steps    3750] [G 0.0095914]
[2023-05-11 12:28:15.431233] [Steps    3760] [G 0.0065951]
[2023-05-11 12:28:32.162309] [Steps    3770] [G 0.0094290]
[2023-05-11 12:28:48.840216] [Steps    3780] [G 0.0087901]
[2023-05-11 12:29:05.522936] [Steps    3790] [G 0.0076903]
[2023-05-11 12:29:22.257126] [Steps    3800] [G 0.0098545]
[2023-05-11 12:29:38.927529] [Steps    3810] [G 0.0111180]
[2023-05-11 12:29:55.622590] [Steps    3820] [G 0.0075170]
[2023-05-11 12:30:12.320157] [Steps    3830] [G 0.0086504]
[2023-05-11 12:30:29.081303] [Steps    3840] [G 0.0077475]
[2023-05-11 12:30:45.780830] [Steps    3850] [G 0.0067141]
[2023-05-11 12:31:02.480905] [Steps    3860] [G 0.0081338]
[2023-05-11 12:31:19.234143] [Steps    3870] [G 0.0090731]
[2023-05-11 12:31:35.930079] [Steps    3880] [G 0.0082060]
[2023-05-11 12:31:52.618337] [Steps    3890] [G 0.0098707]
[2023-05-11 12:32:09.301417] [Steps    3900] [G 0.0090937]
[2023-05-11 12:32:26.093902] [Steps    3910] [G 0.0081510]
[2023-05-11 12:32:42.762042] [Steps    3920] [G 0.0102122]
[2023-05-11 12:33:01.412027] [Steps    3930] [G 0.0099751]
[2023-05-11 12:33:18.091929] [Steps    3940] [G 0.0086968]
[2023-05-11 12:33:34.716607] [Steps    3950] [G 0.0073891]
[2023-05-11 12:33:51.373936] [Steps    3960] [G 0.0079516]
[2023-05-11 12:34:07.991304] [Steps    3970] [G 0.0069983]
[2023-05-11 12:34:24.699203] [Steps    3980] [G 0.0110643]
[2023-05-11 12:34:41.320736] [Steps    3990] [G 0.0079676]
[2023-05-11 12:34:57.964957] [Steps    4000] [G 0.0095942]
Steps 4001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 12:35:15.974975] [Steps    4010] [G 0.0073541]
[2023-05-11 12:35:32.645125] [Steps    4020] [G 0.0109630]
[2023-05-11 12:35:49.391226] [Steps    4030] [G 0.0068429]
[2023-05-11 12:36:06.023634] [Steps    4040] [G 0.0074170]
[2023-05-11 12:36:22.688113] [Steps    4050] [G 0.0092286]
[2023-05-11 12:36:39.494586] [Steps    4060] [G 0.0101948]
[2023-05-11 12:36:56.175318] [Steps    4070] [G 0.0048292]
[2023-05-11 12:37:12.815861] [Steps    4080] [G 0.0081087]
[2023-05-11 12:37:29.575863] [Steps    4090] [G 0.0101954]
[2023-05-11 12:37:46.294359] [Steps    4100] [G 0.0083771]
[2023-05-11 12:38:02.945248] [Steps    4110] [G 0.0095055]
[2023-05-11 12:38:19.614922] [Steps    4120] [G 0.0092786]
[2023-05-11 12:38:36.322419] [Steps    4130] [G 0.0070795]
[2023-05-11 12:38:52.992804] [Steps    4140] [G 0.0070529]
[2023-05-11 12:39:09.658428] [Steps    4150] [G 0.0083212]
[2023-05-11 12:39:26.394325] [Steps    4160] [G 0.0084007]
[2023-05-11 12:39:43.058366] [Steps    4170] [G 0.0082724]
[2023-05-11 12:39:59.680499] [Steps    4180] [G 0.0081677]
[2023-05-11 12:40:16.359874] [Steps    4190] [G 0.0097320]
[2023-05-11 12:40:33.060039] [Steps    4200] [G 0.0091619]
[2023-05-11 12:40:49.739049] [Steps    4210] [G 0.0085480]
[2023-05-11 12:41:06.382875] [Steps    4220] [G 0.0085156]
[2023-05-11 12:41:25.022517] [Steps    4230] [G 0.0090978]
[2023-05-11 12:41:41.618782] [Steps    4240] [G 0.0074563]
[2023-05-11 12:41:58.307974] [Steps    4250] [G 0.0106340]
[2023-05-11 12:42:14.913126] [Steps    4260] [G 0.0085453]
[2023-05-11 12:42:31.597355] [Steps    4270] [G 0.0055977]
[2023-05-11 12:42:48.217908] [Steps    4280] [G 0.0077207]
[2023-05-11 12:43:04.825973] [Steps    4290] [G 0.0082069]
[2023-05-11 12:43:21.510795] [Steps    4300] [G 0.0094668]
[2023-05-11 12:43:38.126139] [Steps    4310] [G 0.0062476]
[2023-05-11 12:43:54.770110] [Steps    4320] [G 0.0085610]
[2023-05-11 12:44:11.460507] [Steps    4330] [G 0.0082918]
[2023-05-11 12:44:28.070748] [Steps    4340] [G 0.0081061]
[2023-05-11 12:44:44.689714] [Steps    4350] [G 0.0106575]
[2023-05-11 12:45:01.296574] [Steps    4360] [G 0.0071574]
[2023-05-11 12:45:17.998090] [Steps    4370] [G 0.0073809]
[2023-05-11 12:45:34.613394] [Steps    4380] [G 0.0068161]
[2023-05-11 12:45:51.217979] [Steps    4390] [G 0.0092834]
[2023-05-11 12:46:07.920116] [Steps    4400] [G 0.0066332]
[2023-05-11 12:46:24.509513] [Steps    4410] [G 0.0071762]
[2023-05-11 12:46:41.101502] [Steps    4420] [G 0.0094087]
[2023-05-11 12:46:57.696520] [Steps    4430] [G 0.0083675]
[2023-05-11 12:47:14.397951] [Steps    4440] [G 0.0083211]
[2023-05-11 12:47:31.006383] [Steps    4450] [G 0.0069460]
[2023-05-11 12:47:47.625947] [Steps    4460] [G 0.0080494]
[2023-05-11 12:48:04.315707] [Steps    4470] [G 0.0078867]
[2023-05-11 12:48:20.930466] [Steps    4480] [G 0.0090687]
[2023-05-11 12:48:37.548813] [Steps    4490] [G 0.0095797]
[2023-05-11 12:48:54.242962] [Steps    4500] [G 0.0070167]
[2023-05-11 12:49:10.867496] [Steps    4510] [G 0.0109726]
[2023-05-11 12:49:27.478832] [Steps    4520] [G 0.0061724]
[2023-05-11 12:49:44.051268] [Steps    4530] [G 0.0084031]
[2023-05-11 12:50:02.719278] [Steps    4540] [G 0.0063957]
[2023-05-11 12:50:19.300614] [Steps    4550] [G 0.0087711]
[2023-05-11 12:50:35.904712] [Steps    4560] [G 0.0073329]
[2023-05-11 12:50:52.597890] [Steps    4570] [G 0.0066842]
[2023-05-11 12:51:09.192963] [Steps    4580] [G 0.0068202]
[2023-05-11 12:51:25.797311] [Steps    4590] [G 0.0061742]
[2023-05-11 12:51:42.379922] [Steps    4600] [G 0.0064360]
[2023-05-11 12:51:59.063074] [Steps    4610] [G 0.0077607]
[2023-05-11 12:52:15.652114] [Steps    4620] [G 0.0065808]
[2023-05-11 12:52:32.238517] [Steps    4630] [G 0.0073149]
[2023-05-11 12:52:48.897523] [Steps    4640] [G 0.0079951]
[2023-05-11 12:53:05.511039] [Steps    4650] [G 0.0071524]
[2023-05-11 12:53:22.105204] [Steps    4660] [G 0.0070638]
[2023-05-11 12:53:38.742943] [Steps    4670] [G 0.0064808]
[2023-05-11 12:53:55.345203] [Steps    4680] [G 0.0075350]
[2023-05-11 12:54:11.934460] [Steps    4690] [G 0.0064819]
[2023-05-11 12:54:28.527687] [Steps    4700] [G 0.0095534]
[2023-05-11 12:54:45.176253] [Steps    4710] [G 0.0079272]
[2023-05-11 12:55:01.758760] [Steps    4720] [G 0.0081302]
[2023-05-11 12:55:18.352074] [Steps    4730] [G 0.0102825]
[2023-05-11 12:55:35.004964] [Steps    4740] [G 0.0066132]
[2023-05-11 12:55:51.593748] [Steps    4750] [G 0.0074868]
[2023-05-11 12:56:08.185276] [Steps    4760] [G 0.0063902]
[2023-05-11 12:56:24.784165] [Steps    4770] [G 0.0066090]
[2023-05-11 12:56:41.428115] [Steps    4780] [G 0.0076985]
[2023-05-11 12:56:58.034684] [Steps    4790] [G 0.0095958]
[2023-05-11 12:57:14.617656] [Steps    4800] [G 0.0102686]
[2023-05-11 12:57:31.265934] [Steps    4810] [G 0.0076828]
[2023-05-11 12:57:47.846947] [Steps    4820] [G 0.0084612]
[2023-05-11 12:58:04.398006] [Steps    4830] [G 0.0071339]
[2023-05-11 12:58:23.081992] [Steps    4840] [G 0.0073452]
[2023-05-11 12:58:39.668659] [Steps    4850] [G 0.0090514]
[2023-05-11 12:58:56.275023] [Steps    4860] [G 0.0080711]
[2023-05-11 12:59:12.888076] [Steps    4870] [G 0.0071811]
[2023-05-11 12:59:29.545289] [Steps    4880] [G 0.0075765]
[2023-05-11 12:59:46.173203] [Steps    4890] [G 0.0082227]
[2023-05-11 13:00:02.781900] [Steps    4900] [G 0.0072634]
[2023-05-11 13:00:19.471576] [Steps    4910] [G 0.0083223]
[2023-05-11 13:00:36.069538] [Steps    4920] [G 0.0091997]
[2023-05-11 13:00:52.692999] [Steps    4930] [G 0.0058792]
[2023-05-11 13:01:09.323235] [Steps    4940] [G 0.0085068]
[2023-05-11 13:01:26.019369] [Steps    4950] [G 0.0068358]
[2023-05-11 13:01:42.628225] [Steps    4960] [G 0.0048402]
[2023-05-11 13:01:59.233855] [Steps    4970] [G 0.0084464]
[2023-05-11 13:02:15.931776] [Steps    4980] [G 0.0087274]
[2023-05-11 13:02:32.548880] [Steps    4990] [G 0.0074260]
[2023-05-11 13:02:49.141135] [Steps    5000] [G 0.0070289]
Steps 5001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 13:03:07.237392] [Steps    5010] [G 0.0071689]
[2023-05-11 13:03:23.841096] [Steps    5020] [G 0.0081704]
[2023-05-11 13:03:40.502346] [Steps    5030] [G 0.0070027]
[2023-05-11 13:03:57.105061] [Steps    5040] [G 0.0110078]
[2023-05-11 13:04:13.722431] [Steps    5050] [G 0.0093896]
[2023-05-11 13:04:30.355340] [Steps    5060] [G 0.0084570]
[2023-05-11 13:04:47.045866] [Steps    5070] [G 0.0075149]
[2023-05-11 13:05:03.638158] [Steps    5080] [G 0.0071810]
[2023-05-11 13:05:20.234878] [Steps    5090] [G 0.0059199]
[2023-05-11 13:05:36.941216] [Steps    5100] [G 0.0069367]
[2023-05-11 13:05:53.535889] [Steps    5110] [G 0.0074448]
[2023-05-11 13:06:10.157990] [Steps    5120] [G 0.0059606]
[2023-05-11 13:06:26.818047] [Steps    5130] [G 0.0079257]
[2023-05-11 13:06:45.253557] [Steps    5140] [G 0.0069597]
[2023-05-11 13:07:01.856220] [Steps    5150] [G 0.0065032]
[2023-05-11 13:07:18.468212] [Steps    5160] [G 0.0087270]
[2023-05-11 13:07:35.209068] [Steps    5170] [G 0.0083518]
[2023-05-11 13:07:51.812085] [Steps    5180] [G 0.0070859]
[2023-05-11 13:08:08.451025] [Steps    5190] [G 0.0074431]
[2023-05-11 13:08:25.155007] [Steps    5200] [G 0.0059685]
[2023-05-11 13:08:41.792449] [Steps    5210] [G 0.0083540]
[2023-05-11 13:08:58.414518] [Steps    5220] [G 0.0083784]
[2023-05-11 13:09:15.020244] [Steps    5230] [G 0.0090449]
[2023-05-11 13:09:31.731344] [Steps    5240] [G 0.0069609]
[2023-05-11 13:09:48.376623] [Steps    5250] [G 0.0067761]
[2023-05-11 13:10:05.018880] [Steps    5260] [G 0.0086195]
[2023-05-11 13:10:21.707275] [Steps    5270] [G 0.0073148]
[2023-05-11 13:10:38.349303] [Steps    5280] [G 0.0079621]
[2023-05-11 13:10:54.972632] [Steps    5290] [G 0.0082701]
[2023-05-11 13:11:11.659018] [Steps    5300] [G 0.0068222]
[2023-05-11 13:11:28.283275] [Steps    5310] [G 0.0084087]
[2023-05-11 13:11:44.909307] [Steps    5320] [G 0.0084025]
[2023-05-11 13:12:01.549176] [Steps    5330] [G 0.0064984]
[2023-05-11 13:12:18.259900] [Steps    5340] [G 0.0069705]
[2023-05-11 13:12:34.884543] [Steps    5350] [G 0.0065455]
[2023-05-11 13:12:51.519548] [Steps    5360] [G 0.0081908]
[2023-05-11 13:13:08.215772] [Steps    5370] [G 0.0081882]
[2023-05-11 13:13:24.845531] [Steps    5380] [G 0.0070722]
[2023-05-11 13:13:41.461928] [Steps    5390] [G 0.0065903]
[2023-05-11 13:13:58.066395] [Steps    5400] [G 0.0095952]
[2023-05-11 13:14:14.752510] [Steps    5410] [G 0.0077696]
[2023-05-11 13:14:31.398703] [Steps    5420] [G 0.0068651]
[2023-05-11 13:14:47.991272] [Steps    5430] [G 0.0089913]
[2023-05-11 13:15:06.545592] [Steps    5440] [G 0.0071177]
[2023-05-11 13:15:23.149081] [Steps    5450] [G 0.0080747]
[2023-05-11 13:15:39.808614] [Steps    5460] [G 0.0069774]
[2023-05-11 13:15:56.453874] [Steps    5470] [G 0.0076595]
[2023-05-11 13:16:13.192783] [Steps    5480] [G 0.0058178]
[2023-05-11 13:16:29.843332] [Steps    5490] [G 0.0069572]
[2023-05-11 13:16:46.524788] [Steps    5500] [G 0.0075163]
[2023-05-11 13:17:03.243245] [Steps    5510] [G 0.0080036]
[2023-05-11 13:17:19.918377] [Steps    5520] [G 0.0069613]
[2023-05-11 13:17:36.555758] [Steps    5530] [G 0.0070362]
[2023-05-11 13:17:53.300149] [Steps    5540] [G 0.0070535]
[2023-05-11 13:18:09.935758] [Steps    5550] [G 0.0088222]
[2023-05-11 13:18:26.593264] [Steps    5560] [G 0.0081124]
[2023-05-11 13:18:43.240668] [Steps    5570] [G 0.0116241]
[2023-05-11 13:18:59.979966] [Steps    5580] [G 0.0085171]
[2023-05-11 13:19:16.631263] [Steps    5590] [G 0.0070332]
[2023-05-11 13:19:33.303602] [Steps    5600] [G 0.0075351]
[2023-05-11 13:19:50.020143] [Steps    5610] [G 0.0071860]
[2023-05-11 13:20:06.682681] [Steps    5620] [G 0.0070542]
[2023-05-11 13:20:23.346121] [Steps    5630] [G 0.0071433]
[2023-05-11 13:20:40.037352] [Steps    5640] [G 0.0070449]
[2023-05-11 13:20:56.772989] [Steps    5650] [G 0.0072864]
[2023-05-11 13:21:13.424984] [Steps    5660] [G 0.0057458]
[2023-05-11 13:21:30.047008] [Steps    5670] [G 0.0070600]
[2023-05-11 13:21:46.764440] [Steps    5680] [G 0.0087384]
[2023-05-11 13:22:03.377868] [Steps    5690] [G 0.0061234]
[2023-05-11 13:22:20.066994] [Steps    5700] [G 0.0096304]
[2023-05-11 13:22:36.759516] [Steps    5710] [G 0.0085302]
[2023-05-11 13:22:53.396466] [Steps    5720] [G 0.0074894]
[2023-05-11 13:23:10.025109] [Steps    5730] [G 0.0068440]
[2023-05-11 13:23:28.627783] [Steps    5740] [G 0.0068922]
[2023-05-11 13:23:45.339632] [Steps    5750] [G 0.0054941]
[2023-05-11 13:24:01.941882] [Steps    5760] [G 0.0066676]
[2023-05-11 13:24:18.564106] [Steps    5770] [G 0.0075684]
[2023-05-11 13:24:35.239625] [Steps    5780] [G 0.0092671]
[2023-05-11 13:24:51.852052] [Steps    5790] [G 0.0075759]
[2023-05-11 13:25:08.470063] [Steps    5800] [G 0.0071564]
[2023-05-11 13:25:25.072627] [Steps    5810] [G 0.0099431]
[2023-05-11 13:25:41.746950] [Steps    5820] [G 0.0062031]
[2023-05-11 13:25:58.363838] [Steps    5830] [G 0.0061105]
[2023-05-11 13:26:14.966585] [Steps    5840] [G 0.0065616]
[2023-05-11 13:26:31.650229] [Steps    5850] [G 0.0057140]
[2023-05-11 13:26:48.216802] [Steps    5860] [G 0.0076680]
[2023-05-11 13:27:04.827031] [Steps    5870] [G 0.0061669]
[2023-05-11 13:27:21.498034] [Steps    5880] [G 0.0065797]
[2023-05-11 13:27:38.107340] [Steps    5890] [G 0.0062726]
[2023-05-11 13:27:54.706934] [Steps    5900] [G 0.0062624]
[2023-05-11 13:28:11.345492] [Steps    5910] [G 0.0084763]
[2023-05-11 13:28:28.015091] [Steps    5920] [G 0.0081851]
[2023-05-11 13:28:44.593957] [Steps    5930] [G 0.0075830]
[2023-05-11 13:29:01.209580] [Steps    5940] [G 0.0073522]
[2023-05-11 13:29:17.891609] [Steps    5950] [G 0.0077923]
[2023-05-11 13:29:34.510735] [Steps    5960] [G 0.0080081]
[2023-05-11 13:29:51.108965] [Steps    5970] [G 0.0097696]
[2023-05-11 13:30:07.717345] [Steps    5980] [G 0.0069928]
[2023-05-11 13:30:24.414484] [Steps    5990] [G 0.0069977]
[2023-05-11 13:30:41.015560] [Steps    6000] [G 0.0074726]
Steps 6001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 13:30:59.087288] [Steps    6010] [G 0.0078974]
[2023-05-11 13:31:15.750844] [Steps    6020] [G 0.0067244]
[2023-05-11 13:31:32.355910] [Steps    6030] [G 0.0081463]
[2023-05-11 13:31:48.959929] [Steps    6040] [G 0.0074651]
[2023-05-11 13:32:07.558045] [Steps    6050] [G 0.0064288]
[2023-05-11 13:32:24.157332] [Steps    6060] [G 0.0079646]
[2023-05-11 13:32:40.816862] [Steps    6070] [G 0.0064165]
[2023-05-11 13:32:57.440745] [Steps    6080] [G 0.0088944]
[2023-05-11 13:33:14.033021] [Steps    6090] [G 0.0055345]
[2023-05-11 13:33:30.658316] [Steps    6100] [G 0.0072679]
[2023-05-11 13:33:47.336046] [Steps    6110] [G 0.0062381]
[2023-05-11 13:34:03.952217] [Steps    6120] [G 0.0064020]
[2023-05-11 13:34:20.556610] [Steps    6130] [G 0.0087901]
[2023-05-11 13:34:37.247143] [Steps    6140] [G 0.0079287]
[2023-05-11 13:34:53.875553] [Steps    6150] [G 0.0072989]
[2023-05-11 13:35:10.490936] [Steps    6160] [G 0.0090356]
[2023-05-11 13:35:27.151757] [Steps    6170] [G 0.0074613]
[2023-05-11 13:35:43.763023] [Steps    6180] [G 0.0091029]
[2023-05-11 13:36:00.396825] [Steps    6190] [G 0.0063849]
[2023-05-11 13:36:17.017823] [Steps    6200] [G 0.0059741]
[2023-05-11 13:36:33.681877] [Steps    6210] [G 0.0088372]
[2023-05-11 13:36:50.318620] [Steps    6220] [G 0.0061731]
[2023-05-11 13:37:06.919753] [Steps    6230] [G 0.0091020]
[2023-05-11 13:37:23.600500] [Steps    6240] [G 0.0089678]
[2023-05-11 13:37:40.181298] [Steps    6250] [G 0.0092628]
[2023-05-11 13:37:56.812175] [Steps    6260] [G 0.0064302]
[2023-05-11 13:38:13.474079] [Steps    6270] [G 0.0044410]
[2023-05-11 13:38:30.259516] [Steps    6280] [G 0.0061196]
[2023-05-11 13:38:46.872725] [Steps    6290] [G 0.0055884]
[2023-05-11 13:39:03.492577] [Steps    6300] [G 0.0065638]
[2023-05-11 13:39:20.183542] [Steps    6310] [G 0.0081198]
[2023-05-11 13:39:36.797326] [Steps    6320] [G 0.0059031]
[2023-05-11 13:39:53.404763] [Steps    6330] [G 0.0068748]
[2023-05-11 13:40:10.037995] [Steps    6340] [G 0.0078152]
[2023-05-11 13:40:28.568405] [Steps    6350] [G 0.0072237]
[2023-05-11 13:40:45.284906] [Steps    6360] [G 0.0079384]
[2023-05-11 13:41:01.896245] [Steps    6370] [G 0.0056317]
[2023-05-11 13:41:18.592616] [Steps    6380] [G 0.0063777]
[2023-05-11 13:41:35.219117] [Steps    6390] [G 0.0064319]
[2023-05-11 13:41:51.877407] [Steps    6400] [G 0.0065485]
[2023-05-11 13:42:08.593275] [Steps    6410] [G 0.0077144]
[2023-05-11 13:42:25.236809] [Steps    6420] [G 0.0076927]
[2023-05-11 13:42:41.882892] [Steps    6430] [G 0.0078314]
[2023-05-11 13:42:58.527713] [Steps    6440] [G 0.0041541]
[2023-05-11 13:43:15.224523] [Steps    6450] [G 0.0093799]
[2023-05-11 13:43:31.845319] [Steps    6460] [G 0.0049986]
[2023-05-11 13:43:48.487415] [Steps    6470] [G 0.0062225]
[2023-05-11 13:44:05.219331] [Steps    6480] [G 0.0093837]
[2023-05-11 13:44:21.827793] [Steps    6490] [G 0.0081054]
[2023-05-11 13:44:38.484792] [Steps    6500] [G 0.0065047]
[2023-05-11 13:44:55.198642] [Steps    6510] [G 0.0071605]
[2023-05-11 13:45:11.825957] [Steps    6520] [G 0.0082966]
[2023-05-11 13:45:28.458041] [Steps    6530] [G 0.0056759]
[2023-05-11 13:45:45.144020] [Steps    6540] [G 0.0054827]
[2023-05-11 13:46:01.842200] [Steps    6550] [G 0.0082425]
[2023-05-11 13:46:18.509336] [Steps    6560] [G 0.0071753]
[2023-05-11 13:46:35.127973] [Steps    6570] [G 0.0045370]
[2023-05-11 13:46:51.813896] [Steps    6580] [G 0.0081639]
[2023-05-11 13:47:08.461295] [Steps    6590] [G 0.0072323]
[2023-05-11 13:47:25.088318] [Steps    6600] [G 0.0069892]
[2023-05-11 13:47:41.725215] [Steps    6610] [G 0.0064519]
[2023-05-11 13:47:58.414023] [Steps    6620] [G 0.0063358]
[2023-05-11 13:48:15.050339] [Steps    6630] [G 0.0085726]
[2023-05-11 13:48:31.662758] [Steps    6640] [G 0.0074484]
[2023-05-11 13:48:50.336483] [Steps    6650] [G 0.0073788]
[2023-05-11 13:49:06.920664] [Steps    6660] [G 0.0069501]
[2023-05-11 13:49:23.585830] [Steps    6670] [G 0.0076808]
[2023-05-11 13:49:40.181335] [Steps    6680] [G 0.0061682]
[2023-05-11 13:49:56.842812] [Steps    6690] [G 0.0059930]
[2023-05-11 13:50:13.431245] [Steps    6700] [G 0.0073498]
[2023-05-11 13:50:30.060446] [Steps    6710] [G 0.0081656]
[2023-05-11 13:50:46.760609] [Steps    6720] [G 0.0071715]
[2023-05-11 13:51:03.377615] [Steps    6730] [G 0.0066670]
[2023-05-11 13:51:19.962228] [Steps    6740] [G 0.0075480]
[2023-05-11 13:51:36.623735] [Steps    6750] [G 0.0075696]
[2023-05-11 13:51:53.215840] [Steps    6760] [G 0.0066434]
[2023-05-11 13:52:09.806806] [Steps    6770] [G 0.0083348]
[2023-05-11 13:52:26.409161] [Steps    6780] [G 0.0060524]
[2023-05-11 13:52:43.079771] [Steps    6790] [G 0.0061318]
[2023-05-11 13:52:59.675050] [Steps    6800] [G 0.0079540]
[2023-05-11 13:53:16.251597] [Steps    6810] [G 0.0086184]
[2023-05-11 13:53:32.933379] [Steps    6820] [G 0.0089364]
[2023-05-11 13:53:49.532213] [Steps    6830] [G 0.0069003]
[2023-05-11 13:54:06.132446] [Steps    6840] [G 0.0074817]
[2023-05-11 13:54:22.717667] [Steps    6850] [G 0.0057212]
[2023-05-11 13:54:39.394025] [Steps    6860] [G 0.0086595]
[2023-05-11 13:54:55.973864] [Steps    6870] [G 0.0075359]
[2023-05-11 13:55:12.560228] [Steps    6880] [G 0.0080801]
[2023-05-11 13:55:29.221753] [Steps    6890] [G 0.0063378]
[2023-05-11 13:55:45.812723] [Steps    6900] [G 0.0059452]
[2023-05-11 13:56:02.396554] [Steps    6910] [G 0.0085871]
[2023-05-11 13:56:19.061239] [Steps    6920] [G 0.0055837]
[2023-05-11 13:56:35.641900] [Steps    6930] [G 0.0061505]
[2023-05-11 13:56:52.208950] [Steps    6940] [G 0.0058402]
[2023-05-11 13:57:10.758857] [Steps    6950] [G 0.0076614]
[2023-05-11 13:57:27.482712] [Steps    6960] [G 0.0087863]
[2023-05-11 13:57:44.071958] [Steps    6970] [G 0.0087192]
[2023-05-11 13:58:00.707173] [Steps    6980] [G 0.0066254]
[2023-05-11 13:58:17.403366] [Steps    6990] [G 0.0068139]
[2023-05-11 13:58:34.094766] [Steps    7000] [G 0.0075663]
Steps 7001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 13:58:52.183195] [Steps    7010] [G 0.0078572]
[2023-05-11 13:59:08.805359] [Steps    7020] [G 0.0058059]
[2023-05-11 13:59:25.513373] [Steps    7030] [G 0.0064674]
[2023-05-11 13:59:42.209422] [Steps    7040] [G 0.0082262]
[2023-05-11 13:59:58.833589] [Steps    7050] [G 0.0076663]
[2023-05-11 14:00:15.555373] [Steps    7060] [G 0.0085568]
[2023-05-11 14:00:32.158334] [Steps    7070] [G 0.0106540]
[2023-05-11 14:00:48.870122] [Steps    7080] [G 0.0062969]
[2023-05-11 14:01:05.498340] [Steps    7090] [G 0.0065930]
[2023-05-11 14:01:22.206691] [Steps    7100] [G 0.0058723]
[2023-05-11 14:01:38.892264] [Steps    7110] [G 0.0069988]
[2023-05-11 14:01:55.522837] [Steps    7120] [G 0.0057190]
[2023-05-11 14:02:12.226125] [Steps    7130] [G 0.0074988]
[2023-05-11 14:02:28.855024] [Steps    7140] [G 0.0058945]
[2023-05-11 14:02:45.558471] [Steps    7150] [G 0.0077256]
[2023-05-11 14:03:02.226637] [Steps    7160] [G 0.0051869]
[2023-05-11 14:03:18.850935] [Steps    7170] [G 0.0075293]
[2023-05-11 14:03:35.552631] [Steps    7180] [G 0.0051535]
[2023-05-11 14:03:52.173736] [Steps    7190] [G 0.0064217]
[2023-05-11 14:04:08.883289] [Steps    7200] [G 0.0057690]
[2023-05-11 14:04:25.591340] [Steps    7210] [G 0.0059525]
[2023-05-11 14:04:42.247107] [Steps    7220] [G 0.0084198]
[2023-05-11 14:04:58.922241] [Steps    7230] [G 0.0056758]
[2023-05-11 14:05:15.515821] [Steps    7240] [G 0.0057349]
[2023-05-11 14:05:34.071071] [Steps    7250] [G 0.0046284]
[2023-05-11 14:05:50.666117] [Steps    7260] [G 0.0049911]
[2023-05-11 14:06:07.352306] [Steps    7270] [G 0.0065502]
[2023-05-11 14:06:24.039686] [Steps    7280] [G 0.0073983]
[2023-05-11 14:06:40.642074] [Steps    7290] [G 0.0091635]
[2023-05-11 14:06:57.316050] [Steps    7300] [G 0.0049728]
[2023-05-11 14:07:13.970796] [Steps    7310] [G 0.0073440]
[2023-05-11 14:07:30.756842] [Steps    7320] [G 0.0064768]
[2023-05-11 14:07:47.350851] [Steps    7330] [G 0.0051588]
[2023-05-11 14:08:04.079750] [Steps    7340] [G 0.0106133]
[2023-05-11 14:08:20.767748] [Steps    7350] [G 0.0054648]
[2023-05-11 14:08:37.389650] [Steps    7360] [G 0.0081144]
[2023-05-11 14:08:54.088314] [Steps    7370] [G 0.0045711]
[2023-05-11 14:09:10.720532] [Steps    7380] [G 0.0050103]
[2023-05-11 14:09:27.388046] [Steps    7390] [G 0.0073055]
[2023-05-11 14:09:44.050376] [Steps    7400] [G 0.0068542]
[2023-05-11 14:10:00.662225] [Steps    7410] [G 0.0048742]
[2023-05-11 14:10:17.354225] [Steps    7420] [G 0.0081345]
[2023-05-11 14:10:33.967983] [Steps    7430] [G 0.0056008]
[2023-05-11 14:10:50.638425] [Steps    7440] [G 0.0066448]
[2023-05-11 14:11:07.344974] [Steps    7450] [G 0.0052771]
[2023-05-11 14:11:23.942278] [Steps    7460] [G 0.0057101]
[2023-05-11 14:11:40.630994] [Steps    7470] [G 0.0078091]
[2023-05-11 14:11:57.261390] [Steps    7480] [G 0.0074638]
[2023-05-11 14:12:13.937857] [Steps    7490] [G 0.0065963]
[2023-05-11 14:12:30.539535] [Steps    7500] [G 0.0063359]
[2023-05-11 14:12:47.270682] [Steps    7510] [G 0.0070277]
[2023-05-11 14:13:03.953139] [Steps    7520] [G 0.0055245]
[2023-05-11 14:13:20.547748] [Steps    7530] [G 0.0068461]
[2023-05-11 14:13:37.220374] [Steps    7540] [G 0.0044745]
[2023-05-11 14:13:53.815344] [Steps    7550] [G 0.0062455]
[2023-05-11 14:14:12.400411] [Steps    7560] [G 0.0059164]
[2023-05-11 14:14:28.984820] [Steps    7570] [G 0.0051638]
[2023-05-11 14:14:45.579407] [Steps    7580] [G 0.0052976]
[2023-05-11 14:15:02.295047] [Steps    7590] [G 0.0045484]
[2023-05-11 14:15:18.919945] [Steps    7600] [G 0.0060343]
[2023-05-11 14:15:35.530084] [Steps    7610] [G 0.0074186]
[2023-05-11 14:15:52.205943] [Steps    7620] [G 0.0054829]
[2023-05-11 14:16:08.834898] [Steps    7630] [G 0.0059516]
[2023-05-11 14:16:25.465110] [Steps    7640] [G 0.0058700]
[2023-05-11 14:16:42.104629] [Steps    7650] [G 0.0078467]
[2023-05-11 14:16:58.801864] [Steps    7660] [G 0.0081842]
[2023-05-11 14:17:15.408062] [Steps    7670] [G 0.0079037]
[2023-05-11 14:17:32.029676] [Steps    7680] [G 0.0070750]
[2023-05-11 14:17:48.853585] [Steps    7690] [G 0.0061226]
[2023-05-11 14:18:05.473143] [Steps    7700] [G 0.0062880]
[2023-05-11 14:18:22.100621] [Steps    7710] [G 0.0065130]
[2023-05-11 14:18:38.772643] [Steps    7720] [G 0.0057514]
[2023-05-11 14:18:55.490891] [Steps    7730] [G 0.0086061]
[2023-05-11 14:19:12.166539] [Steps    7740] [G 0.0059808]
[2023-05-11 14:19:28.855341] [Steps    7750] [G 0.0065389]
[2023-05-11 14:19:45.593862] [Steps    7760] [G 0.0055835]
[2023-05-11 14:20:02.262015] [Steps    7770] [G 0.0060521]
[2023-05-11 14:20:18.928754] [Steps    7780] [G 0.0067749]
[2023-05-11 14:20:35.655356] [Steps    7790] [G 0.0078753]
[2023-05-11 14:20:52.301791] [Steps    7800] [G 0.0059104]
[2023-05-11 14:21:08.971478] [Steps    7810] [G 0.0069969]
[2023-05-11 14:21:25.616841] [Steps    7820] [G 0.0070952]
[2023-05-11 14:21:42.360550] [Steps    7830] [G 0.0054651]
[2023-05-11 14:21:59.003404] [Steps    7840] [G 0.0083388]
[2023-05-11 14:22:15.627376] [Steps    7850] [G 0.0053666]
[2023-05-11 14:22:34.361298] [Steps    7860] [G 0.0060420]
[2023-05-11 14:22:51.022739] [Steps    7870] [G 0.0073412]
[2023-05-11 14:23:07.690093] [Steps    7880] [G 0.0054951]
[2023-05-11 14:23:24.385025] [Steps    7890] [G 0.0051358]
[2023-05-11 14:23:41.138431] [Steps    7900] [G 0.0060973]
[2023-05-11 14:23:57.842387] [Steps    7910] [G 0.0049540]
[2023-05-11 14:24:14.521477] [Steps    7920] [G 0.0065059]
[2023-05-11 14:24:31.307406] [Steps    7930] [G 0.0072199]
[2023-05-11 14:24:48.051266] [Steps    7940] [G 0.0086194]
[2023-05-11 14:25:04.734290] [Steps    7950] [G 0.0059775]
[2023-05-11 14:25:21.488976] [Steps    7960] [G 0.0049519]
[2023-05-11 14:25:38.224747] [Steps    7970] [G 0.0075971]
[2023-05-11 14:25:54.887570] [Steps    7980] [G 0.0054299]
[2023-05-11 14:26:11.621553] [Steps    7990] [G 0.0039475]
[2023-05-11 14:26:28.374750] [Steps    8000] [G 0.0069845]
Steps 8001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 14:26:46.456754] [Steps    8010] [G 0.0063692]
[2023-05-11 14:27:03.216482] [Steps    8020] [G 0.0066264]
[2023-05-11 14:27:19.828979] [Steps    8030] [G 0.0071663]
[2023-05-11 14:27:36.521581] [Steps    8040] [G 0.0053216]
[2023-05-11 14:27:53.266586] [Steps    8050] [G 0.0059415]
[2023-05-11 14:28:09.954015] [Steps    8060] [G 0.0070275]
[2023-05-11 14:28:26.669342] [Steps    8070] [G 0.0069533]
[2023-05-11 14:28:43.392264] [Steps    8080] [G 0.0064003]
[2023-05-11 14:29:00.112205] [Steps    8090] [G 0.0064287]
[2023-05-11 14:29:16.770604] [Steps    8100] [G 0.0081221]
[2023-05-11 14:29:33.447296] [Steps    8110] [G 0.0065591]
[2023-05-11 14:29:50.217819] [Steps    8120] [G 0.0059505]
[2023-05-11 14:30:06.919740] [Steps    8130] [G 0.0076204]
[2023-05-11 14:30:23.617064] [Steps    8140] [G 0.0058162]
[2023-05-11 14:30:40.342858] [Steps    8150] [G 0.0055735]
[2023-05-11 14:30:58.838650] [Steps    8160] [G 0.0059227]
[2023-05-11 14:31:15.471235] [Steps    8170] [G 0.0056916]
[2023-05-11 14:31:32.118157] [Steps    8180] [G 0.0074489]
[2023-05-11 14:31:48.858900] [Steps    8190] [G 0.0067401]
[2023-05-11 14:32:05.550028] [Steps    8200] [G 0.0057794]
[2023-05-11 14:32:22.246479] [Steps    8210] [G 0.0060637]
[2023-05-11 14:32:39.006986] [Steps    8220] [G 0.0054434]
[2023-05-11 14:32:55.700419] [Steps    8230] [G 0.0063710]
[2023-05-11 14:33:12.387086] [Steps    8240] [G 0.0076889]
[2023-05-11 14:33:29.143320] [Steps    8250] [G 0.0072608]
[2023-05-11 14:33:45.792633] [Steps    8260] [G 0.0060868]
[2023-05-11 14:34:02.483808] [Steps    8270] [G 0.0060825]
[2023-05-11 14:34:19.148066] [Steps    8280] [G 0.0058879]
[2023-05-11 14:34:35.931609] [Steps    8290] [G 0.0082277]
[2023-05-11 14:34:52.608961] [Steps    8300] [G 0.0064669]
[2023-05-11 14:35:09.298651] [Steps    8310] [G 0.0046995]
[2023-05-11 14:35:26.076096] [Steps    8320] [G 0.0067761]
[2023-05-11 14:35:42.775322] [Steps    8330] [G 0.0058701]
[2023-05-11 14:35:59.481497] [Steps    8340] [G 0.0055518]
[2023-05-11 14:36:16.176565] [Steps    8350] [G 0.0066830]
[2023-05-11 14:36:32.980632] [Steps    8360] [G 0.0063281]
[2023-05-11 14:36:49.616851] [Steps    8370] [G 0.0056443]
[2023-05-11 14:37:06.253587] [Steps    8380] [G 0.0059097]
[2023-05-11 14:37:22.988762] [Steps    8390] [G 0.0053346]
[2023-05-11 14:37:39.670170] [Steps    8400] [G 0.0051454]
[2023-05-11 14:37:56.346196] [Steps    8410] [G 0.0081717]
[2023-05-11 14:38:13.064230] [Steps    8420] [G 0.0065585]
[2023-05-11 14:38:29.751723] [Steps    8430] [G 0.0057570]
[2023-05-11 14:38:46.419518] [Steps    8440] [G 0.0053786]
[2023-05-11 14:39:03.082756] [Steps    8450] [G 0.0062462]
[2023-05-11 14:39:21.810072] [Steps    8460] [G 0.0057685]
[2023-05-11 14:39:38.547238] [Steps    8470] [G 0.0062091]
[2023-05-11 14:39:55.207554] [Steps    8480] [G 0.0053124]
[2023-05-11 14:40:11.932445] [Steps    8490] [G 0.0066808]
[2023-05-11 14:40:28.636880] [Steps    8500] [G 0.0051912]
[2023-05-11 14:40:45.370755] [Steps    8510] [G 0.0067689]
[2023-05-11 14:41:02.050219] [Steps    8520] [G 0.0061925]
[2023-05-11 14:41:18.868348] [Steps    8530] [G 0.0060075]
[2023-05-11 14:41:35.558040] [Steps    8540] [G 0.0060338]
[2023-05-11 14:41:52.276624] [Steps    8550] [G 0.0063569]
[2023-05-11 14:42:09.040460] [Steps    8560] [G 0.0061690]
[2023-05-11 14:42:25.774078] [Steps    8570] [G 0.0067929]
[2023-05-11 14:42:42.447789] [Steps    8580] [G 0.0081655]
[2023-05-11 14:42:59.157186] [Steps    8590] [G 0.0048923]
[2023-05-11 14:43:15.883281] [Steps    8600] [G 0.0056883]
[2023-05-11 14:43:32.604871] [Steps    8610] [G 0.0039424]
[2023-05-11 14:43:49.312492] [Steps    8620] [G 0.0044614]
[2023-05-11 14:44:06.081617] [Steps    8630] [G 0.0071567]
[2023-05-11 14:44:22.799267] [Steps    8640] [G 0.0048829]
[2023-05-11 14:44:39.492158] [Steps    8650] [G 0.0064181]
[2023-05-11 14:44:56.222864] [Steps    8660] [G 0.0052558]
[2023-05-11 14:45:12.956745] [Steps    8670] [G 0.0068086]
[2023-05-11 14:45:29.669272] [Steps    8680] [G 0.0054907]
[2023-05-11 14:45:46.384289] [Steps    8690] [G 0.0056707]
[2023-05-11 14:46:03.174049] [Steps    8700] [G 0.0059723]
[2023-05-11 14:46:19.899624] [Steps    8710] [G 0.0062048]
[2023-05-11 14:46:36.615175] [Steps    8720] [G 0.0052806]
[2023-05-11 14:46:53.406716] [Steps    8730] [G 0.0066600]
[2023-05-11 14:47:10.131193] [Steps    8740] [G 0.0067755]
[2023-05-11 14:47:26.851233] [Steps    8750] [G 0.0054667]
[2023-05-11 14:47:45.520174] [Steps    8760] [G 0.0050272]
[2023-05-11 14:48:02.101384] [Steps    8770] [G 0.0043781]
[2023-05-11 14:48:18.741760] [Steps    8780] [G 0.0040136]
[2023-05-11 14:48:35.408538] [Steps    8790] [G 0.0055929]
[2023-05-11 14:48:52.158752] [Steps    8800] [G 0.0081798]
[2023-05-11 14:49:08.802711] [Steps    8810] [G 0.0067866]
[2023-05-11 14:49:25.467344] [Steps    8820] [G 0.0065916]
[2023-05-11 14:49:42.213736] [Steps    8830] [G 0.0056712]
[2023-05-11 14:49:58.855581] [Steps    8840] [G 0.0049654]
[2023-05-11 14:50:15.509465] [Steps    8850] [G 0.0056853]
[2023-05-11 14:50:32.178615] [Steps    8860] [G 0.0047460]
[2023-05-11 14:50:48.973708] [Steps    8870] [G 0.0055693]
[2023-05-11 14:51:05.617494] [Steps    8880] [G 0.0054885]
[2023-05-11 14:51:22.260240] [Steps    8890] [G 0.0048811]
[2023-05-11 14:51:38.982438] [Steps    8900] [G 0.0061022]
[2023-05-11 14:51:55.591869] [Steps    8910] [G 0.0055246]
[2023-05-11 14:52:12.250415] [Steps    8920] [G 0.0051248]
[2023-05-11 14:52:28.930437] [Steps    8930] [G 0.0057710]
[2023-05-11 14:52:45.560337] [Steps    8940] [G 0.0051719]
[2023-05-11 14:53:02.204086] [Steps    8950] [G 0.0061874]
[2023-05-11 14:53:18.831484] [Steps    8960] [G 0.0084627]
[2023-05-11 14:53:35.536552] [Steps    8970] [G 0.0036871]
[2023-05-11 14:53:52.165198] [Steps    8980] [G 0.0061235]
[2023-05-11 14:54:08.834858] [Steps    8990] [G 0.0049295]
[2023-05-11 14:54:25.514185] [Steps    9000] [G 0.0055693]
Steps 9001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 14:54:43.490138] [Steps    9010] [G 0.0056434]
[2023-05-11 14:55:00.220291] [Steps    9020] [G 0.0059266]
[2023-05-11 14:55:16.920069] [Steps    9030] [G 0.0088791]
[2023-05-11 14:55:33.569340] [Steps    9040] [G 0.0048797]
[2023-05-11 14:55:50.301071] [Steps    9050] [G 0.0079615]
[2023-05-11 14:56:06.913350] [Steps    9060] [G 0.0077040]
[2023-05-11 14:56:25.461327] [Steps    9070] [G 0.0044506]
[2023-05-11 14:56:42.056379] [Steps    9080] [G 0.0058403]
[2023-05-11 14:56:58.740903] [Steps    9090] [G 0.0063413]
[2023-05-11 14:57:15.374081] [Steps    9100] [G 0.0065333]
[2023-05-11 14:57:31.991067] [Steps    9110] [G 0.0077677]
[2023-05-11 14:57:48.670422] [Steps    9120] [G 0.0051957]
[2023-05-11 14:58:05.286399] [Steps    9130] [G 0.0061963]
[2023-05-11 14:58:21.889978] [Steps    9140] [G 0.0050569]
[2023-05-11 14:58:38.505338] [Steps    9150] [G 0.0060964]
[2023-05-11 14:58:55.216570] [Steps    9160] [G 0.0060820]
[2023-05-11 14:59:11.818303] [Steps    9170] [G 0.0069509]
[2023-05-11 14:59:28.426618] [Steps    9180] [G 0.0051597]
[2023-05-11 14:59:45.099663] [Steps    9190] [G 0.0063446]
[2023-05-11 15:00:01.728531] [Steps    9200] [G 0.0061813]
[2023-05-11 15:00:18.330618] [Steps    9210] [G 0.0056506]
[2023-05-11 15:00:35.004101] [Steps    9220] [G 0.0056234]
[2023-05-11 15:00:51.628196] [Steps    9230] [G 0.0045636]
[2023-05-11 15:01:08.215120] [Steps    9240] [G 0.0056539]
[2023-05-11 15:01:24.832467] [Steps    9250] [G 0.0053600]
[2023-05-11 15:01:41.544708] [Steps    9260] [G 0.0063868]
[2023-05-11 15:01:58.177464] [Steps    9270] [G 0.0043160]
[2023-05-11 15:02:14.784791] [Steps    9280] [G 0.0040176]
[2023-05-11 15:02:31.473348] [Steps    9290] [G 0.0057228]
[2023-05-11 15:02:48.089117] [Steps    9300] [G 0.0059598]
[2023-05-11 15:03:04.708336] [Steps    9310] [G 0.0066357]
[2023-05-11 15:03:21.333142] [Steps    9320] [G 0.0082136]
[2023-05-11 15:03:37.978850] [Steps    9330] [G 0.0067534]
[2023-05-11 15:03:54.600574] [Steps    9340] [G 0.0058445]
[2023-05-11 15:04:11.199937] [Steps    9350] [G 0.0064267]
[2023-05-11 15:04:27.836848] [Steps    9360] [G 0.0064164]
[2023-05-11 15:04:46.202088] [Steps    9370] [G 0.0063581]
[2023-05-11 15:05:02.806884] [Steps    9380] [G 0.0049423]
[2023-05-11 15:05:19.511023] [Steps    9390] [G 0.0061828]
[2023-05-11 15:05:36.109710] [Steps    9400] [G 0.0065898]
[2023-05-11 15:05:52.735221] [Steps    9410] [G 0.0039631]
[2023-05-11 15:06:09.341468] [Steps    9420] [G 0.0075620]
[2023-05-11 15:06:26.018492] [Steps    9430] [G 0.0053145]
[2023-05-11 15:06:42.626710] [Steps    9440] [G 0.0041929]
[2023-05-11 15:06:59.237179] [Steps    9450] [G 0.0063679]
[2023-05-11 15:07:15.940010] [Steps    9460] [G 0.0090906]
[2023-05-11 15:07:32.547456] [Steps    9470] [G 0.0071821]
[2023-05-11 15:07:49.154036] [Steps    9480] [G 0.0036501]
[2023-05-11 15:08:05.784953] [Steps    9490] [G 0.0057268]
[2023-05-11 15:08:22.477891] [Steps    9500] [G 0.0075386]
[2023-05-11 15:08:39.107256] [Steps    9510] [G 0.0057033]
[2023-05-11 15:08:55.793453] [Steps    9520] [G 0.0050035]
[2023-05-11 15:09:12.483740] [Steps    9530] [G 0.0058400]
[2023-05-11 15:09:29.112779] [Steps    9540] [G 0.0054893]
[2023-05-11 15:09:45.728027] [Steps    9550] [G 0.0056236]
[2023-05-11 15:10:02.398150] [Steps    9560] [G 0.0079852]
[2023-05-11 15:10:19.015706] [Steps    9570] [G 0.0051086]
[2023-05-11 15:10:35.639391] [Steps    9580] [G 0.0058770]
[2023-05-11 15:10:52.224386] [Steps    9590] [G 0.0078632]
[2023-05-11 15:11:08.912404] [Steps    9600] [G 0.0056861]
[2023-05-11 15:11:25.518719] [Steps    9610] [G 0.0058237]
[2023-05-11 15:11:42.145481] [Steps    9620] [G 0.0054758]
[2023-05-11 15:11:58.820990] [Steps    9630] [G 0.0060382]
[2023-05-11 15:12:15.432987] [Steps    9640] [G 0.0082437]
[2023-05-11 15:12:32.029249] [Steps    9650] [G 0.0047048]
[2023-05-11 15:12:48.605152] [Steps    9660] [G 0.0074249]
[2023-05-11 15:13:07.124338] [Steps    9670] [G 0.0065602]
[2023-05-11 15:13:23.706621] [Steps    9680] [G 0.0067337]
[2023-05-11 15:13:40.324785] [Steps    9690] [G 0.0056426]
[2023-05-11 15:13:57.014803] [Steps    9700] [G 0.0049782]
[2023-05-11 15:14:13.633603] [Steps    9710] [G 0.0053944]
[2023-05-11 15:14:30.245004] [Steps    9720] [G 0.0048824]
[2023-05-11 15:14:46.961307] [Steps    9730] [G 0.0082229]
[2023-05-11 15:15:03.590060] [Steps    9740] [G 0.0069875]
[2023-05-11 15:15:20.198331] [Steps    9750] [G 0.0074961]
[2023-05-11 15:15:36.921316] [Steps    9760] [G 0.0042704]
[2023-05-11 15:15:53.615981] [Steps    9770] [G 0.0060686]
[2023-05-11 15:16:10.243918] [Steps    9780] [G 0.0049589]
[2023-05-11 15:16:26.870307] [Steps    9790] [G 0.0075126]
[2023-05-11 15:16:43.575460] [Steps    9800] [G 0.0062723]
[2023-05-11 15:17:00.201624] [Steps    9810] [G 0.0072298]
[2023-05-11 15:17:16.811155] [Steps    9820] [G 0.0059879]
[2023-05-11 15:17:33.433497] [Steps    9830] [G 0.0064927]
[2023-05-11 15:17:50.137319] [Steps    9840] [G 0.0060090]
[2023-05-11 15:18:06.764186] [Steps    9850] [G 0.0057703]
[2023-05-11 15:18:23.379046] [Steps    9860] [G 0.0065193]
[2023-05-11 15:18:40.067775] [Steps    9870] [G 0.0049739]
[2023-05-11 15:18:56.700680] [Steps    9880] [G 0.0058955]
[2023-05-11 15:19:13.311446] [Steps    9890] [G 0.0057072]
[2023-05-11 15:19:30.038894] [Steps    9900] [G 0.0077634]
[2023-05-11 15:19:46.641620] [Steps    9910] [G 0.0071727]
[2023-05-11 15:20:03.265991] [Steps    9920] [G 0.0062735]
[2023-05-11 15:20:19.894498] [Steps    9930] [G 0.0039570]
[2023-05-11 15:20:36.581085] [Steps    9940] [G 0.0057338]
[2023-05-11 15:20:53.210363] [Steps    9950] [G 0.0059136]
[2023-05-11 15:21:09.783277] [Steps    9960] [G 0.0066930]
[2023-05-11 15:21:28.512633] [Steps    9970] [G 0.0051250]
[2023-05-11 15:21:45.108059] [Steps    9980] [G 0.0057748]
[2023-05-11 15:22:01.861233] [Steps    9990] [G 0.0043186]
[2023-05-11 15:22:18.473293] [Steps   10000] [G 0.0062933]
Steps 10001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 15:22:37.725252] [Steps   10010] [G 0.0063889]
[2023-05-11 15:22:54.325622] [Steps   10020] [G 0.0060055]
[2023-05-11 15:23:11.013037] [Steps   10030] [G 0.0055825]
[2023-05-11 15:23:27.707724] [Steps   10040] [G 0.0042871]
[2023-05-11 15:23:44.331839] [Steps   10050] [G 0.0075327]
[2023-05-11 15:24:01.038165] [Steps   10060] [G 0.0051174]
[2023-05-11 15:24:17.663705] [Steps   10070] [G 0.0053658]
[2023-05-11 15:24:34.302720] [Steps   10080] [G 0.0050333]
[2023-05-11 15:24:50.930854] [Steps   10090] [G 0.0065078]
[2023-05-11 15:25:07.617613] [Steps   10100] [G 0.0060823]
[2023-05-11 15:25:24.241313] [Steps   10110] [G 0.0052238]
[2023-05-11 15:25:40.860425] [Steps   10120] [G 0.0048864]
[2023-05-11 15:25:57.565202] [Steps   10130] [G 0.0056186]
[2023-05-11 15:26:14.196552] [Steps   10140] [G 0.0050033]
[2023-05-11 15:26:30.794702] [Steps   10150] [G 0.0054723]
[2023-05-11 15:26:47.517097] [Steps   10160] [G 0.0062034]
[2023-05-11 15:27:04.116766] [Steps   10170] [G 0.0080124]
[2023-05-11 15:27:20.741080] [Steps   10180] [G 0.0080804]
[2023-05-11 15:27:37.342593] [Steps   10190] [G 0.0047719]
[2023-05-11 15:27:54.047822] [Steps   10200] [G 0.0047372]
[2023-05-11 15:28:10.677717] [Steps   10210] [G 0.0061422]
[2023-05-11 15:28:27.281902] [Steps   10220] [G 0.0038998]
[2023-05-11 15:28:44.002641] [Steps   10230] [G 0.0062930]
[2023-05-11 15:29:00.612018] [Steps   10240] [G 0.0055121]
[2023-05-11 15:29:17.240658] [Steps   10250] [G 0.0046184]
[2023-05-11 15:29:33.844516] [Steps   10260] [G 0.0053994]
[2023-05-11 15:29:52.508535] [Steps   10270] [G 0.0045209]
[2023-05-11 15:30:09.098198] [Steps   10280] [G 0.0060666]
[2023-05-11 15:30:25.693485] [Steps   10290] [G 0.0054444]
[2023-05-11 15:30:42.388495] [Steps   10300] [G 0.0048429]
[2023-05-11 15:30:59.007716] [Steps   10310] [G 0.0061456]
[2023-05-11 15:31:15.626098] [Steps   10320] [G 0.0048459]
[2023-05-11 15:31:32.335537] [Steps   10330] [G 0.0046532]
[2023-05-11 15:31:48.952776] [Steps   10340] [G 0.0062333]
[2023-05-11 15:32:05.563974] [Steps   10350] [G 0.0060237]
[2023-05-11 15:32:22.211837] [Steps   10360] [G 0.0055745]
[2023-05-11 15:32:38.910271] [Steps   10370] [G 0.0065387]
[2023-05-11 15:32:55.517729] [Steps   10380] [G 0.0046115]
[2023-05-11 15:33:12.123745] [Steps   10390] [G 0.0047950]
[2023-05-11 15:33:28.816925] [Steps   10400] [G 0.0046725]
[2023-05-11 15:33:45.428781] [Steps   10410] [G 0.0054902]
[2023-05-11 15:34:02.065759] [Steps   10420] [G 0.0052591]
[2023-05-11 15:34:18.689631] [Steps   10430] [G 0.0040993]
[2023-05-11 15:34:35.380981] [Steps   10440] [G 0.0033894]
[2023-05-11 15:34:52.007700] [Steps   10450] [G 0.0063683]
[2023-05-11 15:35:08.645544] [Steps   10460] [G 0.0049524]
[2023-05-11 15:35:25.341278] [Steps   10470] [G 0.0049202]
[2023-05-11 15:35:41.960012] [Steps   10480] [G 0.0078021]
[2023-05-11 15:35:58.583520] [Steps   10490] [G 0.0052998]
[2023-05-11 15:36:15.283492] [Steps   10500] [G 0.0054737]
[2023-05-11 15:36:31.877054] [Steps   10510] [G 0.0058638]
[2023-05-11 15:36:48.518965] [Steps   10520] [G 0.0059148]
[2023-05-11 15:37:05.140475] [Steps   10530] [G 0.0064548]
[2023-05-11 15:37:21.813726] [Steps   10540] [G 0.0046173]
[2023-05-11 15:37:38.430272] [Steps   10550] [G 0.0062170]
[2023-05-11 15:37:55.012525] [Steps   10560] [G 0.0047819]
[2023-05-11 15:38:11.622394] [Steps   10570] [G 0.0046563]
[2023-05-11 15:38:30.031009] [Steps   10580] [G 0.0056018]
[2023-05-11 15:38:46.601346] [Steps   10590] [G 0.0048510]
[2023-05-11 15:39:03.210537] [Steps   10600] [G 0.0042720]
[2023-05-11 15:39:19.888941] [Steps   10610] [G 0.0054080]
[2023-05-11 15:39:36.509303] [Steps   10620] [G 0.0037005]
[2023-05-11 15:39:53.127852] [Steps   10630] [G 0.0059460]
[2023-05-11 15:40:09.831581] [Steps   10640] [G 0.0051781]
[2023-05-11 15:40:26.447637] [Steps   10650] [G 0.0054210]
[2023-05-11 15:40:43.087105] [Steps   10660] [G 0.0054754]
[2023-05-11 15:40:59.785407] [Steps   10670] [G 0.0054422]
[2023-05-11 15:41:16.392368] [Steps   10680] [G 0.0057785]
[2023-05-11 15:41:32.990014] [Steps   10690] [G 0.0065072]
[2023-05-11 15:41:49.634060] [Steps   10700] [G 0.0048171]
[2023-05-11 15:42:06.318605] [Steps   10710] [G 0.0062812]
[2023-05-11 15:42:22.938839] [Steps   10720] [G 0.0054371]
[2023-05-11 15:42:39.530626] [Steps   10730] [G 0.0053600]
[2023-05-11 15:42:56.241454] [Steps   10740] [G 0.0034860]
[2023-05-11 15:43:12.858669] [Steps   10750] [G 0.0057260]
[2023-05-11 15:43:29.532475] [Steps   10760] [G 0.0059859]
[2023-05-11 15:43:46.188781] [Steps   10770] [G 0.0050879]
[2023-05-11 15:44:02.879943] [Steps   10780] [G 0.0060623]
[2023-05-11 15:44:19.496748] [Steps   10790] [G 0.0055872]
[2023-05-11 15:44:36.115828] [Steps   10800] [G 0.0050408]
[2023-05-11 15:44:52.817611] [Steps   10810] [G 0.0054005]
[2023-05-11 15:45:09.419244] [Steps   10820] [G 0.0046020]
[2023-05-11 15:45:26.033927] [Steps   10830] [G 0.0061846]
[2023-05-11 15:45:42.727322] [Steps   10840] [G 0.0046246]
[2023-05-11 15:45:59.452500] [Steps   10850] [G 0.0063504]
[2023-05-11 15:46:16.061250] [Steps   10860] [G 0.0056265]
[2023-05-11 15:46:32.613806] [Steps   10870] [G 0.0034757]
[2023-05-11 15:46:51.316578] [Steps   10880] [G 0.0041717]
[2023-05-11 15:47:08.021282] [Steps   10890] [G 0.0038394]
[2023-05-11 15:47:24.636749] [Steps   10900] [G 0.0043137]
[2023-05-11 15:47:41.340221] [Steps   10910] [G 0.0060504]
[2023-05-11 15:47:57.980015] [Steps   10920] [G 0.0058330]
[2023-05-11 15:48:14.597353] [Steps   10930] [G 0.0055540]
[2023-05-11 15:48:31.225829] [Steps   10940] [G 0.0055293]
[2023-05-11 15:48:47.920646] [Steps   10950] [G 0.0037219]
[2023-05-11 15:49:04.539644] [Steps   10960] [G 0.0049948]
[2023-05-11 15:49:21.180872] [Steps   10970] [G 0.0051560]
[2023-05-11 15:49:37.920281] [Steps   10980] [G 0.0038932]
[2023-05-11 15:49:54.553349] [Steps   10990] [G 0.0049089]
[2023-05-11 15:50:11.180518] [Steps   11000] [G 0.0052677]
Steps 11001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 15:50:29.217447] [Steps   11010] [G 0.0056683]
[2023-05-11 15:50:45.871289] [Steps   11020] [G 0.0071529]
[2023-05-11 15:51:02.552413] [Steps   11030] [G 0.0045873]
[2023-05-11 15:51:19.171140] [Steps   11040] [G 0.0063005]
[2023-05-11 15:51:35.806875] [Steps   11050] [G 0.0070481]
[2023-05-11 15:51:52.445648] [Steps   11060] [G 0.0063288]
[2023-05-11 15:52:09.126870] [Steps   11070] [G 0.0058940]
[2023-05-11 15:52:25.745621] [Steps   11080] [G 0.0054953]
[2023-05-11 15:52:42.385148] [Steps   11090] [G 0.0044605]
[2023-05-11 15:52:59.087231] [Steps   11100] [G 0.0067404]
[2023-05-11 15:53:15.746348] [Steps   11110] [G 0.0047316]
[2023-05-11 15:53:32.445719] [Steps   11120] [G 0.0043991]
[2023-05-11 15:53:49.155874] [Steps   11130] [G 0.0045768]
[2023-05-11 15:54:05.830655] [Steps   11140] [G 0.0050837]
[2023-05-11 15:54:22.467126] [Steps   11150] [G 0.0056979]
[2023-05-11 15:54:39.075781] [Steps   11160] [G 0.0054469]
[2023-05-11 15:54:55.739705] [Steps   11170] [G 0.0051253]
[2023-05-11 15:55:14.277486] [Steps   11180] [G 0.0054902]
[2023-05-11 15:55:30.883589] [Steps   11190] [G 0.0069393]
[2023-05-11 15:55:47.598391] [Steps   11200] [G 0.0052624]
[2023-05-11 15:56:04.259014] [Steps   11210] [G 0.0041364]
[2023-05-11 15:56:20.856520] [Steps   11220] [G 0.0054143]
[2023-05-11 15:56:37.487224] [Steps   11230] [G 0.0056496]
[2023-05-11 15:56:54.212034] [Steps   11240] [G 0.0048121]
[2023-05-11 15:57:10.869457] [Steps   11250] [G 0.0063199]
[2023-05-11 15:57:27.517062] [Steps   11260] [G 0.0059887]
[2023-05-11 15:57:44.230893] [Steps   11270] [G 0.0053030]
[2023-05-11 15:58:00.884836] [Steps   11280] [G 0.0036476]
[2023-05-11 15:58:17.516778] [Steps   11290] [G 0.0045789]
[2023-05-11 15:58:34.152668] [Steps   11300] [G 0.0052780]
[2023-05-11 15:58:50.840690] [Steps   11310] [G 0.0073038]
[2023-05-11 15:59:07.496281] [Steps   11320] [G 0.0053642]
[2023-05-11 15:59:24.126683] [Steps   11330] [G 0.0050185]
[2023-05-11 15:59:40.844712] [Steps   11340] [G 0.0040402]
[2023-05-11 15:59:57.487682] [Steps   11350] [G 0.0053090]
[2023-05-11 16:00:14.106601] [Steps   11360] [G 0.0059609]
[2023-05-11 16:00:30.834001] [Steps   11370] [G 0.0054535]
[2023-05-11 16:00:47.450149] [Steps   11380] [G 0.0049995]
[2023-05-11 16:01:04.098041] [Steps   11390] [G 0.0072006]
[2023-05-11 16:01:20.737071] [Steps   11400] [G 0.0070458]
[2023-05-11 16:01:37.541840] [Steps   11410] [G 0.0034348]
[2023-05-11 16:01:54.176409] [Steps   11420] [G 0.0060687]
[2023-05-11 16:02:10.824812] [Steps   11430] [G 0.0063304]
[2023-05-11 16:02:27.525196] [Steps   11440] [G 0.0042915]
[2023-05-11 16:02:44.160801] [Steps   11450] [G 0.0043581]
[2023-05-11 16:03:00.783879] [Steps   11460] [G 0.0042183]
[2023-05-11 16:03:17.401725] [Steps   11470] [G 0.0049227]
[2023-05-11 16:03:35.885879] [Steps   11480] [G 0.0045311]
[2023-05-11 16:03:52.497951] [Steps   11490] [G 0.0050927]
[2023-05-11 16:04:09.072278] [Steps   11500] [G 0.0050209]
[2023-05-11 16:04:25.754391] [Steps   11510] [G 0.0056419]
[2023-05-11 16:04:42.342263] [Steps   11520] [G 0.0051030]
[2023-05-11 16:04:58.966734] [Steps   11530] [G 0.0042632]
[2023-05-11 16:05:15.660893] [Steps   11540] [G 0.0044230]
[2023-05-11 16:05:32.272402] [Steps   11550] [G 0.0053676]
[2023-05-11 16:05:48.902419] [Steps   11560] [G 0.0050400]
[2023-05-11 16:06:05.530515] [Steps   11570] [G 0.0056922]
[2023-05-11 16:06:22.221371] [Steps   11580] [G 0.0057067]
[2023-05-11 16:06:38.851655] [Steps   11590] [G 0.0047442]
[2023-05-11 16:06:55.471551] [Steps   11600] [G 0.0043630]
[2023-05-11 16:07:12.164932] [Steps   11610] [G 0.0055850]
[2023-05-11 16:07:28.793218] [Steps   11620] [G 0.0047346]
[2023-05-11 16:07:45.435515] [Steps   11630] [G 0.0058173]
[2023-05-11 16:08:02.073823] [Steps   11640] [G 0.0049617]
[2023-05-11 16:08:18.777868] [Steps   11650] [G 0.0056135]
[2023-05-11 16:08:35.401792] [Steps   11660] [G 0.0050671]
[2023-05-11 16:08:52.019219] [Steps   11670] [G 0.0052296]
[2023-05-11 16:09:08.746587] [Steps   11680] [G 0.0065411]
[2023-05-11 16:09:25.351878] [Steps   11690] [G 0.0045127]
[2023-05-11 16:09:41.987289] [Steps   11700] [G 0.0049590]
[2023-05-11 16:09:58.662100] [Steps   11710] [G 0.0062405]
[2023-05-11 16:10:15.315359] [Steps   11720] [G 0.0047407]
[2023-05-11 16:10:31.932189] [Steps   11730] [G 0.0055823]
[2023-05-11 16:10:48.544667] [Steps   11740] [G 0.0056041]
[2023-05-11 16:11:05.244174] [Steps   11750] [G 0.0054619]
[2023-05-11 16:11:21.886127] [Steps   11760] [G 0.0069914]
[2023-05-11 16:11:38.507613] [Steps   11770] [G 0.0039483]
[2023-05-11 16:11:56.952391] [Steps   11780] [G 0.0048905]
[2023-05-11 16:12:13.546343] [Steps   11790] [G 0.0041531]
[2023-05-11 16:12:30.182799] [Steps   11800] [G 0.0043400]
[2023-05-11 16:12:46.818140] [Steps   11810] [G 0.0048044]
[2023-05-11 16:13:03.523893] [Steps   11820] [G 0.0056353]
[2023-05-11 16:13:20.189783] [Steps   11830] [G 0.0064837]
[2023-05-11 16:13:36.871079] [Steps   11840] [G 0.0066030]
[2023-05-11 16:13:53.605366] [Steps   11850] [G 0.0053563]
[2023-05-11 16:14:10.234129] [Steps   11860] [G 0.0038053]
[2023-05-11 16:14:26.883454] [Steps   11870] [G 0.0054831]
[2023-05-11 16:14:43.607498] [Steps   11880] [G 0.0050158]
[2023-05-11 16:15:00.266408] [Steps   11890] [G 0.0045212]
[2023-05-11 16:15:16.899208] [Steps   11900] [G 0.0070092]
[2023-05-11 16:15:33.582180] [Steps   11910] [G 0.0066339]
[2023-05-11 16:15:50.286140] [Steps   11920] [G 0.0046691]
[2023-05-11 16:16:06.973925] [Steps   11930] [G 0.0057240]
[2023-05-11 16:16:23.634321] [Steps   11940] [G 0.0043720]
[2023-05-11 16:16:40.337414] [Steps   11950] [G 0.0042753]
[2023-05-11 16:16:57.029155] [Steps   11960] [G 0.0053724]
[2023-05-11 16:17:13.690990] [Steps   11970] [G 0.0064002]
[2023-05-11 16:17:30.337818] [Steps   11980] [G 0.0034046]
[2023-05-11 16:17:47.054739] [Steps   11990] [G 0.0050564]
[2023-05-11 16:18:03.726536] [Steps   12000] [G 0.0052238]
Steps 12001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 16:18:21.828488] [Steps   12010] [G 0.0053220]
[2023-05-11 16:18:38.558076] [Steps   12020] [G 0.0052143]
[2023-05-11 16:18:55.245540] [Steps   12030] [G 0.0040943]
[2023-05-11 16:19:11.957191] [Steps   12040] [G 0.0055881]
[2023-05-11 16:19:28.612232] [Steps   12050] [G 0.0045418]
[2023-05-11 16:19:45.336662] [Steps   12060] [G 0.0039023]
[2023-05-11 16:20:02.045784] [Steps   12070] [G 0.0067113]
[2023-05-11 16:20:18.648074] [Steps   12080] [G 0.0052965]
[2023-05-11 16:20:37.300040] [Steps   12090] [G 0.0039689]
[2023-05-11 16:20:53.874974] [Steps   12100] [G 0.0047552]
[2023-05-11 16:21:10.584459] [Steps   12110] [G 0.0057619]
[2023-05-11 16:21:27.290211] [Steps   12120] [G 0.0061353]
[2023-05-11 16:21:43.901129] [Steps   12130] [G 0.0044822]
[2023-05-11 16:22:00.606049] [Steps   12140] [G 0.0058654]
[2023-05-11 16:22:17.213280] [Steps   12150] [G 0.0047751]
[2023-05-11 16:22:33.884924] [Steps   12160] [G 0.0061807]
[2023-05-11 16:22:50.589321] [Steps   12170] [G 0.0050741]
[2023-05-11 16:23:07.211638] [Steps   12180] [G 0.0034518]
[2023-05-11 16:23:23.918704] [Steps   12190] [G 0.0047881]
[2023-05-11 16:23:40.532323] [Steps   12200] [G 0.0041565]
[2023-05-11 16:23:57.273736] [Steps   12210] [G 0.0061744]
[2023-05-11 16:24:13.889728] [Steps   12220] [G 0.0055226]
[2023-05-11 16:24:30.589842] [Steps   12230] [G 0.0055778]
[2023-05-11 16:24:47.282423] [Steps   12240] [G 0.0050243]
[2023-05-11 16:25:03.926832] [Steps   12250] [G 0.0043729]
[2023-05-11 16:25:20.636816] [Steps   12260] [G 0.0050185]
[2023-05-11 16:25:37.219201] [Steps   12270] [G 0.0050684]
[2023-05-11 16:25:53.925786] [Steps   12280] [G 0.0046431]
[2023-05-11 16:26:10.626788] [Steps   12290] [G 0.0063636]
[2023-05-11 16:26:27.236745] [Steps   12300] [G 0.0062855]
[2023-05-11 16:26:43.930460] [Steps   12310] [G 0.0046291]
[2023-05-11 16:27:00.529103] [Steps   12320] [G 0.0074425]
[2023-05-11 16:27:17.244930] [Steps   12330] [G 0.0045483]
[2023-05-11 16:27:33.923310] [Steps   12340] [G 0.0041761]
[2023-05-11 16:27:50.539370] [Steps   12350] [G 0.0038350]
[2023-05-11 16:28:07.230435] [Steps   12360] [G 0.0050700]
[2023-05-11 16:28:23.821147] [Steps   12370] [G 0.0055201]
[2023-05-11 16:28:40.467036] [Steps   12380] [G 0.0053067]
[2023-05-11 16:28:59.076843] [Steps   12390] [G 0.0037568]
[2023-05-11 16:29:15.755451] [Steps   12400] [G 0.0039487]
[2023-05-11 16:29:32.418481] [Steps   12410] [G 0.0051320]
[2023-05-11 16:29:49.022983] [Steps   12420] [G 0.0048462]
[2023-05-11 16:30:05.683401] [Steps   12430] [G 0.0034767]
[2023-05-11 16:30:22.274748] [Steps   12440] [G 0.0062412]
[2023-05-11 16:30:38.948321] [Steps   12450] [G 0.0035317]
[2023-05-11 16:30:55.608506] [Steps   12460] [G 0.0049702]
[2023-05-11 16:31:12.202231] [Steps   12470] [G 0.0066782]
[2023-05-11 16:31:28.851872] [Steps   12480] [G 0.0054761]
[2023-05-11 16:31:45.455416] [Steps   12490] [G 0.0039776]
[2023-05-11 16:32:02.126315] [Steps   12500] [G 0.0044584]
[2023-05-11 16:32:18.774531] [Steps   12510] [G 0.0045837]
[2023-05-11 16:32:35.361473] [Steps   12520] [G 0.0039414]
[2023-05-11 16:32:52.022950] [Steps   12530] [G 0.0040574]
[2023-05-11 16:33:08.614567] [Steps   12540] [G 0.0055533]
[2023-05-11 16:33:25.294709] [Steps   12550] [G 0.0062615]
[2023-05-11 16:33:41.875131] [Steps   12560] [G 0.0058243]
[2023-05-11 16:33:58.525902] [Steps   12570] [G 0.0056511]
[2023-05-11 16:34:15.172274] [Steps   12580] [G 0.0057350]
[2023-05-11 16:34:31.785549] [Steps   12590] [G 0.0054817]
[2023-05-11 16:34:48.487252] [Steps   12600] [G 0.0040842]
[2023-05-11 16:35:05.077597] [Steps   12610] [G 0.0051082]
[2023-05-11 16:35:21.720834] [Steps   12620] [G 0.0053511]
[2023-05-11 16:35:38.384632] [Steps   12630] [G 0.0061055]
[2023-05-11 16:35:54.963988] [Steps   12640] [G 0.0064439]
[2023-05-11 16:36:11.619268] [Steps   12650] [G 0.0047597]
[2023-05-11 16:36:28.211000] [Steps   12660] [G 0.0040533]
[2023-05-11 16:36:44.864213] [Steps   12670] [G 0.0058091]
[2023-05-11 16:37:01.485939] [Steps   12680] [G 0.0049091]
[2023-05-11 16:37:19.955668] [Steps   12690] [G 0.0060421]
[2023-05-11 16:37:36.654825] [Steps   12700] [G 0.0054084]
[2023-05-11 16:37:53.267289] [Steps   12710] [G 0.0037401]
[2023-05-11 16:38:09.981317] [Steps   12720] [G 0.0048730]
[2023-05-11 16:38:26.616989] [Steps   12730] [G 0.0031913]
[2023-05-11 16:38:43.289560] [Steps   12740] [G 0.0049595]
[2023-05-11 16:38:59.981102] [Steps   12750] [G 0.0043333]
[2023-05-11 16:39:16.600156] [Steps   12760] [G 0.0042561]
[2023-05-11 16:39:33.292356] [Steps   12770] [G 0.0053104]
[2023-05-11 16:39:49.908692] [Steps   12780] [G 0.0041115]
[2023-05-11 16:40:06.599793] [Steps   12790] [G 0.0051990]
[2023-05-11 16:40:23.291095] [Steps   12800] [G 0.0049434]
[2023-05-11 16:40:39.911054] [Steps   12810] [G 0.0046983]
[2023-05-11 16:40:56.601120] [Steps   12820] [G 0.0049881]
[2023-05-11 16:41:13.216931] [Steps   12830] [G 0.0048222]
[2023-05-11 16:41:29.909526] [Steps   12840] [G 0.0043914]
[2023-05-11 16:41:46.597830] [Steps   12850] [G 0.0047531]
[2023-05-11 16:42:03.258892] [Steps   12860] [G 0.0049353]
[2023-05-11 16:42:19.985363] [Steps   12870] [G 0.0052613]
[2023-05-11 16:42:36.608810] [Steps   12880] [G 0.0055304]
[2023-05-11 16:42:53.327053] [Steps   12890] [G 0.0048235]
[2023-05-11 16:43:09.975052] [Steps   12900] [G 0.0060839]
[2023-05-11 16:43:26.688079] [Steps   12910] [G 0.0051291]
[2023-05-11 16:43:43.368277] [Steps   12920] [G 0.0050184]
[2023-05-11 16:43:59.969235] [Steps   12930] [G 0.0042673]
[2023-05-11 16:44:16.652927] [Steps   12940] [G 0.0030890]
[2023-05-11 16:44:33.256250] [Steps   12950] [G 0.0058127]
[2023-05-11 16:44:49.942089] [Steps   12960] [G 0.0047428]
[2023-05-11 16:45:06.662256] [Steps   12970] [G 0.0050832]
[2023-05-11 16:45:23.259811] [Steps   12980] [G 0.0045288]
[2023-05-11 16:45:41.701853] [Steps   12990] [G 0.0057201]
[2023-05-11 16:45:58.300349] [Steps   13000] [G 0.0036816]
Steps 13001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 16:46:16.504554] [Steps   13010] [G 0.0057487]
[2023-05-11 16:46:33.192548] [Steps   13020] [G 0.0050298]
[2023-05-11 16:46:49.812355] [Steps   13030] [G 0.0042876]
[2023-05-11 16:47:06.496991] [Steps   13040] [G 0.0044590]
[2023-05-11 16:47:23.134383] [Steps   13050] [G 0.0049362]
[2023-05-11 16:47:39.759950] [Steps   13060] [G 0.0037713]
[2023-05-11 16:47:56.461369] [Steps   13070] [G 0.0035209]
[2023-05-11 16:48:13.096402] [Steps   13080] [G 0.0052601]
[2023-05-11 16:48:29.720644] [Steps   13090] [G 0.0046641]
[2023-05-11 16:48:46.364723] [Steps   13100] [G 0.0045566]
[2023-05-11 16:49:03.075249] [Steps   13110] [G 0.0045785]
[2023-05-11 16:49:19.689266] [Steps   13120] [G 0.0051622]
[2023-05-11 16:49:36.317005] [Steps   13130] [G 0.0052372]
[2023-05-11 16:49:52.998797] [Steps   13140] [G 0.0054241]
[2023-05-11 16:50:09.643153] [Steps   13150] [G 0.0049490]
[2023-05-11 16:50:26.267122] [Steps   13160] [G 0.0075130]
[2023-05-11 16:50:42.889505] [Steps   13170] [G 0.0039760]
[2023-05-11 16:50:59.588220] [Steps   13180] [G 0.0051501]
[2023-05-11 16:51:16.236151] [Steps   13190] [G 0.0057639]
[2023-05-11 16:51:32.856924] [Steps   13200] [G 0.0047538]
[2023-05-11 16:51:49.545491] [Steps   13210] [G 0.0037350]
[2023-05-11 16:52:06.186213] [Steps   13220] [G 0.0059826]
[2023-05-11 16:52:22.835989] [Steps   13230] [G 0.0051876]
[2023-05-11 16:52:39.542702] [Steps   13240] [G 0.0044589]
[2023-05-11 16:52:56.150357] [Steps   13250] [G 0.0047938]
[2023-05-11 16:53:12.792294] [Steps   13260] [G 0.0084485]
[2023-05-11 16:53:29.432098] [Steps   13270] [G 0.0039310]
[2023-05-11 16:53:46.079141] [Steps   13280] [G 0.0066480]
[2023-05-11 16:54:04.649349] [Steps   13290] [G 0.0051863]
[2023-05-11 16:54:21.222354] [Steps   13300] [G 0.0048519]
[2023-05-11 16:54:37.911598] [Steps   13310] [G 0.0047050]
[2023-05-11 16:54:54.518475] [Steps   13320] [G 0.0054124]
[2023-05-11 16:55:11.144817] [Steps   13330] [G 0.0042076]
[2023-05-11 16:55:27.758520] [Steps   13340] [G 0.0032384]
[2023-05-11 16:55:44.449448] [Steps   13350] [G 0.0049690]
[2023-05-11 16:56:01.048736] [Steps   13360] [G 0.0041591]
[2023-05-11 16:56:17.660311] [Steps   13370] [G 0.0043156]
[2023-05-11 16:56:34.360474] [Steps   13380] [G 0.0042841]
[2023-05-11 16:56:50.967464] [Steps   13390] [G 0.0042569]
[2023-05-11 16:57:07.599639] [Steps   13400] [G 0.0045420]
[2023-05-11 16:57:24.304631] [Steps   13410] [G 0.0042542]
[2023-05-11 16:57:40.917434] [Steps   13420] [G 0.0061398]
[2023-05-11 16:57:57.527353] [Steps   13430] [G 0.0045691]
[2023-05-11 16:58:14.139808] [Steps   13440] [G 0.0044411]
[2023-05-11 16:58:30.848473] [Steps   13450] [G 0.0041654]
[2023-05-11 16:58:47.487694] [Steps   13460] [G 0.0047126]
[2023-05-11 16:59:04.097013] [Steps   13470] [G 0.0039236]
[2023-05-11 16:59:20.794560] [Steps   13480] [G 0.0043719]
[2023-05-11 16:59:37.417270] [Steps   13490] [G 0.0052318]
[2023-05-11 16:59:54.045544] [Steps   13500] [G 0.0058711]
[2023-05-11 17:00:10.663177] [Steps   13510] [G 0.0039436]
[2023-05-11 17:00:27.376907] [Steps   13520] [G 0.0048963]
[2023-05-11 17:00:43.996990] [Steps   13530] [G 0.0037461]
[2023-05-11 17:01:00.605019] [Steps   13540] [G 0.0045792]
[2023-05-11 17:01:17.256257] [Steps   13550] [G 0.0054071]
[2023-05-11 17:01:33.870939] [Steps   13560] [G 0.0044256]
[2023-05-11 17:01:50.521316] [Steps   13570] [G 0.0054476]
[2023-05-11 17:02:07.206099] [Steps   13580] [G 0.0055789]
[2023-05-11 17:02:23.774885] [Steps   13590] [G 0.0047620]
[2023-05-11 17:02:42.526953] [Steps   13600] [G 0.0041684]
[2023-05-11 17:02:59.110821] [Steps   13610] [G 0.0029667]
[2023-05-11 17:03:15.784762] [Steps   13620] [G 0.0046356]
[2023-05-11 17:03:32.408081] [Steps   13630] [G 0.0053785]
[2023-05-11 17:03:49.035588] [Steps   13640] [G 0.0036617]
[2023-05-11 17:04:05.823393] [Steps   13650] [G 0.0062268]
[2023-05-11 17:04:22.448155] [Steps   13660] [G 0.0040640]
[2023-05-11 17:04:39.055099] [Steps   13670] [G 0.0053382]
[2023-05-11 17:04:55.676661] [Steps   13680] [G 0.0039994]
[2023-05-11 17:05:12.369177] [Steps   13690] [G 0.0062188]
[2023-05-11 17:05:28.991228] [Steps   13700] [G 0.0033219]
[2023-05-11 17:05:45.625423] [Steps   13710] [G 0.0051778]
[2023-05-11 17:06:02.329689] [Steps   13720] [G 0.0049731]
[2023-05-11 17:06:18.936857] [Steps   13730] [G 0.0058145]
[2023-05-11 17:06:35.550424] [Steps   13740] [G 0.0038895]
[2023-05-11 17:06:52.181318] [Steps   13750] [G 0.0039731]
[2023-05-11 17:07:08.850620] [Steps   13760] [G 0.0049345]
[2023-05-11 17:07:25.443182] [Steps   13770] [G 0.0056145]
[2023-05-11 17:07:42.059268] [Steps   13780] [G 0.0057211]
[2023-05-11 17:07:58.765483] [Steps   13790] [G 0.0039460]
[2023-05-11 17:08:15.387138] [Steps   13800] [G 0.0044019]
[2023-05-11 17:08:32.004980] [Steps   13810] [G 0.0044740]
[2023-05-11 17:08:48.708355] [Steps   13820] [G 0.0060280]
[2023-05-11 17:09:05.331329] [Steps   13830] [G 0.0040079]
[2023-05-11 17:09:21.947953] [Steps   13840] [G 0.0037960]
[2023-05-11 17:09:38.562470] [Steps   13850] [G 0.0067239]
[2023-05-11 17:09:55.392293] [Steps   13860] [G 0.0065003]
[2023-05-11 17:10:12.101774] [Steps   13870] [G 0.0059674]
[2023-05-11 17:10:28.694747] [Steps   13880] [G 0.0041431]
[2023-05-11 17:10:45.349766] [Steps   13890] [G 0.0049898]
[2023-05-11 17:11:03.856494] [Steps   13900] [G 0.0047141]
[2023-05-11 17:11:20.488604] [Steps   13910] [G 0.0043935]
[2023-05-11 17:11:37.137569] [Steps   13920] [G 0.0038960]
[2023-05-11 17:11:53.904543] [Steps   13930] [G 0.0044453]
[2023-05-11 17:12:10.541409] [Steps   13940] [G 0.0050170]
[2023-05-11 17:12:27.155398] [Steps   13950] [G 0.0043275]
[2023-05-11 17:12:43.877490] [Steps   13960] [G 0.0048826]
[2023-05-11 17:13:00.500455] [Steps   13970] [G 0.0040040]
[2023-05-11 17:13:17.117690] [Steps   13980] [G 0.0032086]
[2023-05-11 17:13:33.831550] [Steps   13990] [G 0.0046949]
[2023-05-11 17:13:50.466653] [Steps   14000] [G 0.0040205]
Steps 14001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 17:14:08.579440] [Steps   14010] [G 0.0046468]
[2023-05-11 17:14:25.183042] [Steps   14020] [G 0.0047397]
[2023-05-11 17:14:41.895854] [Steps   14030] [G 0.0042747]
[2023-05-11 17:14:58.520911] [Steps   14040] [G 0.0041477]
[2023-05-11 17:15:15.187453] [Steps   14050] [G 0.0043214]
[2023-05-11 17:15:31.890444] [Steps   14060] [G 0.0042566]
[2023-05-11 17:15:48.514150] [Steps   14070] [G 0.0043249]
[2023-05-11 17:16:05.217872] [Steps   14080] [G 0.0050166]
[2023-05-11 17:16:21.832016] [Steps   14090] [G 0.0037870]
[2023-05-11 17:16:38.522259] [Steps   14100] [G 0.0044630]
[2023-05-11 17:16:55.194913] [Steps   14110] [G 0.0060690]
[2023-05-11 17:17:11.842473] [Steps   14120] [G 0.0060963]
[2023-05-11 17:17:28.520572] [Steps   14130] [G 0.0042618]
[2023-05-11 17:17:45.132150] [Steps   14140] [G 0.0044535]
[2023-05-11 17:18:01.891885] [Steps   14150] [G 0.0044429]
[2023-05-11 17:18:18.496813] [Steps   14160] [G 0.0045780]
[2023-05-11 17:18:35.227873] [Steps   14170] [G 0.0047424]
[2023-05-11 17:18:51.887950] [Steps   14180] [G 0.0039907]
[2023-05-11 17:19:08.499618] [Steps   14190] [G 0.0062780]
[2023-05-11 17:19:27.148118] [Steps   14200] [G 0.0045255]
[2023-05-11 17:19:43.747708] [Steps   14210] [G 0.0038432]
[2023-05-11 17:20:00.464401] [Steps   14220] [G 0.0049587]
[2023-05-11 17:20:17.162464] [Steps   14230] [G 0.0050906]
[2023-05-11 17:20:33.799908] [Steps   14240] [G 0.0036877]
[2023-05-11 17:20:50.525581] [Steps   14250] [G 0.0034567]
[2023-05-11 17:21:07.180459] [Steps   14260] [G 0.0044755]
[2023-05-11 17:21:23.891666] [Steps   14270] [G 0.0038760]
[2023-05-11 17:21:40.607062] [Steps   14280] [G 0.0047356]
[2023-05-11 17:21:57.308920] [Steps   14290] [G 0.0034008]
[2023-05-11 17:22:14.004669] [Steps   14300] [G 0.0050866]
[2023-05-11 17:22:30.660292] [Steps   14310] [G 0.0054072]
[2023-05-11 17:22:47.378193] [Steps   14320] [G 0.0057468]
[2023-05-11 17:23:04.005708] [Steps   14330] [G 0.0036044]
[2023-05-11 17:23:20.740494] [Steps   14340] [G 0.0047033]
[2023-05-11 17:23:37.441213] [Steps   14350] [G 0.0062539]
[2023-05-11 17:23:54.071711] [Steps   14360] [G 0.0040607]
[2023-05-11 17:24:10.781061] [Steps   14370] [G 0.0054316]
[2023-05-11 17:24:27.429275] [Steps   14380] [G 0.0038366]
[2023-05-11 17:24:44.147701] [Steps   14390] [G 0.0066044]
[2023-05-11 17:25:00.911555] [Steps   14400] [G 0.0058678]
[2023-05-11 17:25:17.570146] [Steps   14410] [G 0.0061475]
[2023-05-11 17:25:34.270704] [Steps   14420] [G 0.0049131]
[2023-05-11 17:25:50.920062] [Steps   14430] [G 0.0043581]
[2023-05-11 17:26:07.607048] [Steps   14440] [G 0.0046022]
[2023-05-11 17:26:24.346697] [Steps   14450] [G 0.0067032]
[2023-05-11 17:26:40.994158] [Steps   14460] [G 0.0049460]
[2023-05-11 17:26:57.712992] [Steps   14470] [G 0.0043215]
[2023-05-11 17:27:14.361220] [Steps   14480] [G 0.0042260]
[2023-05-11 17:27:31.040219] [Steps   14490] [G 0.0036047]
[2023-05-11 17:27:49.652344] [Steps   14500] [G 0.0054810]
[2023-05-11 17:28:06.341391] [Steps   14510] [G 0.0047117]
[2023-05-11 17:28:23.028314] [Steps   14520] [G 0.0058467]
[2023-05-11 17:28:39.632997] [Steps   14530] [G 0.0037618]
[2023-05-11 17:28:56.247591] [Steps   14540] [G 0.0032878]
[2023-05-11 17:29:12.884763] [Steps   14550] [G 0.0043689]
[2023-05-11 17:29:29.581458] [Steps   14560] [G 0.0048109]
[2023-05-11 17:29:46.197046] [Steps   14570] [G 0.0038639]
[2023-05-11 17:30:02.808306] [Steps   14580] [G 0.0051301]
[2023-05-11 17:30:19.511470] [Steps   14590] [G 0.0040360]
[2023-05-11 17:30:36.116569] [Steps   14600] [G 0.0040821]
[2023-05-11 17:30:52.732663] [Steps   14610] [G 0.0052382]
[2023-05-11 17:31:09.432091] [Steps   14620] [G 0.0061319]
[2023-05-11 17:31:26.058262] [Steps   14630] [G 0.0043731]
[2023-05-11 17:31:42.693704] [Steps   14640] [G 0.0049536]
[2023-05-11 17:31:59.318468] [Steps   14650] [G 0.0051171]
[2023-05-11 17:32:16.041629] [Steps   14660] [G 0.0038521]
[2023-05-11 17:32:32.649181] [Steps   14670] [G 0.0041857]
[2023-05-11 17:32:49.267016] [Steps   14680] [G 0.0042212]
[2023-05-11 17:33:05.985071] [Steps   14690] [G 0.0043040]
[2023-05-11 17:33:22.621188] [Steps   14700] [G 0.0060878]
[2023-05-11 17:33:39.275332] [Steps   14710] [G 0.0052612]
[2023-05-11 17:33:55.895176] [Steps   14720] [G 0.0037184]
[2023-05-11 17:34:12.577061] [Steps   14730] [G 0.0045994]
[2023-05-11 17:34:29.222553] [Steps   14740] [G 0.0054420]
[2023-05-11 17:34:45.840929] [Steps   14750] [G 0.0059786]
[2023-05-11 17:35:02.513620] [Steps   14760] [G 0.0040484]
[2023-05-11 17:35:19.130955] [Steps   14770] [G 0.0056127]
[2023-05-11 17:35:35.832866] [Steps   14780] [G 0.0050575]
[2023-05-11 17:35:52.480581] [Steps   14790] [G 0.0062029]
[2023-05-11 17:36:10.977823] [Steps   14800] [G 0.0053770]
[2023-05-11 17:36:27.556507] [Steps   14810] [G 0.0059823]
[2023-05-11 17:36:44.136099] [Steps   14820] [G 0.0035352]
[2023-05-11 17:37:00.839510] [Steps   14830] [G 0.0038333]
[2023-05-11 17:37:17.442685] [Steps   14840] [G 0.0041245]
[2023-05-11 17:37:34.162181] [Steps   14850] [G 0.0041848]
[2023-05-11 17:37:50.816786] [Steps   14860] [G 0.0047771]
[2023-05-11 17:38:07.437095] [Steps   14870] [G 0.0037976]
[2023-05-11 17:38:24.048497] [Steps   14880] [G 0.0047881]
[2023-05-11 17:38:40.659073] [Steps   14890] [G 0.0035155]
[2023-05-11 17:38:57.360642] [Steps   14900] [G 0.0040691]
[2023-05-11 17:39:13.968229] [Steps   14910] [G 0.0037785]
[2023-05-11 17:39:30.596178] [Steps   14920] [G 0.0041816]
[2023-05-11 17:39:47.313802] [Steps   14930] [G 0.0062530]
[2023-05-11 17:40:03.934503] [Steps   14940] [G 0.0035025]
[2023-05-11 17:40:20.561761] [Steps   14950] [G 0.0042222]
[2023-05-11 17:40:37.306325] [Steps   14960] [G 0.0069093]
[2023-05-11 17:40:53.993304] [Steps   14970] [G 0.0031002]
[2023-05-11 17:41:10.601816] [Steps   14980] [G 0.0050315]
[2023-05-11 17:41:27.230043] [Steps   14990] [G 0.0041462]
[2023-05-11 17:41:43.922324] [Steps   15000] [G 0.0035686]
Steps 15001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 17:42:01.944083] [Steps   15010] [G 0.0037411]
[2023-05-11 17:42:18.588531] [Steps   15020] [G 0.0064824]
[2023-05-11 17:42:35.194587] [Steps   15030] [G 0.0041352]
[2023-05-11 17:42:51.828212] [Steps   15040] [G 0.0039494]
[2023-05-11 17:43:08.511421] [Steps   15050] [G 0.0038768]
[2023-05-11 17:43:25.117409] [Steps   15060] [G 0.0036034]
[2023-05-11 17:43:41.706636] [Steps   15070] [G 0.0059128]
[2023-05-11 17:43:58.366718] [Steps   15080] [G 0.0048557]
[2023-05-11 17:44:14.960616] [Steps   15090] [G 0.0059277]
[2023-05-11 17:44:31.522887] [Steps   15100] [G 0.0053079]
[2023-05-11 17:44:50.083081] [Steps   15110] [G 0.0059522]
[2023-05-11 17:45:06.742999] [Steps   15120] [G 0.0026349]
[2023-05-11 17:45:23.403683] [Steps   15130] [G 0.0036131]
[2023-05-11 17:45:40.011507] [Steps   15140] [G 0.0052516]
[2023-05-11 17:45:56.702743] [Steps   15150] [G 0.0037591]
[2023-05-11 17:46:13.321881] [Steps   15160] [G 0.0049486]
[2023-05-11 17:46:29.929568] [Steps   15170] [G 0.0032233]
[2023-05-11 17:46:46.547947] [Steps   15180] [G 0.0032055]
[2023-05-11 17:47:03.236346] [Steps   15190] [G 0.0035939]
[2023-05-11 17:47:19.859870] [Steps   15200] [G 0.0045929]
[2023-05-11 17:47:36.450980] [Steps   15210] [G 0.0031819]
[2023-05-11 17:47:53.128810] [Steps   15220] [G 0.0050088]
[2023-05-11 17:48:09.730529] [Steps   15230] [G 0.0036877]
[2023-05-11 17:48:26.448820] [Steps   15240] [G 0.0036671]
[2023-05-11 17:48:43.155879] [Steps   15250] [G 0.0044605]
[2023-05-11 17:48:59.764486] [Steps   15260] [G 0.0035121]
[2023-05-11 17:49:16.401280] [Steps   15270] [G 0.0048777]
[2023-05-11 17:49:33.020992] [Steps   15280] [G 0.0048027]
[2023-05-11 17:49:49.703170] [Steps   15290] [G 0.0067683]
[2023-05-11 17:50:06.314821] [Steps   15300] [G 0.0057691]
[2023-05-11 17:50:22.921592] [Steps   15310] [G 0.0058553]
[2023-05-11 17:50:39.631555] [Steps   15320] [G 0.0037531]
[2023-05-11 17:50:56.263405] [Steps   15330] [G 0.0046113]
[2023-05-11 17:51:12.882901] [Steps   15340] [G 0.0038639]
[2023-05-11 17:51:29.494560] [Steps   15350] [G 0.0047803]
[2023-05-11 17:51:46.200585] [Steps   15360] [G 0.0052418]
[2023-05-11 17:52:02.815800] [Steps   15370] [G 0.0059346]
[2023-05-11 17:52:19.423359] [Steps   15380] [G 0.0048761]
[2023-05-11 17:52:36.099117] [Steps   15390] [G 0.0052121]
[2023-05-11 17:52:52.681199] [Steps   15400] [G 0.0042217]
[2023-05-11 17:53:11.109769] [Steps   15410] [G 0.0036120]
[2023-05-11 17:53:27.800949] [Steps   15420] [G 0.0035705]
[2023-05-11 17:53:44.430333] [Steps   15430] [G 0.0041438]
[2023-05-11 17:54:01.042630] [Steps   15440] [G 0.0056603]
[2023-05-11 17:54:17.698517] [Steps   15450] [G 0.0034914]
[2023-05-11 17:54:34.416345] [Steps   15460] [G 0.0034039]
[2023-05-11 17:54:51.046088] [Steps   15470] [G 0.0044822]
[2023-05-11 17:55:07.644105] [Steps   15480] [G 0.0046495]
[2023-05-11 17:55:24.335077] [Steps   15490] [G 0.0043520]
[2023-05-11 17:55:40.971171] [Steps   15500] [G 0.0050473]
[2023-05-11 17:55:57.555769] [Steps   15510] [G 0.0048650]
[2023-05-11 17:56:14.165051] [Steps   15520] [G 0.0038042]
[2023-05-11 17:56:30.851113] [Steps   15530] [G 0.0040282]
[2023-05-11 17:56:47.494564] [Steps   15540] [G 0.0036705]
[2023-05-11 17:57:04.109391] [Steps   15550] [G 0.0037861]
[2023-05-11 17:57:20.756167] [Steps   15560] [G 0.0037753]
[2023-05-11 17:57:37.373066] [Steps   15570] [G 0.0054950]
[2023-05-11 17:57:53.995860] [Steps   15580] [G 0.0048364]
[2023-05-11 17:58:10.659297] [Steps   15590] [G 0.0037312]
[2023-05-11 17:58:27.251643] [Steps   15600] [G 0.0045752]
[2023-05-11 17:58:43.898261] [Steps   15610] [G 0.0051137]
[2023-05-11 17:59:00.519091] [Steps   15620] [G 0.0058695]
[2023-05-11 17:59:17.181200] [Steps   15630] [G 0.0039499]
[2023-05-11 17:59:33.792144] [Steps   15640] [G 0.0051841]
[2023-05-11 17:59:50.398021] [Steps   15650] [G 0.0043998]
[2023-05-11 18:00:07.079589] [Steps   15660] [G 0.0044837]
[2023-05-11 18:00:23.677903] [Steps   15670] [G 0.0045806]
[2023-05-11 18:00:40.287831] [Steps   15680] [G 0.0034826]
[2023-05-11 18:00:56.892620] [Steps   15690] [G 0.0042636]
[2023-05-11 18:01:13.550725] [Steps   15700] [G 0.0047123]
[2023-05-11 18:01:32.163399] [Steps   15710] [G 0.0041115]
[2023-05-11 18:01:48.750691] [Steps   15720] [G 0.0045459]
[2023-05-11 18:02:05.398373] [Steps   15730] [G 0.0042556]
[2023-05-11 18:02:22.029373] [Steps   15740] [G 0.0038734]
[2023-05-11 18:02:38.649905] [Steps   15750] [G 0.0042778]
[2023-05-11 18:02:55.281841] [Steps   15760] [G 0.0031552]
[2023-05-11 18:03:12.016700] [Steps   15770] [G 0.0048719]
[2023-05-11 18:03:28.650210] [Steps   15780] [G 0.0044346]
[2023-05-11 18:03:45.246886] [Steps   15790] [G 0.0050674]
[2023-05-11 18:04:01.898768] [Steps   15800] [G 0.0060085]
[2023-05-11 18:04:18.501591] [Steps   15810] [G 0.0036292]
[2023-05-11 18:04:35.086572] [Steps   15820] [G 0.0046076]
[2023-05-11 18:04:51.754105] [Steps   15830] [G 0.0043803]
[2023-05-11 18:05:08.351088] [Steps   15840] [G 0.0037425]
[2023-05-11 18:05:24.947424] [Steps   15850] [G 0.0036495]
[2023-05-11 18:05:41.532382] [Steps   15860] [G 0.0037855]
[2023-05-11 18:05:58.179169] [Steps   15870] [G 0.0049147]
[2023-05-11 18:06:14.809826] [Steps   15880] [G 0.0052057]
[2023-05-11 18:06:31.398252] [Steps   15890] [G 0.0035257]
[2023-05-11 18:06:48.067624] [Steps   15900] [G 0.0054260]
[2023-05-11 18:07:04.657165] [Steps   15910] [G 0.0034944]
[2023-05-11 18:07:21.260937] [Steps   15920] [G 0.0037257]
[2023-05-11 18:07:37.851272] [Steps   15930] [G 0.0045355]
[2023-05-11 18:07:54.519845] [Steps   15940] [G 0.0040097]
[2023-05-11 18:08:11.123223] [Steps   15950] [G 0.0038609]
[2023-05-11 18:08:27.712278] [Steps   15960] [G 0.0047330]
[2023-05-11 18:08:44.396569] [Steps   15970] [G 0.0042745]
[2023-05-11 18:09:00.977857] [Steps   15980] [G 0.0055553]
[2023-05-11 18:09:17.580588] [Steps   15990] [G 0.0036893]
[2023-05-11 18:09:34.200692] [Steps   16000] [G 0.0036831]
Steps 16001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 18:09:54.137876] [Steps   16010] [G 0.0042017]
[2023-05-11 18:10:10.823236] [Steps   16020] [G 0.0054821]
[2023-05-11 18:10:27.430402] [Steps   16030] [G 0.0035988]
[2023-05-11 18:10:44.087205] [Steps   16040] [G 0.0040915]
[2023-05-11 18:11:00.821468] [Steps   16050] [G 0.0047884]
[2023-05-11 18:11:17.469889] [Steps   16060] [G 0.0038637]
[2023-05-11 18:11:34.088487] [Steps   16070] [G 0.0044369]
[2023-05-11 18:11:50.747485] [Steps   16080] [G 0.0052112]
[2023-05-11 18:12:07.457630] [Steps   16090] [G 0.0044560]
[2023-05-11 18:12:24.093698] [Steps   16100] [G 0.0035282]
[2023-05-11 18:12:40.718555] [Steps   16110] [G 0.0041376]
[2023-05-11 18:12:57.442567] [Steps   16120] [G 0.0044503]
[2023-05-11 18:13:14.102767] [Steps   16130] [G 0.0044529]
[2023-05-11 18:13:30.714584] [Steps   16140] [G 0.0044414]
[2023-05-11 18:13:47.323015] [Steps   16150] [G 0.0054254]
[2023-05-11 18:14:04.057207] [Steps   16160] [G 0.0045023]
[2023-05-11 18:14:20.695931] [Steps   16170] [G 0.0033980]
[2023-05-11 18:14:37.371859] [Steps   16180] [G 0.0045252]
[2023-05-11 18:14:54.088522] [Steps   16190] [G 0.0054292]
[2023-05-11 18:15:10.743700] [Steps   16200] [G 0.0031311]
[2023-05-11 18:15:27.391779] [Steps   16210] [G 0.0039439]
[2023-05-11 18:15:44.101099] [Steps   16220] [G 0.0039033]
[2023-05-11 18:16:00.738286] [Steps   16230] [G 0.0036708]
[2023-05-11 18:16:17.370185] [Steps   16240] [G 0.0042856]
[2023-05-11 18:16:34.022442] [Steps   16250] [G 0.0034005]
[2023-05-11 18:16:50.730632] [Steps   16260] [G 0.0038917]
[2023-05-11 18:17:07.392335] [Steps   16270] [G 0.0032703]
[2023-05-11 18:17:24.042566] [Steps   16280] [G 0.0028868]
[2023-05-11 18:17:40.760634] [Steps   16290] [G 0.0052931]
[2023-05-11 18:17:57.378964] [Steps   16300] [G 0.0041506]
[2023-05-11 18:18:15.897364] [Steps   16310] [G 0.0049233]
[2023-05-11 18:18:32.508577] [Steps   16320] [G 0.0054829]
[2023-05-11 18:18:49.218343] [Steps   16330] [G 0.0034850]
[2023-05-11 18:19:05.975518] [Steps   16340] [G 0.0035167]
[2023-05-11 18:19:22.624065] [Steps   16350] [G 0.0048998]
[2023-05-11 18:19:39.367326] [Steps   16360] [G 0.0032168]
[2023-05-11 18:19:56.021152] [Steps   16370] [G 0.0044491]
[2023-05-11 18:20:12.659060] [Steps   16380] [G 0.0051297]
[2023-05-11 18:20:29.292288] [Steps   16390] [G 0.0031468]
[2023-05-11 18:20:46.039299] [Steps   16400] [G 0.0053423]
[2023-05-11 18:21:02.716731] [Steps   16410] [G 0.0038505]
[2023-05-11 18:21:19.332582] [Steps   16420] [G 0.0038661]
[2023-05-11 18:21:36.072643] [Steps   16430] [G 0.0043077]
[2023-05-11 18:21:52.730293] [Steps   16440] [G 0.0039302]
[2023-05-11 18:22:09.381674] [Steps   16450] [G 0.0043187]
[2023-05-11 18:22:26.120711] [Steps   16460] [G 0.0053592]
[2023-05-11 18:22:42.763468] [Steps   16470] [G 0.0030173]
[2023-05-11 18:22:59.398018] [Steps   16480] [G 0.0037457]
[2023-05-11 18:23:16.041351] [Steps   16490] [G 0.0036722]
[2023-05-11 18:23:32.741312] [Steps   16500] [G 0.0040138]
[2023-05-11 18:23:49.394271] [Steps   16510] [G 0.0044879]
[2023-05-11 18:24:06.013230] [Steps   16520] [G 0.0037607]
[2023-05-11 18:24:22.718936] [Steps   16530] [G 0.0030423]
[2023-05-11 18:24:39.342926] [Steps   16540] [G 0.0035389]
[2023-05-11 18:24:56.007432] [Steps   16550] [G 0.0042487]
[2023-05-11 18:25:12.639689] [Steps   16560] [G 0.0043447]
[2023-05-11 18:25:29.372886] [Steps   16570] [G 0.0041786]
[2023-05-11 18:25:46.043717] [Steps   16580] [G 0.0039640]
[2023-05-11 18:26:02.664282] [Steps   16590] [G 0.0055697]
[2023-05-11 18:26:19.372323] [Steps   16600] [G 0.0039029]
[2023-05-11 18:26:35.940395] [Steps   16610] [G 0.0031502]
[2023-05-11 18:26:54.384168] [Steps   16620] [G 0.0037832]
[2023-05-11 18:27:11.089915] [Steps   16630] [G 0.0039493]
[2023-05-11 18:27:27.705576] [Steps   16640] [G 0.0035021]
[2023-05-11 18:27:44.329137] [Steps   16650] [G 0.0046539]
[2023-05-11 18:28:00.944988] [Steps   16660] [G 0.0035388]
[2023-05-11 18:28:17.654841] [Steps   16670] [G 0.0041072]
[2023-05-11 18:28:34.268038] [Steps   16680] [G 0.0045332]
[2023-05-11 18:28:50.878464] [Steps   16690] [G 0.0034607]
[2023-05-11 18:29:07.605331] [Steps   16700] [G 0.0035441]
[2023-05-11 18:29:24.231451] [Steps   16710] [G 0.0059694]
[2023-05-11 18:29:40.873301] [Steps   16720] [G 0.0031008]
[2023-05-11 18:29:57.507525] [Steps   16730] [G 0.0032440]
[2023-05-11 18:30:14.257116] [Steps   16740] [G 0.0040075]
[2023-05-11 18:30:30.887560] [Steps   16750] [G 0.0038652]
[2023-05-11 18:30:47.533064] [Steps   16760] [G 0.0052398]
[2023-05-11 18:31:04.226583] [Steps   16770] [G 0.0032885]
[2023-05-11 18:31:20.867187] [Steps   16780] [G 0.0047353]
[2023-05-11 18:31:37.481059] [Steps   16790] [G 0.0048276]
[2023-05-11 18:31:54.176206] [Steps   16800] [G 0.0052772]
[2023-05-11 18:32:10.797503] [Steps   16810] [G 0.0051336]
[2023-05-11 18:32:27.441384] [Steps   16820] [G 0.0050455]
[2023-05-11 18:32:44.080984] [Steps   16830] [G 0.0044958]
[2023-05-11 18:33:00.785036] [Steps   16840] [G 0.0035918]
[2023-05-11 18:33:17.390831] [Steps   16850] [G 0.0033749]
[2023-05-11 18:33:34.018097] [Steps   16860] [G 0.0035803]
[2023-05-11 18:33:50.737439] [Steps   16870] [G 0.0030452]
[2023-05-11 18:34:07.364087] [Steps   16880] [G 0.0041548]
[2023-05-11 18:34:23.988670] [Steps   16890] [G 0.0037572]
[2023-05-11 18:34:40.650254] [Steps   16900] [G 0.0060624]
[2023-05-11 18:34:57.290645] [Steps   16910] [G 0.0047166]
[2023-05-11 18:35:15.952305] [Steps   16920] [G 0.0039973]
[2023-05-11 18:35:32.580862] [Steps   16930] [G 0.0049556]
[2023-05-11 18:35:49.277053] [Steps   16940] [G 0.0040831]
[2023-05-11 18:36:05.904918] [Steps   16950] [G 0.0036902]
[2023-05-11 18:36:22.523670] [Steps   16960] [G 0.0050346]
[2023-05-11 18:36:39.240868] [Steps   16970] [G 0.0033022]
[2023-05-11 18:36:55.981102] [Steps   16980] [G 0.0034387]
[2023-05-11 18:37:12.608183] [Steps   16990] [G 0.0036021]
[2023-05-11 18:37:29.256292] [Steps   17000] [G 0.0045552]
Steps 17001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 18:37:47.311838] [Steps   17010] [G 0.0049600]
[2023-05-11 18:38:03.954213] [Steps   17020] [G 0.0050490]
[2023-05-11 18:38:20.630739] [Steps   17030] [G 0.0051694]
[2023-05-11 18:38:37.275876] [Steps   17040] [G 0.0044937]
[2023-05-11 18:38:53.885439] [Steps   17050] [G 0.0037896]
[2023-05-11 18:39:10.605334] [Steps   17060] [G 0.0042763]
[2023-05-11 18:39:27.241887] [Steps   17070] [G 0.0038484]
[2023-05-11 18:39:43.876585] [Steps   17080] [G 0.0045798]
[2023-05-11 18:40:00.575556] [Steps   17090] [G 0.0047024]
[2023-05-11 18:40:17.210039] [Steps   17100] [G 0.0050251]
[2023-05-11 18:40:33.857210] [Steps   17110] [G 0.0057517]
[2023-05-11 18:40:50.457277] [Steps   17120] [G 0.0047047]
[2023-05-11 18:41:07.162287] [Steps   17130] [G 0.0031030]
[2023-05-11 18:41:23.805139] [Steps   17140] [G 0.0043794]
[2023-05-11 18:41:40.430084] [Steps   17150] [G 0.0037488]
[2023-05-11 18:41:57.215387] [Steps   17160] [G 0.0032763]
[2023-05-11 18:42:13.836478] [Steps   17170] [G 0.0042427]
[2023-05-11 18:42:30.496690] [Steps   17180] [G 0.0040249]
[2023-05-11 18:42:47.117903] [Steps   17190] [G 0.0045649]
[2023-05-11 18:43:03.796427] [Steps   17200] [G 0.0036733]
[2023-05-11 18:43:20.393234] [Steps   17210] [G 0.0047467]
[2023-05-11 18:43:38.759216] [Steps   17220] [G 0.0041717]
[2023-05-11 18:43:55.465581] [Steps   17230] [G 0.0040284]
[2023-05-11 18:44:12.065028] [Steps   17240] [G 0.0044271]
[2023-05-11 18:44:28.736531] [Steps   17250] [G 0.0035380]
[2023-05-11 18:44:45.536819] [Steps   17260] [G 0.0030382]
[2023-05-11 18:45:02.190796] [Steps   17270] [G 0.0043804]
[2023-05-11 18:45:18.834180] [Steps   17280] [G 0.0041768]
[2023-05-11 18:45:35.495252] [Steps   17290] [G 0.0045837]
[2023-05-11 18:45:52.237249] [Steps   17300] [G 0.0043299]
[2023-05-11 18:46:08.858068] [Steps   17310] [G 0.0036435]
[2023-05-11 18:46:25.509578] [Steps   17320] [G 0.0027805]
[2023-05-11 18:46:42.222266] [Steps   17330] [G 0.0041860]
[2023-05-11 18:46:58.857037] [Steps   17340] [G 0.0044738]
[2023-05-11 18:47:15.522570] [Steps   17350] [G 0.0035017]
[2023-05-11 18:47:32.177789] [Steps   17360] [G 0.0034755]
[2023-05-11 18:47:48.906453] [Steps   17370] [G 0.0041721]
[2023-05-11 18:48:05.553698] [Steps   17380] [G 0.0037905]
[2023-05-11 18:48:22.191825] [Steps   17390] [G 0.0032081]
[2023-05-11 18:48:38.889850] [Steps   17400] [G 0.0031742]
[2023-05-11 18:48:55.557294] [Steps   17410] [G 0.0038971]
[2023-05-11 18:49:12.214580] [Steps   17420] [G 0.0036389]
[2023-05-11 18:49:28.917814] [Steps   17430] [G 0.0048055]
[2023-05-11 18:49:45.560433] [Steps   17440] [G 0.0052474]
[2023-05-11 18:50:02.200742] [Steps   17450] [G 0.0038792]
[2023-05-11 18:50:18.887523] [Steps   17460] [G 0.0042001]
[2023-05-11 18:50:35.622970] [Steps   17470] [G 0.0040481]
[2023-05-11 18:50:52.282844] [Steps   17480] [G 0.0044631]
[2023-05-11 18:51:08.957971] [Steps   17490] [G 0.0030984]
[2023-05-11 18:51:25.663271] [Steps   17500] [G 0.0042804]
[2023-05-11 18:51:42.276025] [Steps   17510] [G 0.0043825]
[2023-05-11 18:52:00.844638] [Steps   17520] [G 0.0037338]
[2023-05-11 18:52:17.438384] [Steps   17530] [G 0.0031566]
[2023-05-11 18:52:34.132949] [Steps   17540] [G 0.0029195]
[2023-05-11 18:52:50.744866] [Steps   17550] [G 0.0038001]
[2023-05-11 18:53:07.377927] [Steps   17560] [G 0.0046428]
[2023-05-11 18:53:24.073841] [Steps   17570] [G 0.0032803]
[2023-05-11 18:53:40.703305] [Steps   17580] [G 0.0045901]
[2023-05-11 18:53:57.298720] [Steps   17590] [G 0.0047173]
[2023-05-11 18:54:14.000464] [Steps   17600] [G 0.0041055]
[2023-05-11 18:54:30.626141] [Steps   17610] [G 0.0033833]
[2023-05-11 18:54:47.250458] [Steps   17620] [G 0.0049150]
[2023-05-11 18:55:03.857341] [Steps   17630] [G 0.0043065]
[2023-05-11 18:55:20.544325] [Steps   17640] [G 0.0036916]
[2023-05-11 18:55:37.172904] [Steps   17650] [G 0.0034628]
[2023-05-11 18:55:53.790397] [Steps   17660] [G 0.0024746]
[2023-05-11 18:56:10.489738] [Steps   17670] [G 0.0036305]
[2023-05-11 18:56:27.103716] [Steps   17680] [G 0.0030701]
[2023-05-11 18:56:43.741985] [Steps   17690] [G 0.0033314]
[2023-05-11 18:57:00.394473] [Steps   17700] [G 0.0040471]
[2023-05-11 18:57:17.081039] [Steps   17710] [G 0.0026530]
[2023-05-11 18:57:33.681754] [Steps   17720] [G 0.0035811]
[2023-05-11 18:57:50.278465] [Steps   17730] [G 0.0044338]
[2023-05-11 18:58:06.965766] [Steps   17740] [G 0.0033219]
[2023-05-11 18:58:23.567572] [Steps   17750] [G 0.0051898]
[2023-05-11 18:58:40.160461] [Steps   17760] [G 0.0038016]
[2023-05-11 18:58:56.845056] [Steps   17770] [G 0.0032113]
[2023-05-11 18:59:13.521933] [Steps   17780] [G 0.0039450]
[2023-05-11 18:59:30.142755] [Steps   17790] [G 0.0048363]
[2023-05-11 18:59:46.741819] [Steps   17800] [G 0.0030609]
[2023-05-11 19:00:03.397668] [Steps   17810] [G 0.0039133]
[2023-05-11 19:00:21.853646] [Steps   17820] [G 0.0040477]
[2023-05-11 19:00:38.453530] [Steps   17830] [G 0.0039450]
[2023-05-11 19:00:55.185510] [Steps   17840] [G 0.0039150]
[2023-05-11 19:01:11.799681] [Steps   17850] [G 0.0043559]
[2023-05-11 19:01:28.429130] [Steps   17860] [G 0.0041569]
[2023-05-11 19:01:45.067203] [Steps   17870] [G 0.0035497]
[2023-05-11 19:02:01.775953] [Steps   17880] [G 0.0040098]
[2023-05-11 19:02:18.396813] [Steps   17890] [G 0.0036164]
[2023-05-11 19:02:35.024161] [Steps   17900] [G 0.0030377]
[2023-05-11 19:02:51.736730] [Steps   17910] [G 0.0030302]
[2023-05-11 19:03:08.352332] [Steps   17920] [G 0.0033913]
[2023-05-11 19:03:24.982953] [Steps   17930] [G 0.0043597]
[2023-05-11 19:03:41.690141] [Steps   17940] [G 0.0038112]
[2023-05-11 19:03:58.340352] [Steps   17950] [G 0.0032761]
[2023-05-11 19:04:14.942383] [Steps   17960] [G 0.0042088]
[2023-05-11 19:04:31.599479] [Steps   17970] [G 0.0038222]
[2023-05-11 19:04:48.332221] [Steps   17980] [G 0.0038039]
[2023-05-11 19:05:04.982807] [Steps   17990] [G 0.0039842]
[2023-05-11 19:05:21.613514] [Steps   18000] [G 0.0044473]
Steps 18001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 19:05:39.742680] [Steps   18010] [G 0.0030076]
[2023-05-11 19:05:56.386326] [Steps   18020] [G 0.0040051]
[2023-05-11 19:06:13.113684] [Steps   18030] [G 0.0043656]
[2023-05-11 19:06:29.800050] [Steps   18040] [G 0.0034580]
[2023-05-11 19:06:46.434583] [Steps   18050] [G 0.0033709]
[2023-05-11 19:07:03.145617] [Steps   18060] [G 0.0035798]
[2023-05-11 19:07:19.779458] [Steps   18070] [G 0.0046181]
[2023-05-11 19:07:36.406890] [Steps   18080] [G 0.0045172]
[2023-05-11 19:07:53.042046] [Steps   18090] [G 0.0033185]
[2023-05-11 19:08:09.792041] [Steps   18100] [G 0.0035729]
[2023-05-11 19:08:26.422322] [Steps   18110] [G 0.0045572]
[2023-05-11 19:08:42.995493] [Steps   18120] [G 0.0046400]
[2023-05-11 19:09:01.677993] [Steps   18130] [G 0.0032228]
[2023-05-11 19:09:18.285552] [Steps   18140] [G 0.0046424]
[2023-05-11 19:09:34.884712] [Steps   18150] [G 0.0033547]
[2023-05-11 19:09:51.477124] [Steps   18160] [G 0.0030244]
[2023-05-11 19:10:08.142151] [Steps   18170] [G 0.0032333]
[2023-05-11 19:10:24.730891] [Steps   18180] [G 0.0030715]
[2023-05-11 19:10:41.341240] [Steps   18190] [G 0.0038857]
[2023-05-11 19:10:58.020085] [Steps   18200] [G 0.0050962]
[2023-05-11 19:11:14.624652] [Steps   18210] [G 0.0032973]
[2023-05-11 19:11:31.213729] [Steps   18220] [G 0.0041365]
[2023-05-11 19:11:47.903113] [Steps   18230] [G 0.0030511]
[2023-05-11 19:12:04.507736] [Steps   18240] [G 0.0041001]
[2023-05-11 19:12:21.098991] [Steps   18250] [G 0.0045855]
[2023-05-11 19:12:37.701855] [Steps   18260] [G 0.0045262]
[2023-05-11 19:12:54.359330] [Steps   18270] [G 0.0040581]
[2023-05-11 19:13:10.959639] [Steps   18280] [G 0.0031023]
[2023-05-11 19:13:27.550506] [Steps   18290] [G 0.0038665]
[2023-05-11 19:13:44.243434] [Steps   18300] [G 0.0036111]
[2023-05-11 19:14:00.835627] [Steps   18310] [G 0.0043681]
[2023-05-11 19:14:17.422699] [Steps   18320] [G 0.0037054]
[2023-05-11 19:14:34.015903] [Steps   18330] [G 0.0035741]
[2023-05-11 19:14:50.675998] [Steps   18340] [G 0.0057998]
[2023-05-11 19:15:07.283574] [Steps   18350] [G 0.0037975]
[2023-05-11 19:15:23.870299] [Steps   18360] [G 0.0033881]
[2023-05-11 19:15:40.555868] [Steps   18370] [G 0.0046210]
[2023-05-11 19:15:57.169819] [Steps   18380] [G 0.0043321]
[2023-05-11 19:16:13.774947] [Steps   18390] [G 0.0036179]
[2023-05-11 19:16:30.465865] [Steps   18400] [G 0.0035818]
[2023-05-11 19:16:47.050528] [Steps   18410] [G 0.0038259]
[2023-05-11 19:17:03.624991] [Steps   18420] [G 0.0028400]
[2023-05-11 19:17:22.170152] [Steps   18430] [G 0.0041409]
[2023-05-11 19:17:38.880755] [Steps   18440] [G 0.0038922]
[2023-05-11 19:17:55.484505] [Steps   18450] [G 0.0035545]
[2023-05-11 19:18:12.104067] [Steps   18460] [G 0.0034153]
[2023-05-11 19:18:28.801921] [Steps   18470] [G 0.0037680]
[2023-05-11 19:18:45.437179] [Steps   18480] [G 0.0038075]
[2023-05-11 19:19:02.045724] [Steps   18490] [G 0.0034150]
[2023-05-11 19:19:18.647208] [Steps   18500] [G 0.0038798]
[2023-05-11 19:19:35.341050] [Steps   18510] [G 0.0037865]
[2023-05-11 19:19:51.949778] [Steps   18520] [G 0.0037370]
[2023-05-11 19:20:08.650849] [Steps   18530] [G 0.0044301]
[2023-05-11 19:20:25.344197] [Steps   18540] [G 0.0043138]
[2023-05-11 19:20:41.959760] [Steps   18550] [G 0.0041768]
[2023-05-11 19:20:58.587771] [Steps   18560] [G 0.0048556]
[2023-05-11 19:21:15.280138] [Steps   18570] [G 0.0048587]
[2023-05-11 19:21:31.907407] [Steps   18580] [G 0.0036040]
[2023-05-11 19:21:48.511562] [Steps   18590] [G 0.0037389]
[2023-05-11 19:22:05.143269] [Steps   18600] [G 0.0043671]
[2023-05-11 19:22:21.853869] [Steps   18610] [G 0.0036223]
[2023-05-11 19:22:38.456370] [Steps   18620] [G 0.0046963]
[2023-05-11 19:22:55.074663] [Steps   18630] [G 0.0054535]
[2023-05-11 19:23:11.755968] [Steps   18640] [G 0.0028292]
[2023-05-11 19:23:28.379207] [Steps   18650] [G 0.0032549]
[2023-05-11 19:23:45.043102] [Steps   18660] [G 0.0035128]
[2023-05-11 19:24:01.742126] [Steps   18670] [G 0.0033525]
[2023-05-11 19:24:18.460345] [Steps   18680] [G 0.0031453]
[2023-05-11 19:24:35.077529] [Steps   18690] [G 0.0035273]
[2023-05-11 19:24:51.711257] [Steps   18700] [G 0.0038641]
[2023-05-11 19:25:08.382687] [Steps   18710] [G 0.0034147]
[2023-05-11 19:25:24.978794] [Steps   18720] [G 0.0035781]
[2023-05-11 19:25:43.503195] [Steps   18730] [G 0.0033767]
[2023-05-11 19:26:00.202931] [Steps   18740] [G 0.0046695]
[2023-05-11 19:26:16.825124] [Steps   18750] [G 0.0031065]
[2023-05-11 19:26:33.441357] [Steps   18760] [G 0.0041513]
[2023-05-11 19:26:50.059959] [Steps   18770] [G 0.0036309]
[2023-05-11 19:27:06.739977] [Steps   18780] [G 0.0039049]
[2023-05-11 19:27:23.366461] [Steps   18790] [G 0.0034432]
[2023-05-11 19:27:40.013108] [Steps   18800] [G 0.0031934]
[2023-05-11 19:27:56.693488] [Steps   18810] [G 0.0037808]
[2023-05-11 19:28:13.320177] [Steps   18820] [G 0.0037397]
[2023-05-11 19:28:29.922435] [Steps   18830] [G 0.0040459]
[2023-05-11 19:28:46.536810] [Steps   18840] [G 0.0030233]
[2023-05-11 19:29:03.211884] [Steps   18850] [G 0.0030932]
[2023-05-11 19:29:19.858010] [Steps   18860] [G 0.0031042]
[2023-05-11 19:29:36.505360] [Steps   18870] [G 0.0032844]
[2023-05-11 19:29:53.227302] [Steps   18880] [G 0.0027167]
[2023-05-11 19:30:09.853302] [Steps   18890] [G 0.0036539]
[2023-05-11 19:30:26.467241] [Steps   18900] [G 0.0033157]
[2023-05-11 19:30:43.183751] [Steps   18910] [G 0.0026527]
[2023-05-11 19:30:59.814719] [Steps   18920] [G 0.0041933]
[2023-05-11 19:31:16.446491] [Steps   18930] [G 0.0042397]
[2023-05-11 19:31:33.065610] [Steps   18940] [G 0.0037038]
[2023-05-11 19:31:49.722193] [Steps   18950] [G 0.0042931]
[2023-05-11 19:32:06.358076] [Steps   18960] [G 0.0033003]
[2023-05-11 19:32:22.961374] [Steps   18970] [G 0.0029160]
[2023-05-11 19:32:39.675543] [Steps   18980] [G 0.0038049]
[2023-05-11 19:32:56.270954] [Steps   18990] [G 0.0038340]
[2023-05-11 19:33:12.899208] [Steps   19000] [G 0.0036890]
Steps 19001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 19:33:30.967452] [Steps   19010] [G 0.0037057]
[2023-05-11 19:33:47.510482] [Steps   19020] [G 0.0031873]
[2023-05-11 19:34:06.171110] [Steps   19030] [G 0.0031323]
[2023-05-11 19:34:22.782646] [Steps   19040] [G 0.0035204]
[2023-05-11 19:34:39.402511] [Steps   19050] [G 0.0033489]
[2023-05-11 19:34:55.990832] [Steps   19060] [G 0.0032025]
[2023-05-11 19:35:12.685282] [Steps   19070] [G 0.0039852]
[2023-05-11 19:35:29.299474] [Steps   19080] [G 0.0031750]
[2023-05-11 19:35:45.914007] [Steps   19090] [G 0.0036448]
[2023-05-11 19:36:02.623501] [Steps   19100] [G 0.0030237]
[2023-05-11 19:36:19.253315] [Steps   19110] [G 0.0024749]
[2023-05-11 19:36:35.876253] [Steps   19120] [G 0.0030775]
[2023-05-11 19:36:52.469119] [Steps   19130] [G 0.0035976]
[2023-05-11 19:37:09.198893] [Steps   19140] [G 0.0035214]
[2023-05-11 19:37:25.842064] [Steps   19150] [G 0.0030148]
[2023-05-11 19:37:42.455293] [Steps   19160] [G 0.0036251]
[2023-05-11 19:37:59.147982] [Steps   19170] [G 0.0029179]
[2023-05-11 19:38:15.765040] [Steps   19180] [G 0.0032701]
[2023-05-11 19:38:32.380517] [Steps   19190] [G 0.0035697]
[2023-05-11 19:38:49.081861] [Steps   19200] [G 0.0032369]
[2023-05-11 19:39:05.720103] [Steps   19210] [G 0.0036937]
[2023-05-11 19:39:22.341072] [Steps   19220] [G 0.0028265]
[2023-05-11 19:39:38.942169] [Steps   19230] [G 0.0039818]
[2023-05-11 19:39:55.631205] [Steps   19240] [G 0.0032610]
[2023-05-11 19:40:12.244036] [Steps   19250] [G 0.0034551]
[2023-05-11 19:40:28.873002] [Steps   19260] [G 0.0048416]
[2023-05-11 19:40:45.581241] [Steps   19270] [G 0.0039704]
[2023-05-11 19:41:02.214779] [Steps   19280] [G 0.0031414]
[2023-05-11 19:41:18.837730] [Steps   19290] [G 0.0048452]
[2023-05-11 19:41:35.477391] [Steps   19300] [G 0.0048044]
[2023-05-11 19:41:52.187365] [Steps   19310] [G 0.0046805]
[2023-05-11 19:42:08.777397] [Steps   19320] [G 0.0036204]
[2023-05-11 19:42:27.517972] [Steps   19330] [G 0.0037414]
[2023-05-11 19:42:44.198623] [Steps   19340] [G 0.0033618]
[2023-05-11 19:43:00.807023] [Steps   19350] [G 0.0026316]
[2023-05-11 19:43:17.419495] [Steps   19360] [G 0.0035384]
[2023-05-11 19:43:34.157482] [Steps   19370] [G 0.0030171]
[2023-05-11 19:43:50.793365] [Steps   19380] [G 0.0030392]
[2023-05-11 19:44:07.419126] [Steps   19390] [G 0.0038928]
[2023-05-11 19:44:24.048793] [Steps   19400] [G 0.0030319]
[2023-05-11 19:44:40.746145] [Steps   19410] [G 0.0037876]
[2023-05-11 19:44:57.389922] [Steps   19420] [G 0.0034185]
[2023-05-11 19:45:14.007231] [Steps   19430] [G 0.0035658]
[2023-05-11 19:45:30.718662] [Steps   19440] [G 0.0043342]
[2023-05-11 19:45:47.332854] [Steps   19450] [G 0.0048270]
[2023-05-11 19:46:03.973143] [Steps   19460] [G 0.0035857]
[2023-05-11 19:46:20.577779] [Steps   19470] [G 0.0029859]
[2023-05-11 19:46:37.290544] [Steps   19480] [G 0.0029387]
[2023-05-11 19:46:53.888608] [Steps   19490] [G 0.0029188]
[2023-05-11 19:47:10.520476] [Steps   19500] [G 0.0029049]
[2023-05-11 19:47:27.166779] [Steps   19510] [G 0.0036223]
[2023-05-11 19:47:43.789416] [Steps   19520] [G 0.0034729]
[2023-05-11 19:48:00.406283] [Steps   19530] [G 0.0037542]
[2023-05-11 19:48:17.116023] [Steps   19540] [G 0.0032810]
[2023-05-11 19:48:33.728643] [Steps   19550] [G 0.0039614]
[2023-05-11 19:48:50.341206] [Steps   19560] [G 0.0039973]
[2023-05-11 19:49:06.944442] [Steps   19570] [G 0.0043245]
[2023-05-11 19:49:23.611474] [Steps   19580] [G 0.0033353]
[2023-05-11 19:49:40.245366] [Steps   19590] [G 0.0065336]
[2023-05-11 19:49:56.917898] [Steps   19600] [G 0.0036018]
[2023-05-11 19:50:13.603043] [Steps   19610] [G 0.0034382]
[2023-05-11 19:50:30.180739] [Steps   19620] [G 0.0031046]
[2023-05-11 19:50:46.761572] [Steps   19630] [G 0.0040097]
[2023-05-11 19:51:05.189628] [Steps   19640] [G 0.0029811]
[2023-05-11 19:51:21.885606] [Steps   19650] [G 0.0027754]
[2023-05-11 19:51:38.511736] [Steps   19660] [G 0.0034012]
[2023-05-11 19:51:55.130995] [Steps   19670] [G 0.0032964]
[2023-05-11 19:52:11.853072] [Steps   19680] [G 0.0026434]
[2023-05-11 19:52:28.485856] [Steps   19690] [G 0.0043492]
[2023-05-11 19:52:45.106541] [Steps   19700] [G 0.0035789]
[2023-05-11 19:53:01.789718] [Steps   19710] [G 0.0046329]
[2023-05-11 19:53:18.516678] [Steps   19720] [G 0.0026483]
[2023-05-11 19:53:35.203872] [Steps   19730] [G 0.0024862]
[2023-05-11 19:53:51.827522] [Steps   19740] [G 0.0032830]
[2023-05-11 19:54:08.514306] [Steps   19750] [G 0.0029627]
[2023-05-11 19:54:25.135968] [Steps   19760] [G 0.0026856]
[2023-05-11 19:54:41.778816] [Steps   19770] [G 0.0029478]
[2023-05-11 19:54:58.489606] [Steps   19780] [G 0.0043340]
[2023-05-11 19:55:15.116630] [Steps   19790] [G 0.0037644]
[2023-05-11 19:55:31.753189] [Steps   19800] [G 0.0032654]
[2023-05-11 19:55:48.391618] [Steps   19810] [G 0.0033660]
[2023-05-11 19:56:05.115197] [Steps   19820] [G 0.0030959]
[2023-05-11 19:56:21.722201] [Steps   19830] [G 0.0032552]
[2023-05-11 19:56:38.379572] [Steps   19840] [G 0.0033229]
[2023-05-11 19:56:55.095401] [Steps   19850] [G 0.0033794]
[2023-05-11 19:57:11.726485] [Steps   19860] [G 0.0029188]
[2023-05-11 19:57:28.362984] [Steps   19870] [G 0.0032221]
[2023-05-11 19:57:45.093010] [Steps   19880] [G 0.0052185]
[2023-05-11 19:58:01.715685] [Steps   19890] [G 0.0035059]
[2023-05-11 19:58:18.335359] [Steps   19900] [G 0.0035897]
[2023-05-11 19:58:34.968576] [Steps   19910] [G 0.0034771]
[2023-05-11 19:58:51.685459] [Steps   19920] [G 0.0027503]
[2023-05-11 19:59:08.272458] [Steps   19930] [G 0.0034339]
[2023-05-11 19:59:26.851500] [Steps   19940] [G 0.0031342]
[2023-05-11 19:59:43.550211] [Steps   19950] [G 0.0025524]
[2023-05-11 20:00:00.206283] [Steps   19960] [G 0.0042758]
[2023-05-11 20:00:16.797519] [Steps   19970] [G 0.0033695]
[2023-05-11 20:00:33.450174] [Steps   19980] [G 0.0043765]
[2023-05-11 20:00:50.145487] [Steps   19990] [G 0.0028825]
[2023-05-11 20:01:06.782866] [Steps   20000] [G 0.0035308]
Steps 20001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 20:01:25.945777] [Steps   20010] [G 0.0045454]
[2023-05-11 20:01:42.628191] [Steps   20020] [G 0.0031052]
[2023-05-11 20:01:59.256940] [Steps   20030] [G 0.0036998]
[2023-05-11 20:02:15.960317] [Steps   20040] [G 0.0028902]
[2023-05-11 20:02:32.671361] [Steps   20050] [G 0.0028822]
[2023-05-11 20:02:49.268211] [Steps   20060] [G 0.0031803]
[2023-05-11 20:03:05.971114] [Steps   20070] [G 0.0028584]
[2023-05-11 20:03:22.602435] [Steps   20080] [G 0.0027999]
[2023-05-11 20:03:39.290764] [Steps   20090] [G 0.0039006]
[2023-05-11 20:03:55.904057] [Steps   20100] [G 0.0031341]
[2023-05-11 20:04:12.584791] [Steps   20110] [G 0.0034765]
[2023-05-11 20:04:29.303793] [Steps   20120] [G 0.0026893]
[2023-05-11 20:04:45.914744] [Steps   20130] [G 0.0034141]
[2023-05-11 20:05:02.619714] [Steps   20140] [G 0.0025597]
[2023-05-11 20:05:19.250882] [Steps   20150] [G 0.0030558]
[2023-05-11 20:05:35.973417] [Steps   20160] [G 0.0040759]
[2023-05-11 20:05:52.668322] [Steps   20170] [G 0.0028520]
[2023-05-11 20:06:09.283271] [Steps   20180] [G 0.0036682]
[2023-05-11 20:06:25.961712] [Steps   20190] [G 0.0044982]
[2023-05-11 20:06:42.617217] [Steps   20200] [G 0.0026024]
[2023-05-11 20:06:59.320642] [Steps   20210] [G 0.0034401]
[2023-05-11 20:07:15.983760] [Steps   20220] [G 0.0031109]
[2023-05-11 20:07:32.543026] [Steps   20230] [G 0.0054896]
[2023-05-11 20:07:51.073800] [Steps   20240] [G 0.0033906]
[2023-05-11 20:08:07.643427] [Steps   20250] [G 0.0033906]
[2023-05-11 20:08:24.330119] [Steps   20260] [G 0.0036434]
[2023-05-11 20:08:40.962455] [Steps   20270] [G 0.0032530]
[2023-05-11 20:08:57.653226] [Steps   20280] [G 0.0027754]
[2023-05-11 20:09:14.335625] [Steps   20290] [G 0.0039557]
[2023-05-11 20:09:30.964314] [Steps   20300] [G 0.0035700]
[2023-05-11 20:09:47.675606] [Steps   20310] [G 0.0029732]
[2023-05-11 20:10:04.270197] [Steps   20320] [G 0.0033205]
[2023-05-11 20:10:20.984855] [Steps   20330] [G 0.0029404]
[2023-05-11 20:10:37.681385] [Steps   20340] [G 0.0030932]
[2023-05-11 20:10:54.317372] [Steps   20350] [G 0.0038042]
[2023-05-11 20:11:11.023335] [Steps   20360] [G 0.0031329]
[2023-05-11 20:11:27.642980] [Steps   20370] [G 0.0035168]
[2023-05-11 20:11:44.350584] [Steps   20380] [G 0.0038845]
[2023-05-11 20:12:01.099070] [Steps   20390] [G 0.0021897]
[2023-05-11 20:12:17.730760] [Steps   20400] [G 0.0036840]
[2023-05-11 20:12:34.443331] [Steps   20410] [G 0.0028435]
[2023-05-11 20:12:51.063052] [Steps   20420] [G 0.0028606]
[2023-05-11 20:13:07.766628] [Steps   20430] [G 0.0035564]
[2023-05-11 20:13:24.380846] [Steps   20440] [G 0.0036852]
[2023-05-11 20:13:41.071267] [Steps   20450] [G 0.0031646]
[2023-05-11 20:13:57.745989] [Steps   20460] [G 0.0039576]
[2023-05-11 20:14:14.391286] [Steps   20470] [G 0.0042647]
[2023-05-11 20:14:31.075530] [Steps   20480] [G 0.0043744]
[2023-05-11 20:14:47.702678] [Steps   20490] [G 0.0037034]
[2023-05-11 20:15:04.386593] [Steps   20500] [G 0.0026884]
[2023-05-11 20:15:21.086490] [Steps   20510] [G 0.0029973]
[2023-05-11 20:15:37.723740] [Steps   20520] [G 0.0031965]
[2023-05-11 20:15:54.370078] [Steps   20530] [G 0.0032928]
[2023-05-11 20:16:12.769667] [Steps   20540] [G 0.0044160]
[2023-05-11 20:16:29.491591] [Steps   20550] [G 0.0033932]
[2023-05-11 20:16:46.180681] [Steps   20560] [G 0.0026958]
[2023-05-11 20:17:02.827859] [Steps   20570] [G 0.0024648]
[2023-05-11 20:17:19.537059] [Steps   20580] [G 0.0040385]
[2023-05-11 20:17:36.183849] [Steps   20590] [G 0.0029138]
[2023-05-11 20:17:52.886625] [Steps   20600] [G 0.0031206]
[2023-05-11 20:18:09.533445] [Steps   20610] [G 0.0027220]
[2023-05-11 20:18:26.272375] [Steps   20620] [G 0.0022013]
[2023-05-11 20:18:42.992786] [Steps   20630] [G 0.0040875]
[2023-05-11 20:18:59.629065] [Steps   20640] [G 0.0026805]
[2023-05-11 20:19:16.353995] [Steps   20650] [G 0.0031377]
[2023-05-11 20:19:33.011399] [Steps   20660] [G 0.0032988]
[2023-05-11 20:19:49.739908] [Steps   20670] [G 0.0035568]
[2023-05-11 20:20:06.473415] [Steps   20680] [G 0.0030576]
[2023-05-11 20:20:23.079898] [Steps   20690] [G 0.0028100]
[2023-05-11 20:20:39.802266] [Steps   20700] [G 0.0030859]
[2023-05-11 20:20:56.442668] [Steps   20710] [G 0.0032063]
[2023-05-11 20:21:13.143902] [Steps   20720] [G 0.0029556]
[2023-05-11 20:21:29.859741] [Steps   20730] [G 0.0029980]
[2023-05-11 20:21:46.483100] [Steps   20740] [G 0.0025300]
[2023-05-11 20:22:03.219693] [Steps   20750] [G 0.0029432]
[2023-05-11 20:22:19.841681] [Steps   20760] [G 0.0029889]
[2023-05-11 20:22:36.542210] [Steps   20770] [G 0.0027565]
[2023-05-11 20:22:53.182617] [Steps   20780] [G 0.0037228]
[2023-05-11 20:23:09.909604] [Steps   20790] [G 0.0038866]
[2023-05-11 20:23:26.604639] [Steps   20800] [G 0.0034893]
[2023-05-11 20:23:43.230386] [Steps   20810] [G 0.0028001]
[2023-05-11 20:23:59.929666] [Steps   20820] [G 0.0032299]
[2023-05-11 20:24:16.563370] [Steps   20830] [G 0.0023663]
[2023-05-11 20:24:35.246943] [Steps   20840] [G 0.0035339]
[2023-05-11 20:24:51.841511] [Steps   20850] [G 0.0044391]
[2023-05-11 20:25:08.435735] [Steps   20860] [G 0.0036609]
[2023-05-11 20:25:25.135382] [Steps   20870] [G 0.0031692]
[2023-05-11 20:25:41.730633] [Steps   20880] [G 0.0028956]
[2023-05-11 20:25:58.356964] [Steps   20890] [G 0.0034989]
[2023-05-11 20:26:15.064318] [Steps   20900] [G 0.0033534]
[2023-05-11 20:26:31.682526] [Steps   20910] [G 0.0035528]
[2023-05-11 20:26:48.313267] [Steps   20920] [G 0.0030346]
[2023-05-11 20:27:04.954544] [Steps   20930] [G 0.0031856]
[2023-05-11 20:27:21.699454] [Steps   20940] [G 0.0024466]
[2023-05-11 20:27:38.310249] [Steps   20950] [G 0.0031691]
[2023-05-11 20:27:54.941338] [Steps   20960] [G 0.0028892]
[2023-05-11 20:28:11.610125] [Steps   20970] [G 0.0036068]
[2023-05-11 20:28:28.211992] [Steps   20980] [G 0.0026606]
[2023-05-11 20:28:44.850205] [Steps   20990] [G 0.0030510]
[2023-05-11 20:29:01.467312] [Steps   21000] [G 0.0057237]
Steps 21001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 20:29:19.622498] [Steps   21010] [G 0.0029575]
[2023-05-11 20:29:36.236186] [Steps   21020] [G 0.0030702]
[2023-05-11 20:29:52.958471] [Steps   21030] [G 0.0028642]
[2023-05-11 20:30:09.572273] [Steps   21040] [G 0.0034756]
[2023-05-11 20:30:26.277489] [Steps   21050] [G 0.0026379]
[2023-05-11 20:30:42.963528] [Steps   21060] [G 0.0037645]
[2023-05-11 20:30:59.582728] [Steps   21070] [G 0.0032572]
[2023-05-11 20:31:16.304306] [Steps   21080] [G 0.0026260]
[2023-05-11 20:31:32.938298] [Steps   21090] [G 0.0042980]
[2023-05-11 20:31:49.539730] [Steps   21100] [G 0.0040975]
[2023-05-11 20:32:06.254142] [Steps   21110] [G 0.0021519]
[2023-05-11 20:32:22.866059] [Steps   21120] [G 0.0033812]
[2023-05-11 20:32:39.461023] [Steps   21130] [G 0.0039027]
[2023-05-11 20:32:56.023368] [Steps   21140] [G 0.0031860]
[2023-05-11 20:33:14.708035] [Steps   21150] [G 0.0028857]
[2023-05-11 20:33:31.308452] [Steps   21160] [G 0.0039002]
[2023-05-11 20:33:47.938264] [Steps   21170] [G 0.0035525]
[2023-05-11 20:34:04.635951] [Steps   21180] [G 0.0033637]
[2023-05-11 20:34:21.258562] [Steps   21190] [G 0.0030693]
[2023-05-11 20:34:37.896698] [Steps   21200] [G 0.0032838]
[2023-05-11 20:34:54.523154] [Steps   21210] [G 0.0027427]
[2023-05-11 20:35:11.227604] [Steps   21220] [G 0.0031931]
[2023-05-11 20:35:27.857025] [Steps   21230] [G 0.0023133]
[2023-05-11 20:35:44.488466] [Steps   21240] [G 0.0028686]
[2023-05-11 20:36:01.180543] [Steps   21250] [G 0.0030242]
[2023-05-11 20:36:17.783528] [Steps   21260] [G 0.0030424]
[2023-05-11 20:36:34.407715] [Steps   21270] [G 0.0024622]
[2023-05-11 20:36:51.085874] [Steps   21280] [G 0.0021256]
[2023-05-11 20:37:07.692847] [Steps   21290] [G 0.0028929]
[2023-05-11 20:37:24.308822] [Steps   21300] [G 0.0028459]
[2023-05-11 20:37:40.924905] [Steps   21310] [G 0.0027707]
[2023-05-11 20:37:57.588520] [Steps   21320] [G 0.0036959]
[2023-05-11 20:38:14.212847] [Steps   21330] [G 0.0036736]
[2023-05-11 20:38:30.851409] [Steps   21340] [G 0.0020644]
[2023-05-11 20:38:47.588780] [Steps   21350] [G 0.0028268]
[2023-05-11 20:39:04.191973] [Steps   21360] [G 0.0034068]
[2023-05-11 20:39:20.799999] [Steps   21370] [G 0.0038797]
[2023-05-11 20:39:37.435441] [Steps   21380] [G 0.0034309]
[2023-05-11 20:39:54.116591] [Steps   21390] [G 0.0040352]
[2023-05-11 20:40:10.748771] [Steps   21400] [G 0.0035143]
[2023-05-11 20:40:27.356411] [Steps   21410] [G 0.0032382]
[2023-05-11 20:40:44.048665] [Steps   21420] [G 0.0026342]
[2023-05-11 20:41:00.672863] [Steps   21430] [G 0.0036640]
[2023-05-11 20:41:17.231338] [Steps   21440] [G 0.0034364]
[2023-05-11 20:41:35.865678] [Steps   21450] [G 0.0028006]
[2023-05-11 20:41:52.468497] [Steps   21460] [G 0.0025855]
[2023-05-11 20:42:09.091106] [Steps   21470] [G 0.0024152]
[2023-05-11 20:42:25.716521] [Steps   21480] [G 0.0023048]
[2023-05-11 20:42:42.421646] [Steps   21490] [G 0.0022549]
[2023-05-11 20:42:59.054614] [Steps   21500] [G 0.0041850]
[2023-05-11 20:43:15.683010] [Steps   21510] [G 0.0030833]
[2023-05-11 20:43:32.468816] [Steps   21520] [G 0.0043287]
[2023-05-11 20:43:49.159612] [Steps   21530] [G 0.0032277]
[2023-05-11 20:44:05.771344] [Steps   21540] [G 0.0030611]
[2023-05-11 20:44:22.384978] [Steps   21550] [G 0.0034363]
[2023-05-11 20:44:39.104162] [Steps   21560] [G 0.0024519]
[2023-05-11 20:44:55.699452] [Steps   21570] [G 0.0030583]
[2023-05-11 20:45:12.291175] [Steps   21580] [G 0.0027034]
[2023-05-11 20:45:29.026767] [Steps   21590] [G 0.0026445]
[2023-05-11 20:45:45.653028] [Steps   21600] [G 0.0027213]
[2023-05-11 20:46:02.269834] [Steps   21610] [G 0.0025686]
[2023-05-11 20:46:18.966539] [Steps   21620] [G 0.0028028]
[2023-05-11 20:46:35.593653] [Steps   21630] [G 0.0026827]
[2023-05-11 20:46:52.225889] [Steps   21640] [G 0.0024772]
[2023-05-11 20:47:08.847175] [Steps   21650] [G 0.0031617]
[2023-05-11 20:47:25.545363] [Steps   21660] [G 0.0028496]
[2023-05-11 20:47:42.155520] [Steps   21670] [G 0.0042473]
[2023-05-11 20:47:58.781069] [Steps   21680] [G 0.0045240]
[2023-05-11 20:48:15.478108] [Steps   21690] [G 0.0030879]
[2023-05-11 20:48:32.107186] [Steps   21700] [G 0.0029248]
[2023-05-11 20:48:48.742712] [Steps   21710] [G 0.0031538]
[2023-05-11 20:49:05.347915] [Steps   21720] [G 0.0026946]
[2023-05-11 20:49:22.053824] [Steps   21730] [G 0.0035352]
[2023-05-11 20:49:38.635213] [Steps   21740] [G 0.0026157]
[2023-05-11 20:49:57.008819] [Steps   21750] [G 0.0029195]
[2023-05-11 20:50:13.730785] [Steps   21760] [G 0.0023726]
[2023-05-11 20:50:30.374224] [Steps   21770] [G 0.0039588]
[2023-05-11 20:50:46.995917] [Steps   21780] [G 0.0026129]
[2023-05-11 20:51:03.700533] [Steps   21790] [G 0.0025501]
[2023-05-11 20:51:20.343106] [Steps   21800] [G 0.0033844]
[2023-05-11 20:51:36.985448] [Steps   21810] [G 0.0025429]
[2023-05-11 20:51:53.630610] [Steps   21820] [G 0.0024660]
[2023-05-11 20:52:10.345630] [Steps   21830] [G 0.0032512]
[2023-05-11 20:52:26.980010] [Steps   21840] [G 0.0029003]
[2023-05-11 20:52:43.668847] [Steps   21850] [G 0.0030918]
[2023-05-11 20:53:00.393407] [Steps   21860] [G 0.0043995]
[2023-05-11 20:53:17.024674] [Steps   21870] [G 0.0037352]
[2023-05-11 20:53:33.682522] [Steps   21880] [G 0.0036963]
[2023-05-11 20:53:50.313445] [Steps   21890] [G 0.0022775]
[2023-05-11 20:54:07.072019] [Steps   21900] [G 0.0026991]
[2023-05-11 20:54:23.705990] [Steps   21910] [G 0.0047131]
[2023-05-11 20:54:40.342989] [Steps   21920] [G 0.0027202]
[2023-05-11 20:54:57.070475] [Steps   21930] [G 0.0034666]
[2023-05-11 20:55:13.717410] [Steps   21940] [G 0.0027064]
[2023-05-11 20:55:30.381326] [Steps   21950] [G 0.0032506]
[2023-05-11 20:55:47.096871] [Steps   21960] [G 0.0034403]
[2023-05-11 20:56:03.709230] [Steps   21970] [G 0.0022468]
[2023-05-11 20:56:20.374640] [Steps   21980] [G 0.0024828]
[2023-05-11 20:56:37.011641] [Steps   21990] [G 0.0026930]
[2023-05-11 20:56:53.717082] [Steps   22000] [G 0.0025407]
Steps 22001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 20:57:11.799568] [Steps   22010] [G 0.0034592]
[2023-05-11 20:57:28.550089] [Steps   22020] [G 0.0036684]
[2023-05-11 20:57:45.193875] [Steps   22030] [G 0.0022095]
[2023-05-11 20:58:01.812805] [Steps   22040] [G 0.0025493]
[2023-05-11 20:58:20.418684] [Steps   22050] [G 0.0023713]
[2023-05-11 20:58:37.134203] [Steps   22060] [G 0.0023208]
[2023-05-11 20:58:53.750784] [Steps   22070] [G 0.0023267]
[2023-05-11 20:59:10.445939] [Steps   22080] [G 0.0026478]
[2023-05-11 20:59:27.060656] [Steps   22090] [G 0.0041017]
[2023-05-11 20:59:43.663299] [Steps   22100] [G 0.0023726]
[2023-05-11 21:00:00.290146] [Steps   22110] [G 0.0024222]
[2023-05-11 21:00:16.996240] [Steps   22120] [G 0.0021757]
[2023-05-11 21:00:33.704612] [Steps   22130] [G 0.0024331]
[2023-05-11 21:00:50.309636] [Steps   22140] [G 0.0030597]
[2023-05-11 21:01:07.019608] [Steps   22150] [G 0.0022756]
[2023-05-11 21:01:23.641511] [Steps   22160] [G 0.0025275]
[2023-05-11 21:01:40.267008] [Steps   22170] [G 0.0026600]
[2023-05-11 21:01:56.906265] [Steps   22180] [G 0.0023551]
[2023-05-11 21:02:13.598986] [Steps   22190] [G 0.0031652]
[2023-05-11 21:02:30.249350] [Steps   22200] [G 0.0035532]
[2023-05-11 21:02:46.911622] [Steps   22210] [G 0.0024497]
[2023-05-11 21:03:03.639115] [Steps   22220] [G 0.0026349]
[2023-05-11 21:03:20.250858] [Steps   22230] [G 0.0023434]
[2023-05-11 21:03:36.907184] [Steps   22240] [G 0.0025056]
[2023-05-11 21:03:53.708060] [Steps   22250] [G 0.0046916]
[2023-05-11 21:04:10.323974] [Steps   22260] [G 0.0034580]
[2023-05-11 21:04:26.983267] [Steps   22270] [G 0.0039894]
[2023-05-11 21:04:43.630368] [Steps   22280] [G 0.0037117]
[2023-05-11 21:05:00.377867] [Steps   22290] [G 0.0033735]
[2023-05-11 21:05:17.013273] [Steps   22300] [G 0.0028576]
[2023-05-11 21:05:33.628431] [Steps   22310] [G 0.0023911]
[2023-05-11 21:05:50.354256] [Steps   22320] [G 0.0028662]
[2023-05-11 21:06:06.978010] [Steps   22330] [G 0.0027041]
[2023-05-11 21:06:23.581505] [Steps   22340] [G 0.0036869]
[2023-05-11 21:06:42.016990] [Steps   22350] [G 0.0039674]
[2023-05-11 21:06:58.780151] [Steps   22360] [G 0.0023267]
[2023-05-11 21:07:15.405229] [Steps   22370] [G 0.0033532]
[2023-05-11 21:07:32.040261] [Steps   22380] [G 0.0027708]
[2023-05-11 21:07:48.779017] [Steps   22390] [G 0.0029094]
[2023-05-11 21:08:05.490879] [Steps   22400] [G 0.0023708]
[2023-05-11 21:08:22.120679] [Steps   22410] [G 0.0025468]
[2023-05-11 21:08:38.799024] [Steps   22420] [G 0.0029541]
[2023-05-11 21:08:55.465111] [Steps   22430] [G 0.0039482]
[2023-05-11 21:09:12.111858] [Steps   22440] [G 0.0028152]
[2023-05-11 21:09:28.756752] [Steps   22450] [G 0.0026228]
[2023-05-11 21:09:45.462655] [Steps   22460] [G 0.0027264]
[2023-05-11 21:10:02.120314] [Steps   22470] [G 0.0026413]
[2023-05-11 21:10:18.769831] [Steps   22480] [G 0.0027515]
[2023-05-11 21:10:35.487669] [Steps   22490] [G 0.0033405]
[2023-05-11 21:10:52.126452] [Steps   22500] [G 0.0025110]
[2023-05-11 21:11:08.773455] [Steps   22510] [G 0.0036372]
[2023-05-11 21:11:25.391822] [Steps   22520] [G 0.0023548]
[2023-05-11 21:11:42.103063] [Steps   22530] [G 0.0024069]
[2023-05-11 21:11:58.709786] [Steps   22540] [G 0.0028352]
[2023-05-11 21:12:15.384395] [Steps   22550] [G 0.0033436]
[2023-05-11 21:12:32.071465] [Steps   22560] [G 0.0031954]
[2023-05-11 21:12:48.732036] [Steps   22570] [G 0.0039139]
[2023-05-11 21:13:05.384873] [Steps   22580] [G 0.0030264]
[2023-05-11 21:13:22.174597] [Steps   22590] [G 0.0033463]
[2023-05-11 21:13:38.870167] [Steps   22600] [G 0.0024980]
[2023-05-11 21:13:55.545256] [Steps   22610] [G 0.0030447]
[2023-05-11 21:14:12.233965] [Steps   22620] [G 0.0030018]
[2023-05-11 21:14:28.973862] [Steps   22630] [G 0.0023057]
[2023-05-11 21:14:45.653018] [Steps   22640] [G 0.0027325]
[2023-05-11 21:15:02.279445] [Steps   22650] [G 0.0031419]
[2023-05-11 21:15:20.923348] [Steps   22660] [G 0.0023013]
[2023-05-11 21:15:37.564911] [Steps   22670] [G 0.0024236]
[2023-05-11 21:15:54.240377] [Steps   22680] [G 0.0026504]
[2023-05-11 21:16:10.942758] [Steps   22690] [G 0.0031330]
[2023-05-11 21:16:27.666217] [Steps   22700] [G 0.0023426]
[2023-05-11 21:16:44.344638] [Steps   22710] [G 0.0024382]
[2023-05-11 21:17:01.052827] [Steps   22720] [G 0.0028150]
[2023-05-11 21:17:17.819670] [Steps   22730] [G 0.0035141]
[2023-05-11 21:17:34.504945] [Steps   22740] [G 0.0029125]
[2023-05-11 21:17:51.189388] [Steps   22750] [G 0.0023342]
[2023-05-11 21:18:07.957784] [Steps   22760] [G 0.0021950]
[2023-05-11 21:18:24.624302] [Steps   22770] [G 0.0024770]
[2023-05-11 21:18:41.281180] [Steps   22780] [G 0.0031493]
[2023-05-11 21:18:57.941831] [Steps   22790] [G 0.0033314]
[2023-05-11 21:19:14.690760] [Steps   22800] [G 0.0031891]
[2023-05-11 21:19:31.374027] [Steps   22810] [G 0.0025872]
[2023-05-11 21:19:48.046277] [Steps   22820] [G 0.0023880]
[2023-05-11 21:20:04.795622] [Steps   22830] [G 0.0023856]
[2023-05-11 21:20:21.493931] [Steps   22840] [G 0.0026321]
[2023-05-11 21:20:38.210041] [Steps   22850] [G 0.0021870]
[2023-05-11 21:20:54.887219] [Steps   22860] [G 0.0027405]
[2023-05-11 21:21:11.617474] [Steps   22870] [G 0.0028929]
[2023-05-11 21:21:28.315684] [Steps   22880] [G 0.0035676]
[2023-05-11 21:21:45.014567] [Steps   22890] [G 0.0024063]
[2023-05-11 21:22:01.804350] [Steps   22900] [G 0.0023547]
[2023-05-11 21:22:18.487446] [Steps   22910] [G 0.0032752]
[2023-05-11 21:22:35.150246] [Steps   22920] [G 0.0022287]
[2023-05-11 21:22:51.976989] [Steps   22930] [G 0.0037297]
[2023-05-11 21:23:08.666175] [Steps   22940] [G 0.0025893]
[2023-05-11 21:23:25.313885] [Steps   22950] [G 0.0031049]
[2023-05-11 21:23:43.807182] [Steps   22960] [G 0.0025160]
[2023-05-11 21:24:00.535888] [Steps   22970] [G 0.0022662]
[2023-05-11 21:24:17.178573] [Steps   22980] [G 0.0027682]
[2023-05-11 21:24:33.853649] [Steps   22990] [G 0.0028268]
[2023-05-11 21:24:50.594732] [Steps   23000] [G 0.0021223]
Steps 23001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 21:25:08.694873] [Steps   23010] [G 0.0023652]
[2023-05-11 21:25:25.433448] [Steps   23020] [G 0.0030342]
[2023-05-11 21:25:42.150531] [Steps   23030] [G 0.0028077]
[2023-05-11 21:25:58.857241] [Steps   23040] [G 0.0034369]
[2023-05-11 21:26:15.574152] [Steps   23050] [G 0.0036412]
[2023-05-11 21:26:32.278022] [Steps   23060] [G 0.0027954]
[2023-05-11 21:26:48.965639] [Steps   23070] [G 0.0027837]
[2023-05-11 21:27:05.666763] [Steps   23080] [G 0.0025371]
[2023-05-11 21:27:22.402703] [Steps   23090] [G 0.0023911]
[2023-05-11 21:27:39.070992] [Steps   23100] [G 0.0024203]
[2023-05-11 21:27:55.761670] [Steps   23110] [G 0.0027347]
[2023-05-11 21:28:12.509641] [Steps   23120] [G 0.0036833]
[2023-05-11 21:28:29.156056] [Steps   23130] [G 0.0042435]
[2023-05-11 21:28:45.823299] [Steps   23140] [G 0.0035435]
[2023-05-11 21:29:02.530274] [Steps   23150] [G 0.0028010]
[2023-05-11 21:29:19.280886] [Steps   23160] [G 0.0021208]
[2023-05-11 21:29:35.956316] [Steps   23170] [G 0.0038218]
[2023-05-11 21:29:52.639287] [Steps   23180] [G 0.0028677]
[2023-05-11 21:30:09.390804] [Steps   23190] [G 0.0030831]
[2023-05-11 21:30:26.084446] [Steps   23200] [G 0.0032628]
[2023-05-11 21:30:42.746508] [Steps   23210] [G 0.0031317]
[2023-05-11 21:30:59.502697] [Steps   23220] [G 0.0019754]
[2023-05-11 21:31:16.231420] [Steps   23230] [G 0.0027191]
[2023-05-11 21:31:32.940869] [Steps   23240] [G 0.0022416]
[2023-05-11 21:31:49.592271] [Steps   23250] [G 0.0024069]
[2023-05-11 21:32:08.333958] [Steps   23260] [G 0.0022831]
[2023-05-11 21:32:25.064730] [Steps   23270] [G 0.0030683]
[2023-05-11 21:32:41.753844] [Steps   23280] [G 0.0030451]
[2023-05-11 21:32:58.510964] [Steps   23290] [G 0.0028151]
[2023-05-11 21:33:15.254769] [Steps   23300] [G 0.0026278]
[2023-05-11 21:33:31.954840] [Steps   23310] [G 0.0029442]
[2023-05-11 21:33:48.679055] [Steps   23320] [G 0.0024521]
[2023-05-11 21:34:05.429631] [Steps   23330] [G 0.0026278]
[2023-05-11 21:34:22.134566] [Steps   23340] [G 0.0039882]
[2023-05-11 21:34:38.796786] [Steps   23350] [G 0.0022556]
[2023-05-11 21:34:55.546999] [Steps   23360] [G 0.0027565]
[2023-05-11 21:35:12.225302] [Steps   23370] [G 0.0023213]
[2023-05-11 21:35:28.935547] [Steps   23380] [G 0.0021606]
[2023-05-11 21:35:45.721414] [Steps   23390] [G 0.0036076]
[2023-05-11 21:36:02.407814] [Steps   23400] [G 0.0032648]
[2023-05-11 21:36:19.119605] [Steps   23410] [G 0.0024186]
[2023-05-11 21:36:35.805851] [Steps   23420] [G 0.0026426]
[2023-05-11 21:36:52.560269] [Steps   23430] [G 0.0032180]
[2023-05-11 21:37:09.264799] [Steps   23440] [G 0.0046487]
[2023-05-11 21:37:25.973981] [Steps   23450] [G 0.0022713]
[2023-05-11 21:37:42.735418] [Steps   23460] [G 0.0030293]
[2023-05-11 21:37:59.407805] [Steps   23470] [G 0.0025007]
[2023-05-11 21:38:16.098308] [Steps   23480] [G 0.0025210]
[2023-05-11 21:38:32.800333] [Steps   23490] [G 0.0024160]
[2023-05-11 21:38:49.571376] [Steps   23500] [G 0.0019865]
[2023-05-11 21:39:06.290010] [Steps   23510] [G 0.0024781]
[2023-05-11 21:39:22.919846] [Steps   23520] [G 0.0027569]
[2023-05-11 21:39:39.694146] [Steps   23530] [G 0.0027298]
[2023-05-11 21:39:56.416317] [Steps   23540] [G 0.0035706]
[2023-05-11 21:40:13.100799] [Steps   23550] [G 0.0027547]
[2023-05-11 21:40:31.695260] [Steps   23560] [G 0.0020182]
[2023-05-11 21:40:48.419513] [Steps   23570] [G 0.0029098]
[2023-05-11 21:41:05.087865] [Steps   23580] [G 0.0026946]
[2023-05-11 21:41:21.735328] [Steps   23590] [G 0.0022227]
[2023-05-11 21:41:38.479696] [Steps   23600] [G 0.0023743]
[2023-05-11 21:41:55.144015] [Steps   23610] [G 0.0033240]
[2023-05-11 21:42:11.842481] [Steps   23620] [G 0.0032302]
[2023-05-11 21:42:28.599695] [Steps   23630] [G 0.0026567]
[2023-05-11 21:42:45.279716] [Steps   23640] [G 0.0029769]
[2023-05-11 21:43:01.979121] [Steps   23650] [G 0.0021276]
[2023-05-11 21:43:18.671876] [Steps   23660] [G 0.0022648]
[2023-05-11 21:43:35.434134] [Steps   23670] [G 0.0035852]
[2023-05-11 21:43:52.098318] [Steps   23680] [G 0.0023676]
[2023-05-11 21:44:08.776830] [Steps   23690] [G 0.0021460]
[2023-05-11 21:44:25.514544] [Steps   23700] [G 0.0023901]
[2023-05-11 21:44:42.152094] [Steps   23710] [G 0.0032909]
[2023-05-11 21:44:58.865991] [Steps   23720] [G 0.0022520]
[2023-05-11 21:45:15.553074] [Steps   23730] [G 0.0038557]
[2023-05-11 21:45:32.293202] [Steps   23740] [G 0.0029227]
[2023-05-11 21:45:48.967186] [Steps   23750] [G 0.0026029]
[2023-05-11 21:46:05.652090] [Steps   23760] [G 0.0038671]
[2023-05-11 21:46:22.402359] [Steps   23770] [G 0.0021132]
[2023-05-11 21:46:39.080878] [Steps   23780] [G 0.0026728]
[2023-05-11 21:46:55.768904] [Steps   23790] [G 0.0022419]
[2023-05-11 21:47:12.515259] [Steps   23800] [G 0.0035833]
[2023-05-11 21:47:29.162127] [Steps   23810] [G 0.0032444]
[2023-05-11 21:47:45.761970] [Steps   23820] [G 0.0027005]
[2023-05-11 21:48:02.396104] [Steps   23830] [G 0.0023357]
[2023-05-11 21:48:19.115850] [Steps   23840] [G 0.0026407]
[2023-05-11 21:48:35.732000] [Steps   23850] [G 0.0017918]
[2023-05-11 21:48:54.344361] [Steps   23860] [G 0.0025717]
[2023-05-11 21:49:11.055685] [Steps   23870] [G 0.0025570]
[2023-05-11 21:49:27.669900] [Steps   23880] [G 0.0028532]
[2023-05-11 21:49:44.262120] [Steps   23890] [G 0.0028335]
[2023-05-11 21:50:00.891816] [Steps   23900] [G 0.0021149]
[2023-05-11 21:50:17.591517] [Steps   23910] [G 0.0030059]
[2023-05-11 21:50:34.188909] [Steps   23920] [G 0.0023100]
[2023-05-11 21:50:50.804905] [Steps   23930] [G 0.0022092]
[2023-05-11 21:51:07.475005] [Steps   23940] [G 0.0024817]
[2023-05-11 21:51:24.095722] [Steps   23950] [G 0.0022216]
[2023-05-11 21:51:40.709888] [Steps   23960] [G 0.0023123]
[2023-05-11 21:51:57.414151] [Steps   23970] [G 0.0026898]
[2023-05-11 21:52:14.016544] [Steps   23980] [G 0.0022334]
[2023-05-11 21:52:30.634265] [Steps   23990] [G 0.0021751]
[2023-05-11 21:52:47.254356] [Steps   24000] [G 0.0019198]
Steps 24001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 21:53:05.400693] [Steps   24010] [G 0.0023805]
[2023-05-11 21:53:22.000329] [Steps   24020] [G 0.0032668]
[2023-05-11 21:53:38.704403] [Steps   24030] [G 0.0022447]
[2023-05-11 21:53:55.388944] [Steps   24040] [G 0.0022814]
[2023-05-11 21:54:12.013405] [Steps   24050] [G 0.0026921]
[2023-05-11 21:54:28.697039] [Steps   24060] [G 0.0032017]
[2023-05-11 21:54:45.313825] [Steps   24070] [G 0.0024481]
[2023-05-11 21:55:01.991748] [Steps   24080] [G 0.0037510]
[2023-05-11 21:55:18.689956] [Steps   24090] [G 0.0029208]
[2023-05-11 21:55:35.304520] [Steps   24100] [G 0.0023684]
[2023-05-11 21:55:52.005818] [Steps   24110] [G 0.0033848]
[2023-05-11 21:56:08.637959] [Steps   24120] [G 0.0033898]
[2023-05-11 21:56:25.319951] [Steps   24130] [G 0.0027424]
[2023-05-11 21:56:41.910482] [Steps   24140] [G 0.0026642]
[2023-05-11 21:56:58.589272] [Steps   24150] [G 0.0023664]
[2023-05-11 21:57:15.223561] [Steps   24160] [G 0.0032609]
[2023-05-11 21:57:33.745469] [Steps   24170] [G 0.0022545]
[2023-05-11 21:57:50.454964] [Steps   24180] [G 0.0026216]
[2023-05-11 21:58:07.060658] [Steps   24190] [G 0.0043182]
[2023-05-11 21:58:23.742761] [Steps   24200] [G 0.0020860]
[2023-05-11 21:58:40.425693] [Steps   24210] [G 0.0024916]
[2023-05-11 21:58:57.024287] [Steps   24220] [G 0.0021454]
[2023-05-11 21:59:13.706199] [Steps   24230] [G 0.0024976]
[2023-05-11 21:59:30.323016] [Steps   24240] [G 0.0027387]
[2023-05-11 21:59:46.994569] [Steps   24250] [G 0.0022501]
[2023-05-11 22:00:03.671133] [Steps   24260] [G 0.0029068]
[2023-05-11 22:00:20.274268] [Steps   24270] [G 0.0027534]
[2023-05-11 22:00:36.983575] [Steps   24280] [G 0.0031586]
[2023-05-11 22:00:53.589578] [Steps   24290] [G 0.0025450]
[2023-05-11 22:01:10.299593] [Steps   24300] [G 0.0022935]
[2023-05-11 22:01:26.896874] [Steps   24310] [G 0.0024457]
[2023-05-11 22:01:43.597132] [Steps   24320] [G 0.0019031]
[2023-05-11 22:02:00.287961] [Steps   24330] [G 0.0022975]
[2023-05-11 22:02:16.882121] [Steps   24340] [G 0.0028628]
[2023-05-11 22:02:33.571861] [Steps   24350] [G 0.0023330]
[2023-05-11 22:02:50.175202] [Steps   24360] [G 0.0018268]
[2023-05-11 22:03:06.828324] [Steps   24370] [G 0.0030053]
[2023-05-11 22:03:23.487971] [Steps   24380] [G 0.0043970]
[2023-05-11 22:03:40.086704] [Steps   24390] [G 0.0032825]
[2023-05-11 22:03:56.743271] [Steps   24400] [G 0.0026328]
[2023-05-11 22:04:13.342168] [Steps   24410] [G 0.0028863]
[2023-05-11 22:04:30.020371] [Steps   24420] [G 0.0042978]
[2023-05-11 22:04:46.698637] [Steps   24430] [G 0.0019380]
[2023-05-11 22:05:03.290554] [Steps   24440] [G 0.0032507]
[2023-05-11 22:05:19.951464] [Steps   24450] [G 0.0025645]
[2023-05-11 22:05:36.542552] [Steps   24460] [G 0.0032529]
[2023-05-11 22:05:55.402292] [Steps   24470] [G 0.0024631]
[2023-05-11 22:06:12.000898] [Steps   24480] [G 0.0021415]
[2023-05-11 22:06:28.621638] [Steps   24490] [G 0.0021194]
[2023-05-11 22:06:45.320235] [Steps   24500] [G 0.0023617]
[2023-05-11 22:07:01.933454] [Steps   24510] [G 0.0020654]
[2023-05-11 22:07:18.519219] [Steps   24520] [G 0.0026804]
[2023-05-11 22:07:35.155334] [Steps   24530] [G 0.0030145]
[2023-05-11 22:07:51.846289] [Steps   24540] [G 0.0016405]
[2023-05-11 22:08:08.480070] [Steps   24550] [G 0.0020695]
[2023-05-11 22:08:25.133403] [Steps   24560] [G 0.0031596]
[2023-05-11 22:08:41.829427] [Steps   24570] [G 0.0026553]
[2023-05-11 22:08:58.430034] [Steps   24580] [G 0.0036159]
[2023-05-11 22:09:15.014837] [Steps   24590] [G 0.0021953]
[2023-05-11 22:09:31.718560] [Steps   24600] [G 0.0020308]
[2023-05-11 22:09:48.337255] [Steps   24610] [G 0.0024134]
[2023-05-11 22:10:04.941454] [Steps   24620] [G 0.0020982]
[2023-05-11 22:10:21.561152] [Steps   24630] [G 0.0022354]
[2023-05-11 22:10:38.248158] [Steps   24640] [G 0.0030833]
[2023-05-11 22:10:54.850487] [Steps   24650] [G 0.0029452]
[2023-05-11 22:11:11.442182] [Steps   24660] [G 0.0034735]
[2023-05-11 22:11:28.139785] [Steps   24670] [G 0.0018325]
[2023-05-11 22:11:44.730871] [Steps   24680] [G 0.0025021]
[2023-05-11 22:12:01.365917] [Steps   24690] [G 0.0033441]
[2023-05-11 22:12:17.966826] [Steps   24700] [G 0.0028853]
[2023-05-11 22:12:34.653185] [Steps   24710] [G 0.0025750]
[2023-05-11 22:12:51.276963] [Steps   24720] [G 0.0023679]
[2023-05-11 22:13:07.918232] [Steps   24730] [G 0.0022720]
[2023-05-11 22:13:24.701582] [Steps   24740] [G 0.0021466]
[2023-05-11 22:13:41.306121] [Steps   24750] [G 0.0023965]
[2023-05-11 22:13:57.902826] [Steps   24760] [G 0.0027306]
[2023-05-11 22:14:16.616709] [Steps   24770] [G 0.0019612]
[2023-05-11 22:14:33.181862] [Steps   24780] [G 0.0024550]
[2023-05-11 22:14:49.775519] [Steps   24790] [G 0.0022301]
[2023-05-11 22:15:06.391400] [Steps   24800] [G 0.0021872]
[2023-05-11 22:15:23.118482] [Steps   24810] [G 0.0025246]
[2023-05-11 22:15:39.716697] [Steps   24820] [G 0.0025322]
[2023-05-11 22:15:56.413534] [Steps   24830] [G 0.0020664]
[2023-05-11 22:16:13.184004] [Steps   24840] [G 0.0026489]
[2023-05-11 22:16:29.812869] [Steps   24850] [G 0.0027179]
[2023-05-11 22:16:46.432654] [Steps   24860] [G 0.0023402]
[2023-05-11 22:17:03.028990] [Steps   24870] [G 0.0020690]
[2023-05-11 22:17:19.754001] [Steps   24880] [G 0.0028202]
[2023-05-11 22:17:36.356443] [Steps   24890] [G 0.0026811]
[2023-05-11 22:17:52.968794] [Steps   24900] [G 0.0021961]
[2023-05-11 22:18:09.686623] [Steps   24910] [G 0.0024185]
[2023-05-11 22:18:26.294282] [Steps   24920] [G 0.0026357]
[2023-05-11 22:18:42.895763] [Steps   24930] [G 0.0022520]
[2023-05-11 22:18:59.572785] [Steps   24940] [G 0.0024584]
[2023-05-11 22:19:16.190475] [Steps   24950] [G 0.0026985]
[2023-05-11 22:19:32.817520] [Steps   24960] [G 0.0021025]
[2023-05-11 22:19:49.423216] [Steps   24970] [G 0.0025399]
[2023-05-11 22:20:06.103107] [Steps   24980] [G 0.0027801]
[2023-05-11 22:20:22.701313] [Steps   24990] [G 0.0029889]
[2023-05-11 22:20:39.373849] [Steps   25000] [G 0.0020619]
Steps 25001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 22:20:57.494441] [Steps   25010] [G 0.0021870]
[2023-05-11 22:21:14.110501] [Steps   25020] [G 0.0037896]
[2023-05-11 22:21:30.794983] [Steps   25030] [G 0.0023475]
[2023-05-11 22:21:47.478142] [Steps   25040] [G 0.0024327]
[2023-05-11 22:22:04.117841] [Steps   25050] [G 0.0022893]
[2023-05-11 22:22:20.827632] [Steps   25060] [G 0.0026526]
[2023-05-11 22:22:39.236208] [Steps   25070] [G 0.0026492]
[2023-05-11 22:22:55.847714] [Steps   25080] [G 0.0019396]
[2023-05-11 22:23:12.476675] [Steps   25090] [G 0.0021529]
[2023-05-11 22:23:29.184073] [Steps   25100] [G 0.0021560]
[2023-05-11 22:23:45.815830] [Steps   25110] [G 0.0027358]
[2023-05-11 22:24:02.474288] [Steps   25120] [G 0.0020949]
[2023-05-11 22:24:19.160048] [Steps   25130] [G 0.0024860]
[2023-05-11 22:24:35.823206] [Steps   25140] [G 0.0020216]
[2023-05-11 22:24:52.443384] [Steps   25150] [G 0.0026389]
[2023-05-11 22:25:09.126230] [Steps   25160] [G 0.0033322]
[2023-05-11 22:25:25.848532] [Steps   25170] [G 0.0022769]
[2023-05-11 22:25:42.494378] [Steps   25180] [G 0.0017795]
[2023-05-11 22:25:59.163284] [Steps   25190] [G 0.0019601]
[2023-05-11 22:26:15.885588] [Steps   25200] [G 0.0025687]
[2023-05-11 22:26:32.525147] [Steps   25210] [G 0.0018354]
[2023-05-11 22:26:49.166353] [Steps   25220] [G 0.0024246]
[2023-05-11 22:27:05.894151] [Steps   25230] [G 0.0019350]
[2023-05-11 22:27:22.541668] [Steps   25240] [G 0.0024971]
[2023-05-11 22:27:39.169187] [Steps   25250] [G 0.0026566]
[2023-05-11 22:27:55.824614] [Steps   25260] [G 0.0023812]
[2023-05-11 22:28:12.554469] [Steps   25270] [G 0.0018042]
[2023-05-11 22:28:29.211637] [Steps   25280] [G 0.0018312]
[2023-05-11 22:28:45.823379] [Steps   25290] [G 0.0022607]
[2023-05-11 22:29:02.557212] [Steps   25300] [G 0.0020022]
[2023-05-11 22:29:19.182505] [Steps   25310] [G 0.0028372]
[2023-05-11 22:29:35.812452] [Steps   25320] [G 0.0025917]
[2023-05-11 22:29:52.444766] [Steps   25330] [G 0.0018967]
[2023-05-11 22:30:09.193937] [Steps   25340] [G 0.0027014]
[2023-05-11 22:30:25.847210] [Steps   25350] [G 0.0026750]
[2023-05-11 22:30:42.494157] [Steps   25360] [G 0.0023064]
[2023-05-11 22:31:01.104497] [Steps   25370] [G 0.0022753]
[2023-05-11 22:31:17.680430] [Steps   25380] [G 0.0036319]
[2023-05-11 22:31:34.255442] [Steps   25390] [G 0.0021629]
[2023-05-11 22:31:50.945923] [Steps   25400] [G 0.0024840]
[2023-05-11 22:32:07.575173] [Steps   25410] [G 0.0024180]
[2023-05-11 22:32:24.182161] [Steps   25420] [G 0.0024280]
[2023-05-11 22:32:40.787817] [Steps   25430] [G 0.0023242]
[2023-05-11 22:32:57.487213] [Steps   25440] [G 0.0027394]
[2023-05-11 22:33:14.086813] [Steps   25450] [G 0.0029518]
[2023-05-11 22:33:30.714320] [Steps   25460] [G 0.0022425]
[2023-05-11 22:33:47.416241] [Steps   25470] [G 0.0028790]
[2023-05-11 22:34:04.022472] [Steps   25480] [G 0.0027624]
[2023-05-11 22:34:20.608903] [Steps   25490] [G 0.0017018]
[2023-05-11 22:34:37.228423] [Steps   25500] [G 0.0019158]
[2023-05-11 22:34:53.912766] [Steps   25510] [G 0.0026172]
[2023-05-11 22:35:10.515950] [Steps   25520] [G 0.0016332]
[2023-05-11 22:35:27.149372] [Steps   25530] [G 0.0024879]
[2023-05-11 22:35:43.842293] [Steps   25540] [G 0.0019454]
[2023-05-11 22:36:00.454808] [Steps   25550] [G 0.0035355]
[2023-05-11 22:36:17.059894] [Steps   25560] [G 0.0025978]
[2023-05-11 22:36:33.731894] [Steps   25570] [G 0.0026993]
[2023-05-11 22:36:50.364289] [Steps   25580] [G 0.0030607]
[2023-05-11 22:37:06.979889] [Steps   25590] [G 0.0022611]
[2023-05-11 22:37:23.590221] [Steps   25600] [G 0.0020486]
[2023-05-11 22:37:40.293144] [Steps   25610] [G 0.0021116]
[2023-05-11 22:37:56.904793] [Steps   25620] [G 0.0020894]
[2023-05-11 22:38:13.524967] [Steps   25630] [G 0.0024273]
[2023-05-11 22:38:30.193835] [Steps   25640] [G 0.0034766]
[2023-05-11 22:38:46.806945] [Steps   25650] [G 0.0025767]
[2023-05-11 22:39:03.404444] [Steps   25660] [G 0.0022064]
[2023-05-11 22:39:19.981403] [Steps   25670] [G 0.0021972]
[2023-05-11 22:39:38.774465] [Steps   25680] [G 0.0019106]
[2023-05-11 22:39:55.422916] [Steps   25690] [G 0.0021598]
[2023-05-11 22:40:12.019763] [Steps   25700] [G 0.0024315]
[2023-05-11 22:40:28.688926] [Steps   25710] [G 0.0017723]
[2023-05-11 22:40:45.297546] [Steps   25720] [G 0.0017966]
[2023-05-11 22:41:01.897817] [Steps   25730] [G 0.0023592]
[2023-05-11 22:41:18.584736] [Steps   25740] [G 0.0026226]
[2023-05-11 22:41:35.217726] [Steps   25750] [G 0.0019854]
[2023-05-11 22:41:51.837301] [Steps   25760] [G 0.0025780]
[2023-05-11 22:42:08.493294] [Steps   25770] [G 0.0021750]
[2023-05-11 22:42:25.195491] [Steps   25780] [G 0.0023481]
[2023-05-11 22:42:41.810773] [Steps   25790] [G 0.0024241]
[2023-05-11 22:42:58.437877] [Steps   25800] [G 0.0034646]
[2023-05-11 22:43:15.141231] [Steps   25810] [G 0.0023118]
[2023-05-11 22:43:31.769159] [Steps   25820] [G 0.0022854]
[2023-05-11 22:43:48.372554] [Steps   25830] [G 0.0037126]
[2023-05-11 22:44:05.063618] [Steps   25840] [G 0.0023789]
[2023-05-11 22:44:21.840147] [Steps   25850] [G 0.0024448]
[2023-05-11 22:44:38.493311] [Steps   25860] [G 0.0021518]
[2023-05-11 22:44:55.172674] [Steps   25870] [G 0.0019694]
[2023-05-11 22:45:11.915529] [Steps   25880] [G 0.0017658]
[2023-05-11 22:45:28.590950] [Steps   25890] [G 0.0022110]
[2023-05-11 22:45:45.262933] [Steps   25900] [G 0.0028173]
[2023-05-11 22:46:02.001457] [Steps   25910] [G 0.0019151]
[2023-05-11 22:46:18.650757] [Steps   25920] [G 0.0022605]
[2023-05-11 22:46:35.323028] [Steps   25930] [G 0.0022901]
[2023-05-11 22:46:51.999541] [Steps   25940] [G 0.0021764]
[2023-05-11 22:47:08.702190] [Steps   25950] [G 0.0026608]
[2023-05-11 22:47:25.343396] [Steps   25960] [G 0.0018726]
[2023-05-11 22:47:41.941419] [Steps   25970] [G 0.0025150]
[2023-05-11 22:48:00.632878] [Steps   25980] [G 0.0019895]
[2023-05-11 22:48:17.242612] [Steps   25990] [G 0.0026142]
[2023-05-11 22:48:33.911104] [Steps   26000] [G 0.0028189]
Steps 26001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 22:48:51.968571] [Steps   26010] [G 0.0023243]
[2023-05-11 22:49:08.680659] [Steps   26020] [G 0.0019813]
[2023-05-11 22:49:25.377418] [Steps   26030] [G 0.0022352]
[2023-05-11 22:49:42.129333] [Steps   26040] [G 0.0019179]
[2023-05-11 22:49:58.851561] [Steps   26050] [G 0.0017743]
[2023-05-11 22:50:15.527593] [Steps   26060] [G 0.0024624]
[2023-05-11 22:50:32.275754] [Steps   26070] [G 0.0018732]
[2023-05-11 22:50:48.953810] [Steps   26080] [G 0.0025237]
[2023-05-11 22:51:05.685148] [Steps   26090] [G 0.0025969]
[2023-05-11 22:51:22.428266] [Steps   26100] [G 0.0018706]
[2023-05-11 22:51:39.121234] [Steps   26110] [G 0.0022587]
[2023-05-11 22:51:55.853147] [Steps   26120] [G 0.0016872]
[2023-05-11 22:52:12.507917] [Steps   26130] [G 0.0027357]
[2023-05-11 22:52:29.242429] [Steps   26140] [G 0.0021783]
[2023-05-11 22:52:45.933151] [Steps   26150] [G 0.0020132]
[2023-05-11 22:53:02.682144] [Steps   26160] [G 0.0030849]
[2023-05-11 22:53:19.454760] [Steps   26170] [G 0.0032737]
[2023-05-11 22:53:36.121111] [Steps   26180] [G 0.0025574]
[2023-05-11 22:53:52.842576] [Steps   26190] [G 0.0034015]
[2023-05-11 22:54:09.527359] [Steps   26200] [G 0.0026965]
[2023-05-11 22:54:26.277452] [Steps   26210] [G 0.0037888]
[2023-05-11 22:54:43.011456] [Steps   26220] [G 0.0018936]
[2023-05-11 22:54:59.675524] [Steps   26230] [G 0.0034931]
[2023-05-11 22:55:16.460284] [Steps   26240] [G 0.0026354]
[2023-05-11 22:55:33.112002] [Steps   26250] [G 0.0041415]
[2023-05-11 22:55:49.876555] [Steps   26260] [G 0.0018581]
[2023-05-11 22:56:06.556752] [Steps   26270] [G 0.0021143]
[2023-05-11 22:56:25.095154] [Steps   26280] [G 0.0029331]
[2023-05-11 22:56:41.816282] [Steps   26290] [G 0.0017993]
[2023-05-11 22:56:58.451777] [Steps   26300] [G 0.0025542]
[2023-05-11 22:57:15.177559] [Steps   26310] [G 0.0019701]
[2023-05-11 22:57:31.847610] [Steps   26320] [G 0.0020675]
[2023-05-11 22:57:48.561972] [Steps   26330] [G 0.0026707]
[2023-05-11 22:58:05.270663] [Steps   26340] [G 0.0020334]
[2023-05-11 22:58:21.957726] [Steps   26350] [G 0.0022042]
[2023-05-11 22:58:38.632190] [Steps   26360] [G 0.0019628]
[2023-05-11 22:58:55.332971] [Steps   26370] [G 0.0024543]
[2023-05-11 22:59:12.069423] [Steps   26380] [G 0.0019791]
[2023-05-11 22:59:28.785596] [Steps   26390] [G 0.0020090]
[2023-05-11 22:59:45.478027] [Steps   26400] [G 0.0023799]
[2023-05-11 23:00:02.211446] [Steps   26410] [G 0.0027888]
[2023-05-11 23:00:18.901781] [Steps   26420] [G 0.0026841]
[2023-05-11 23:00:35.573425] [Steps   26430] [G 0.0023172]
[2023-05-11 23:00:52.318022] [Steps   26440] [G 0.0021734]
[2023-05-11 23:01:09.025717] [Steps   26450] [G 0.0022397]
[2023-05-11 23:01:25.693623] [Steps   26460] [G 0.0022485]
[2023-05-11 23:01:42.364536] [Steps   26470] [G 0.0024254]
[2023-05-11 23:01:59.150760] [Steps   26480] [G 0.0021009]
[2023-05-11 23:02:15.832635] [Steps   26490] [G 0.0025469]
[2023-05-11 23:02:32.497644] [Steps   26500] [G 0.0022598]
[2023-05-11 23:02:49.295397] [Steps   26510] [G 0.0017605]
[2023-05-11 23:03:05.988511] [Steps   26520] [G 0.0024658]
[2023-05-11 23:03:22.652032] [Steps   26530] [G 0.0024314]
[2023-05-11 23:03:39.324373] [Steps   26540] [G 0.0016155]
[2023-05-11 23:03:56.080222] [Steps   26550] [G 0.0033733]
[2023-05-11 23:04:12.780847] [Steps   26560] [G 0.0021024]
[2023-05-11 23:04:29.422413] [Steps   26570] [G 0.0029318]
[2023-05-11 23:04:48.023398] [Steps   26580] [G 0.0020333]
[2023-05-11 23:05:04.743191] [Steps   26590] [G 0.0019864]
[2023-05-11 23:05:21.376034] [Steps   26600] [G 0.0015830]
[2023-05-11 23:05:38.112233] [Steps   26610] [G 0.0022559]
[2023-05-11 23:05:54.722486] [Steps   26620] [G 0.0018768]
[2023-05-11 23:06:11.410016] [Steps   26630] [G 0.0021845]
[2023-05-11 23:06:28.096579] [Steps   26640] [G 0.0018399]
[2023-05-11 23:06:44.850046] [Steps   26650] [G 0.0019496]
[2023-05-11 23:07:01.536609] [Steps   26660] [G 0.0021833]
[2023-05-11 23:07:18.209592] [Steps   26670] [G 0.0038832]
[2023-05-11 23:07:34.971148] [Steps   26680] [G 0.0020216]
[2023-05-11 23:07:51.644568] [Steps   26690] [G 0.0019233]
[2023-05-11 23:08:08.336806] [Steps   26700] [G 0.0020616]
[2023-05-11 23:08:25.023045] [Steps   26710] [G 0.0023652]
[2023-05-11 23:08:41.745360] [Steps   26720] [G 0.0030039]
[2023-05-11 23:08:58.450307] [Steps   26730] [G 0.0019397]
[2023-05-11 23:09:15.081885] [Steps   26740] [G 0.0023524]
[2023-05-11 23:09:31.839790] [Steps   26750] [G 0.0020605]
[2023-05-11 23:09:48.503829] [Steps   26760] [G 0.0025313]
[2023-05-11 23:10:05.230847] [Steps   26770] [G 0.0024688]
[2023-05-11 23:10:21.952090] [Steps   26780] [G 0.0016738]
[2023-05-11 23:10:38.645632] [Steps   26790] [G 0.0027729]
[2023-05-11 23:10:55.269584] [Steps   26800] [G 0.0020120]
[2023-05-11 23:11:11.941216] [Steps   26810] [G 0.0016753]
[2023-05-11 23:11:28.713292] [Steps   26820] [G 0.0027219]
[2023-05-11 23:11:45.371539] [Steps   26830] [G 0.0019851]
[2023-05-11 23:12:02.058094] [Steps   26840] [G 0.0021745]
[2023-05-11 23:12:18.804422] [Steps   26850] [G 0.0023336]
[2023-05-11 23:12:35.477085] [Steps   26860] [G 0.0026060]
[2023-05-11 23:12:52.147381] [Steps   26870] [G 0.0024354]
[2023-05-11 23:13:10.708155] [Steps   26880] [G 0.0019330]
[2023-05-11 23:13:27.390273] [Steps   26890] [G 0.0020339]
[2023-05-11 23:13:43.981437] [Steps   26900] [G 0.0032701]
[2023-05-11 23:14:00.617772] [Steps   26910] [G 0.0021991]
[2023-05-11 23:14:17.290394] [Steps   26920] [G 0.0017321]
[2023-05-11 23:14:33.900047] [Steps   26930] [G 0.0018300]
[2023-05-11 23:14:50.558710] [Steps   26940] [G 0.0019754]
[2023-05-11 23:15:07.288896] [Steps   26950] [G 0.0019505]
[2023-05-11 23:15:23.948291] [Steps   26960] [G 0.0022705]
[2023-05-11 23:15:40.611357] [Steps   26970] [G 0.0014911]
[2023-05-11 23:15:57.280019] [Steps   26980] [G 0.0018059]
[2023-05-11 23:16:13.963963] [Steps   26990] [G 0.0021473]
[2023-05-11 23:16:30.618196] [Steps   27000] [G 0.0018972]
Steps 27001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 23:16:48.657220] [Steps   27010] [G 0.0019335]
[2023-05-11 23:17:05.442236] [Steps   27020] [G 0.0029637]
[2023-05-11 23:17:22.121574] [Steps   27030] [G 0.0022634]
[2023-05-11 23:17:38.860210] [Steps   27040] [G 0.0018802]
[2023-05-11 23:17:55.523201] [Steps   27050] [G 0.0019364]
[2023-05-11 23:18:12.192856] [Steps   27060] [G 0.0027617]
[2023-05-11 23:18:28.945421] [Steps   27070] [G 0.0018458]
[2023-05-11 23:18:45.606610] [Steps   27080] [G 0.0018090]
[2023-05-11 23:19:02.276749] [Steps   27090] [G 0.0022798]
[2023-05-11 23:19:18.967382] [Steps   27100] [G 0.0028821]
[2023-05-11 23:19:35.651387] [Steps   27110] [G 0.0016230]
[2023-05-11 23:19:52.336152] [Steps   27120] [G 0.0026874]
[2023-05-11 23:20:09.007695] [Steps   27130] [G 0.0023138]
[2023-05-11 23:20:25.790957] [Steps   27140] [G 0.0028826]
[2023-05-11 23:20:42.428238] [Steps   27150] [G 0.0021159]
[2023-05-11 23:20:59.084199] [Steps   27160] [G 0.0021117]
[2023-05-11 23:21:15.706526] [Steps   27170] [G 0.0017666]
[2023-05-11 23:21:32.365785] [Steps   27180] [G 0.0017079]
[2023-05-11 23:21:51.134122] [Steps   27190] [G 0.0019526]
[2023-05-11 23:22:07.723827] [Steps   27200] [G 0.0018292]
[2023-05-11 23:22:24.407437] [Steps   27210] [G 0.0012869]
[2023-05-11 23:22:41.074484] [Steps   27220] [G 0.0026279]
[2023-05-11 23:22:57.741118] [Steps   27230] [G 0.0019711]
[2023-05-11 23:23:14.471063] [Steps   27240] [G 0.0019973]
[2023-05-11 23:23:31.076765] [Steps   27250] [G 0.0017896]
[2023-05-11 23:23:47.739448] [Steps   27260] [G 0.0017750]
[2023-05-11 23:24:04.403708] [Steps   27270] [G 0.0028107]
[2023-05-11 23:24:21.115068] [Steps   27280] [G 0.0015526]
[2023-05-11 23:24:37.737222] [Steps   27290] [G 0.0026387]
[2023-05-11 23:24:54.362275] [Steps   27300] [G 0.0018371]
[2023-05-11 23:25:11.113442] [Steps   27310] [G 0.0026243]
[2023-05-11 23:25:27.729844] [Steps   27320] [G 0.0023611]
[2023-05-11 23:25:44.370326] [Steps   27330] [G 0.0021387]
[2023-05-11 23:26:01.013723] [Steps   27340] [G 0.0023031]
[2023-05-11 23:26:17.735394] [Steps   27350] [G 0.0024798]
[2023-05-11 23:26:34.403317] [Steps   27360] [G 0.0029021]
[2023-05-11 23:26:51.016843] [Steps   27370] [G 0.0021199]
[2023-05-11 23:27:07.827260] [Steps   27380] [G 0.0016980]
[2023-05-11 23:27:24.470407] [Steps   27390] [G 0.0019158]
[2023-05-11 23:27:41.157363] [Steps   27400] [G 0.0018527]
[2023-05-11 23:27:57.854688] [Steps   27410] [G 0.0022675]
[2023-05-11 23:28:14.477490] [Steps   27420] [G 0.0022248]
[2023-05-11 23:28:31.120890] [Steps   27430] [G 0.0023400]
[2023-05-11 23:28:47.772407] [Steps   27440] [G 0.0019483]
[2023-05-11 23:29:04.502346] [Steps   27450] [G 0.0022557]
[2023-05-11 23:29:21.118917] [Steps   27460] [G 0.0019396]
[2023-05-11 23:29:37.749442] [Steps   27470] [G 0.0017576]
[2023-05-11 23:29:54.380648] [Steps   27480] [G 0.0025196]
[2023-05-11 23:30:12.985748] [Steps   27490] [G 0.0016690]
[2023-05-11 23:30:29.649875] [Steps   27500] [G 0.0023388]
[2023-05-11 23:30:46.245849] [Steps   27510] [G 0.0022938]
[2023-05-11 23:31:02.918984] [Steps   27520] [G 0.0019909]
[2023-05-11 23:31:19.492409] [Steps   27530] [G 0.0024251]
[2023-05-11 23:31:36.095903] [Steps   27540] [G 0.0032732]
[2023-05-11 23:31:52.754217] [Steps   27550] [G 0.0022056]
[2023-05-11 23:32:09.357489] [Steps   27560] [G 0.0014157]
[2023-05-11 23:32:26.049299] [Steps   27570] [G 0.0020371]
[2023-05-11 23:32:42.644934] [Steps   27580] [G 0.0026442]
[2023-05-11 23:32:59.367384] [Steps   27590] [G 0.0030886]
[2023-05-11 23:33:15.969140] [Steps   27600] [G 0.0024431]
[2023-05-11 23:33:32.596368] [Steps   27610] [G 0.0023374]
[2023-05-11 23:33:49.281564] [Steps   27620] [G 0.0016346]
[2023-05-11 23:34:05.903626] [Steps   27630] [G 0.0015957]
[2023-05-11 23:34:22.522474] [Steps   27640] [G 0.0026436]
[2023-05-11 23:34:39.215732] [Steps   27650] [G 0.0056230]
[2023-05-11 23:34:55.827293] [Steps   27660] [G 0.0019504]
[2023-05-11 23:35:12.431600] [Steps   27670] [G 0.0020606]
[2023-05-11 23:35:29.039540] [Steps   27680] [G 0.0026695]
[2023-05-11 23:35:45.703289] [Steps   27690] [G 0.0017875]
[2023-05-11 23:36:02.306243] [Steps   27700] [G 0.0019233]
[2023-05-11 23:36:18.924176] [Steps   27710] [G 0.0019003]
[2023-05-11 23:36:35.598610] [Steps   27720] [G 0.0018545]
[2023-05-11 23:36:52.190037] [Steps   27730] [G 0.0018073]
[2023-05-11 23:37:08.785077] [Steps   27740] [G 0.0015893]
[2023-05-11 23:37:25.406844] [Steps   27750] [G 0.0016744]
[2023-05-11 23:37:42.095870] [Steps   27760] [G 0.0023800]
[2023-05-11 23:37:58.683755] [Steps   27770] [G 0.0026689]
[2023-05-11 23:38:15.248889] [Steps   27780] [G 0.0019448]
[2023-05-11 23:38:33.900255] [Steps   27790] [G 0.0021896]
[2023-05-11 23:38:50.560573] [Steps   27800] [G 0.0018075]
[2023-05-11 23:39:07.148009] [Steps   27810] [G 0.0020160]
[2023-05-11 23:39:23.870104] [Steps   27820] [G 0.0019824]
[2023-05-11 23:39:40.504432] [Steps   27830] [G 0.0022949]
[2023-05-11 23:39:57.118624] [Steps   27840] [G 0.0021105]
[2023-05-11 23:40:13.738873] [Steps   27850] [G 0.0022501]
[2023-05-11 23:40:30.445987] [Steps   27860] [G 0.0023047]
[2023-05-11 23:40:47.058839] [Steps   27870] [G 0.0018035]
[2023-05-11 23:41:03.661450] [Steps   27880] [G 0.0019775]
[2023-05-11 23:41:20.362791] [Steps   27890] [G 0.0019237]
[2023-05-11 23:41:36.981788] [Steps   27900] [G 0.0026700]
[2023-05-11 23:41:53.595371] [Steps   27910] [G 0.0024177]
[2023-05-11 23:42:10.200769] [Steps   27920] [G 0.0025204]
[2023-05-11 23:42:26.900959] [Steps   27930] [G 0.0017913]
[2023-05-11 23:42:43.569351] [Steps   27940] [G 0.0021957]
[2023-05-11 23:43:00.173546] [Steps   27950] [G 0.0019684]
[2023-05-11 23:43:16.904313] [Steps   27960] [G 0.0013459]
[2023-05-11 23:43:33.522969] [Steps   27970] [G 0.0014562]
[2023-05-11 23:43:50.147740] [Steps   27980] [G 0.0030613]
[2023-05-11 23:44:06.832296] [Steps   27990] [G 0.0021256]
[2023-05-11 23:44:23.435595] [Steps   28000] [G 0.0021564]
Steps 28001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-11 23:44:41.507765] [Steps   28010] [G 0.0021483]
[2023-05-11 23:44:58.133042] [Steps   28020] [G 0.0020926]
[2023-05-11 23:45:14.856525] [Steps   28030] [G 0.0024542]
[2023-05-11 23:45:31.467431] [Steps   28040] [G 0.0023585]
[2023-05-11 23:45:48.144391] [Steps   28050] [G 0.0018110]
[2023-05-11 23:46:04.795145] [Steps   28060] [G 0.0020901]
[2023-05-11 23:46:21.478042] [Steps   28070] [G 0.0021001]
[2023-05-11 23:46:38.083383] [Steps   28080] [G 0.0023329]
[2023-05-11 23:46:56.495432] [Steps   28090] [G 0.0018602]
[2023-05-11 23:47:13.201112] [Steps   28100] [G 0.0020690]
[2023-05-11 23:47:29.778654] [Steps   28110] [G 0.0018112]
[2023-05-11 23:47:46.497849] [Steps   28120] [G 0.0020174]
[2023-05-11 23:48:03.129748] [Steps   28130] [G 0.0021080]
[2023-05-11 23:48:19.820884] [Steps   28140] [G 0.0017553]
[2023-05-11 23:48:36.440528] [Steps   28150] [G 0.0020382]
[2023-05-11 23:48:53.054861] [Steps   28160] [G 0.0018128]
[2023-05-11 23:49:09.759804] [Steps   28170] [G 0.0018182]
[2023-05-11 23:49:26.361201] [Steps   28180] [G 0.0022399]
[2023-05-11 23:49:42.976057] [Steps   28190] [G 0.0018987]
[2023-05-11 23:49:59.593602] [Steps   28200] [G 0.0019827]
[2023-05-11 23:50:16.283510] [Steps   28210] [G 0.0023758]
[2023-05-11 23:50:32.897913] [Steps   28220] [G 0.0023875]
[2023-05-11 23:50:49.463474] [Steps   28230] [G 0.0020781]
[2023-05-11 23:51:06.178249] [Steps   28240] [G 0.0023427]
[2023-05-11 23:51:22.769611] [Steps   28250] [G 0.0018334]
[2023-05-11 23:51:39.363800] [Steps   28260] [G 0.0018662]
[2023-05-11 23:51:56.046619] [Steps   28270] [G 0.0025101]
[2023-05-11 23:52:12.665199] [Steps   28280] [G 0.0018187]
[2023-05-11 23:52:29.285018] [Steps   28290] [G 0.0016620]
[2023-05-11 23:52:45.882052] [Steps   28300] [G 0.0018510]
[2023-05-11 23:53:02.599386] [Steps   28310] [G 0.0028638]
[2023-05-11 23:53:19.202857] [Steps   28320] [G 0.0020917]
[2023-05-11 23:53:35.845312] [Steps   28330] [G 0.0022575]
[2023-05-11 23:53:52.540808] [Steps   28340] [G 0.0029169]
[2023-05-11 23:54:09.159022] [Steps   28350] [G 0.0019333]
[2023-05-11 23:54:25.787810] [Steps   28360] [G 0.0014204]
[2023-05-11 23:54:42.396812] [Steps   28370] [G 0.0028615]
[2023-05-11 23:54:59.099117] [Steps   28380] [G 0.0023633]
[2023-05-11 23:55:17.716591] [Steps   28390] [G 0.0020519]
[2023-05-11 23:55:34.283301] [Steps   28400] [G 0.0016020]
[2023-05-11 23:55:50.958370] [Steps   28410] [G 0.0017324]
[2023-05-11 23:56:07.556180] [Steps   28420] [G 0.0019882]
[2023-05-11 23:56:24.132404] [Steps   28430] [G 0.0020177]
[2023-05-11 23:56:40.794677] [Steps   28440] [G 0.0017799]
[2023-05-11 23:56:57.402422] [Steps   28450] [G 0.0022497]
[2023-05-11 23:57:13.995852] [Steps   28460] [G 0.0021389]
[2023-05-11 23:57:30.596865] [Steps   28470] [G 0.0022495]
[2023-05-11 23:57:47.284699] [Steps   28480] [G 0.0022886]
[2023-05-11 23:58:03.886671] [Steps   28490] [G 0.0029728]
[2023-05-11 23:58:20.472764] [Steps   28500] [G 0.0012650]
[2023-05-11 23:58:37.146687] [Steps   28510] [G 0.0036589]
[2023-05-11 23:58:53.765160] [Steps   28520] [G 0.0016026]
[2023-05-11 23:59:10.369998] [Steps   28530] [G 0.0017970]
[2023-05-11 23:59:26.966664] [Steps   28540] [G 0.0014773]
[2023-05-11 23:59:43.648680] [Steps   28550] [G 0.0023666]
[2023-05-12 00:00:00.238274] [Steps   28560] [G 0.0021468]
[2023-05-12 00:00:16.839938] [Steps   28570] [G 0.0019523]
[2023-05-12 00:00:33.494541] [Steps   28580] [G 0.0023418]
[2023-05-12 00:00:50.158105] [Steps   28590] [G 0.0021324]
[2023-05-12 00:01:06.764066] [Steps   28600] [G 0.0025576]
[2023-05-12 00:01:23.430569] [Steps   28610] [G 0.0014534]
[2023-05-12 00:01:40.041856] [Steps   28620] [G 0.0020546]
[2023-05-12 00:01:56.638391] [Steps   28630] [G 0.0022133]
[2023-05-12 00:02:13.245575] [Steps   28640] [G 0.0017913]
[2023-05-12 00:02:29.927124] [Steps   28650] [G 0.0021680]
[2023-05-12 00:02:46.541699] [Steps   28660] [G 0.0020154]
[2023-05-12 00:03:03.117096] [Steps   28670] [G 0.0013745]
[2023-05-12 00:03:19.756899] [Steps   28680] [G 0.0018217]
[2023-05-12 00:03:36.339879] [Steps   28690] [G 0.0021277]
[2023-05-12 00:03:54.925608] [Steps   28700] [G 0.0021355]
[2023-05-12 00:04:11.557317] [Steps   28710] [G 0.0016830]
[2023-05-12 00:04:28.306993] [Steps   28720] [G 0.0015967]
[2023-05-12 00:04:44.946833] [Steps   28730] [G 0.0015205]
[2023-05-12 00:05:01.589960] [Steps   28740] [G 0.0019080]
[2023-05-12 00:05:18.317858] [Steps   28750] [G 0.0019421]
[2023-05-12 00:05:34.952259] [Steps   28760] [G 0.0020621]
[2023-05-12 00:05:51.583286] [Steps   28770] [G 0.0017233]
[2023-05-12 00:06:08.293611] [Steps   28780] [G 0.0026509]
[2023-05-12 00:06:24.911504] [Steps   28790] [G 0.0021216]
[2023-05-12 00:06:41.555568] [Steps   28800] [G 0.0016111]
[2023-05-12 00:06:58.174595] [Steps   28810] [G 0.0025290]
[2023-05-12 00:07:14.910134] [Steps   28820] [G 0.0015089]
[2023-05-12 00:07:31.543279] [Steps   28830] [G 0.0014796]
[2023-05-12 00:07:48.148324] [Steps   28840] [G 0.0036313]
[2023-05-12 00:08:04.840869] [Steps   28850] [G 0.0031053]
[2023-05-12 00:08:21.456897] [Steps   28860] [G 0.0019230]
[2023-05-12 00:08:38.088068] [Steps   28870] [G 0.0016255]
[2023-05-12 00:08:54.708017] [Steps   28880] [G 0.0014687]
[2023-05-12 00:09:11.378270] [Steps   28890] [G 0.0018980]
[2023-05-12 00:09:28.004039] [Steps   28900] [G 0.0024754]
[2023-05-12 00:09:44.590861] [Steps   28910] [G 0.0023197]
[2023-05-12 00:10:01.265583] [Steps   28920] [G 0.0028347]
[2023-05-12 00:10:17.858924] [Steps   28930] [G 0.0018550]
[2023-05-12 00:10:34.476268] [Steps   28940] [G 0.0019662]
[2023-05-12 00:10:51.187428] [Steps   28950] [G 0.0019720]
[2023-05-12 00:11:07.805828] [Steps   28960] [G 0.0014882]
[2023-05-12 00:11:24.432364] [Steps   28970] [G 0.0018114]
[2023-05-12 00:11:41.048084] [Steps   28980] [G 0.0024387]
[2023-05-12 00:11:57.663033] [Steps   28990] [G 0.0016728]
[2023-05-12 00:12:16.321292] [Steps   29000] [G 0.0020671]
Steps 29001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 00:12:34.302486] [Steps   29010] [G 0.0018208]
[2023-05-12 00:12:51.014126] [Steps   29020] [G 0.0016302]
[2023-05-12 00:13:07.625143] [Steps   29030] [G 0.0019839]
[2023-05-12 00:13:24.292369] [Steps   29040] [G 0.0025295]
[2023-05-12 00:13:40.893979] [Steps   29050] [G 0.0024717]
[2023-05-12 00:13:57.507440] [Steps   29060] [G 0.0019160]
[2023-05-12 00:14:14.180860] [Steps   29070] [G 0.0024529]
[2023-05-12 00:14:30.792954] [Steps   29080] [G 0.0019589]
[2023-05-12 00:14:47.382100] [Steps   29090] [G 0.0024200]
[2023-05-12 00:15:03.980150] [Steps   29100] [G 0.0024710]
[2023-05-12 00:15:20.690313] [Steps   29110] [G 0.0015690]
[2023-05-12 00:15:37.291019] [Steps   29120] [G 0.0029514]
[2023-05-12 00:15:53.898584] [Steps   29130] [G 0.0012571]
[2023-05-12 00:16:10.589990] [Steps   29140] [G 0.0016244]
[2023-05-12 00:16:27.191474] [Steps   29150] [G 0.0018068]
[2023-05-12 00:16:43.790195] [Steps   29160] [G 0.0023415]
[2023-05-12 00:17:00.378573] [Steps   29170] [G 0.0024721]
[2023-05-12 00:17:17.080817] [Steps   29180] [G 0.0014128]
[2023-05-12 00:17:33.687788] [Steps   29190] [G 0.0016469]
[2023-05-12 00:17:50.269879] [Steps   29200] [G 0.0018511]
[2023-05-12 00:18:06.932430] [Steps   29210] [G 0.0015722]
[2023-05-12 00:18:23.570912] [Steps   29220] [G 0.0020610]
[2023-05-12 00:18:40.181581] [Steps   29230] [G 0.0028292]
[2023-05-12 00:18:56.833042] [Steps   29240] [G 0.0020355]
[2023-05-12 00:19:13.447457] [Steps   29250] [G 0.0022046]
[2023-05-12 00:19:30.056752] [Steps   29260] [G 0.0017457]
[2023-05-12 00:19:46.659660] [Steps   29270] [G 0.0018234]
[2023-05-12 00:20:03.333785] [Steps   29280] [G 0.0027789]
[2023-05-12 00:20:19.908220] [Steps   29290] [G 0.0023044]
[2023-05-12 00:20:38.323159] [Steps   29300] [G 0.0017612]
[2023-05-12 00:20:55.039461] [Steps   29310] [G 0.0013852]
[2023-05-12 00:21:11.664904] [Steps   29320] [G 0.0015491]
[2023-05-12 00:21:28.357035] [Steps   29330] [G 0.0023238]
[2023-05-12 00:21:44.968721] [Steps   29340] [G 0.0018848]
[2023-05-12 00:22:01.642899] [Steps   29350] [G 0.0023069]
[2023-05-12 00:22:18.287444] [Steps   29360] [G 0.0019159]
[2023-05-12 00:22:34.897594] [Steps   29370] [G 0.0030730]
[2023-05-12 00:22:51.583304] [Steps   29380] [G 0.0017251]
[2023-05-12 00:23:08.198536] [Steps   29390] [G 0.0017074]
[2023-05-12 00:23:24.844260] [Steps   29400] [G 0.0020199]
[2023-05-12 00:23:41.528238] [Steps   29410] [G 0.0016842]
[2023-05-12 00:23:58.140801] [Steps   29420] [G 0.0018364]
[2023-05-12 00:24:14.771380] [Steps   29430] [G 0.0020332]
[2023-05-12 00:24:31.394131] [Steps   29440] [G 0.0023175]
[2023-05-12 00:24:48.089451] [Steps   29450] [G 0.0019796]
[2023-05-12 00:25:04.712518] [Steps   29460] [G 0.0028691]
[2023-05-12 00:25:21.316976] [Steps   29470] [G 0.0027570]
[2023-05-12 00:25:38.008878] [Steps   29480] [G 0.0018780]
[2023-05-12 00:25:54.618748] [Steps   29490] [G 0.0019450]
[2023-05-12 00:26:11.230956] [Steps   29500] [G 0.0025072]
[2023-05-12 00:26:27.846834] [Steps   29510] [G 0.0028442]
[2023-05-12 00:26:44.556919] [Steps   29520] [G 0.0024829]
[2023-05-12 00:27:01.161501] [Steps   29530] [G 0.0015260]
[2023-05-12 00:27:17.766841] [Steps   29540] [G 0.0019999]
[2023-05-12 00:27:34.540908] [Steps   29550] [G 0.0022867]
[2023-05-12 00:27:51.201413] [Steps   29560] [G 0.0018374]
[2023-05-12 00:28:07.864914] [Steps   29570] [G 0.0035505]
[2023-05-12 00:28:24.572372] [Steps   29580] [G 0.0021782]
[2023-05-12 00:28:41.162260] [Steps   29590] [G 0.0017923]
[2023-05-12 00:28:59.881652] [Steps   29600] [G 0.0017969]
[2023-05-12 00:29:16.457756] [Steps   29610] [G 0.0022324]
[2023-05-12 00:29:33.185191] [Steps   29620] [G 0.0021034]
[2023-05-12 00:29:49.828460] [Steps   29630] [G 0.0018154]
[2023-05-12 00:30:06.541794] [Steps   29640] [G 0.0023295]
[2023-05-12 00:30:23.321462] [Steps   29650] [G 0.0016102]
[2023-05-12 00:30:40.004510] [Steps   29660] [G 0.0015721]
[2023-05-12 00:30:56.624073] [Steps   29670] [G 0.0024025]
[2023-05-12 00:31:13.220955] [Steps   29680] [G 0.0022958]
[2023-05-12 00:31:29.916098] [Steps   29690] [G 0.0041112]
[2023-05-12 00:31:46.536166] [Steps   29700] [G 0.0021428]
[2023-05-12 00:32:03.142531] [Steps   29710] [G 0.0018117]
[2023-05-12 00:32:19.815832] [Steps   29720] [G 0.0021230]
[2023-05-12 00:32:36.413774] [Steps   29730] [G 0.0022558]
[2023-05-12 00:32:53.022764] [Steps   29740] [G 0.0026652]
[2023-05-12 00:33:09.619546] [Steps   29750] [G 0.0014409]
[2023-05-12 00:33:26.301502] [Steps   29760] [G 0.0015758]
[2023-05-12 00:33:42.886033] [Steps   29770] [G 0.0020728]
[2023-05-12 00:33:59.497285] [Steps   29780] [G 0.0025786]
[2023-05-12 00:34:16.160941] [Steps   29790] [G 0.0018770]
[2023-05-12 00:34:32.762810] [Steps   29800] [G 0.0018865]
[2023-05-12 00:34:49.397455] [Steps   29810] [G 0.0017443]
[2023-05-12 00:35:06.078538] [Steps   29820] [G 0.0024393]
[2023-05-12 00:35:22.668435] [Steps   29830] [G 0.0021665]
[2023-05-12 00:35:39.274512] [Steps   29840] [G 0.0023264]
[2023-05-12 00:35:55.860983] [Steps   29850] [G 0.0023004]
[2023-05-12 00:36:12.536716] [Steps   29860] [G 0.0020871]
[2023-05-12 00:36:29.152353] [Steps   29870] [G 0.0015426]
[2023-05-12 00:36:45.757392] [Steps   29880] [G 0.0019279]
[2023-05-12 00:37:02.394523] [Steps   29890] [G 0.0017679]
[2023-05-12 00:37:20.881891] [Steps   29900] [G 0.0018584]
[2023-05-12 00:37:37.476240] [Steps   29910] [G 0.0022325]
[2023-05-12 00:37:54.049346] [Steps   29920] [G 0.0018060]
[2023-05-12 00:38:10.718727] [Steps   29930] [G 0.0020506]
[2023-05-12 00:38:27.289010] [Steps   29940] [G 0.0030598]
[2023-05-12 00:38:43.901856] [Steps   29950] [G 0.0022338]
[2023-05-12 00:39:00.626228] [Steps   29960] [G 0.0015957]
[2023-05-12 00:39:17.221469] [Steps   29970] [G 0.0026286]
[2023-05-12 00:39:33.828446] [Steps   29980] [G 0.0016976]
[2023-05-12 00:39:50.512571] [Steps   29990] [G 0.0022315]
[2023-05-12 00:40:07.115347] [Steps   30000] [G 0.0015744]
Steps 30001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 00:40:26.174127] [Steps   30010] [G 0.0024474]
[2023-05-12 00:40:42.813841] [Steps   30020] [G 0.0016417]
[2023-05-12 00:40:59.470897] [Steps   30030] [G 0.0016096]
[2023-05-12 00:41:16.045289] [Steps   30040] [G 0.0022941]
[2023-05-12 00:41:32.743224] [Steps   30050] [G 0.0015535]
[2023-05-12 00:41:49.448289] [Steps   30060] [G 0.0019466]
[2023-05-12 00:42:06.041311] [Steps   30070] [G 0.0018798]
[2023-05-12 00:42:22.690636] [Steps   30080] [G 0.0028804]
[2023-05-12 00:42:39.303921] [Steps   30090] [G 0.0025954]
[2023-05-12 00:42:55.933253] [Steps   30100] [G 0.0015564]
[2023-05-12 00:43:12.644658] [Steps   30110] [G 0.0017351]
[2023-05-12 00:43:29.391442] [Steps   30120] [G 0.0017228]
[2023-05-12 00:43:46.011814] [Steps   30130] [G 0.0013836]
[2023-05-12 00:44:02.638634] [Steps   30140] [G 0.0025248]
[2023-05-12 00:44:19.329626] [Steps   30150] [G 0.0023022]
[2023-05-12 00:44:35.934629] [Steps   30160] [G 0.0016052]
[2023-05-12 00:44:52.510028] [Steps   30170] [G 0.0018743]
[2023-05-12 00:45:09.180662] [Steps   30180] [G 0.0016176]
[2023-05-12 00:45:25.767927] [Steps   30190] [G 0.0016216]
[2023-05-12 00:45:42.350945] [Steps   30200] [G 0.0019899]
[2023-05-12 00:46:00.902345] [Steps   30210] [G 0.0017242]
[2023-05-12 00:46:17.565513] [Steps   30220] [G 0.0021273]
[2023-05-12 00:46:34.243085] [Steps   30230] [G 0.0018325]
[2023-05-12 00:46:50.822857] [Steps   30240] [G 0.0021785]
[2023-05-12 00:47:07.483286] [Steps   30250] [G 0.0014268]
[2023-05-12 00:47:24.066483] [Steps   30260] [G 0.0013189]
[2023-05-12 00:47:40.672878] [Steps   30270] [G 0.0024151]
[2023-05-12 00:47:57.269691] [Steps   30280] [G 0.0012498]
[2023-05-12 00:48:13.934587] [Steps   30290] [G 0.0021017]
[2023-05-12 00:48:30.540342] [Steps   30300] [G 0.0016746]
[2023-05-12 00:48:47.135741] [Steps   30310] [G 0.0023993]
[2023-05-12 00:49:03.820949] [Steps   30320] [G 0.0024657]
[2023-05-12 00:49:20.408326] [Steps   30330] [G 0.0018563]
[2023-05-12 00:49:37.081707] [Steps   30340] [G 0.0018259]
[2023-05-12 00:49:53.723859] [Steps   30350] [G 0.0018113]
[2023-05-12 00:50:10.410523] [Steps   30360] [G 0.0015175]
[2023-05-12 00:50:27.053370] [Steps   30370] [G 0.0018773]
[2023-05-12 00:50:43.667538] [Steps   30380] [G 0.0013837]
[2023-05-12 00:51:00.332140] [Steps   30390] [G 0.0016217]
[2023-05-12 00:51:16.948295] [Steps   30400] [G 0.0018979]
[2023-05-12 00:51:33.540104] [Steps   30410] [G 0.0017200]
[2023-05-12 00:51:50.230546] [Steps   30420] [G 0.0019553]
[2023-05-12 00:52:06.826050] [Steps   30430] [G 0.0022372]
[2023-05-12 00:52:23.430447] [Steps   30440] [G 0.0027212]
[2023-05-12 00:52:40.032805] [Steps   30450] [G 0.0020522]
[2023-05-12 00:52:56.711227] [Steps   30460] [G 0.0018899]
[2023-05-12 00:53:13.310004] [Steps   30470] [G 0.0023318]
[2023-05-12 00:53:29.915223] [Steps   30480] [G 0.0020183]
[2023-05-12 00:53:46.570151] [Steps   30490] [G 0.0016698]
[2023-05-12 00:54:03.116547] [Steps   30500] [G 0.0018757]
[2023-05-12 00:54:21.525350] [Steps   30510] [G 0.0018419]
[2023-05-12 00:54:38.103992] [Steps   30520] [G 0.0022227]
[2023-05-12 00:54:54.800998] [Steps   30530] [G 0.0015954]
[2023-05-12 00:55:11.418805] [Steps   30540] [G 0.0017746]
[2023-05-12 00:55:28.053001] [Steps   30550] [G 0.0019403]
[2023-05-12 00:55:44.768681] [Steps   30560] [G 0.0015298]
[2023-05-12 00:56:01.381828] [Steps   30570] [G 0.0018101]
[2023-05-12 00:56:18.018940] [Steps   30580] [G 0.0020601]
[2023-05-12 00:56:34.828687] [Steps   30590] [G 0.0019118]
[2023-05-12 00:56:51.515279] [Steps   30600] [G 0.0019464]
[2023-05-12 00:57:08.133328] [Steps   30610] [G 0.0015831]
[2023-05-12 00:57:24.755446] [Steps   30620] [G 0.0017934]
[2023-05-12 00:57:41.480803] [Steps   30630] [G 0.0015644]
[2023-05-12 00:57:58.104099] [Steps   30640] [G 0.0014373]
[2023-05-12 00:58:14.741293] [Steps   30650] [G 0.0021604]
[2023-05-12 00:58:31.431228] [Steps   30660] [G 0.0015893]
[2023-05-12 00:58:48.059093] [Steps   30670] [G 0.0012613]
[2023-05-12 00:59:04.657270] [Steps   30680] [G 0.0016769]
[2023-05-12 00:59:21.286034] [Steps   30690] [G 0.0022042]
[2023-05-12 00:59:37.967126] [Steps   30700] [G 0.0015609]
[2023-05-12 00:59:54.585666] [Steps   30710] [G 0.0021736]
[2023-05-12 01:00:11.224198] [Steps   30720] [G 0.0015444]
[2023-05-12 01:00:27.905723] [Steps   30730] [G 0.0018167]
[2023-05-12 01:00:44.546967] [Steps   30740] [G 0.0018980]
[2023-05-12 01:01:01.196341] [Steps   30750] [G 0.0019159]
[2023-05-12 01:01:17.890145] [Steps   30760] [G 0.0015771]
[2023-05-12 01:01:34.502168] [Steps   30770] [G 0.0016006]
[2023-05-12 01:01:51.104239] [Steps   30780] [G 0.0019518]
[2023-05-12 01:02:07.732707] [Steps   30790] [G 0.0018693]
[2023-05-12 01:02:24.355557] [Steps   30800] [G 0.0024469]
[2023-05-12 01:02:43.123623] [Steps   30810] [G 0.0023254]
[2023-05-12 01:02:59.745823] [Steps   30820] [G 0.0022367]
[2023-05-12 01:03:16.447365] [Steps   30830] [G 0.0016863]
[2023-05-12 01:03:33.101647] [Steps   30840] [G 0.0017840]
[2023-05-12 01:03:49.773098] [Steps   30850] [G 0.0017182]
[2023-05-12 01:04:06.440429] [Steps   30860] [G 0.0023154]
[2023-05-12 01:04:23.164090] [Steps   30870] [G 0.0022911]
[2023-05-12 01:04:39.832941] [Steps   30880] [G 0.0017663]
[2023-05-12 01:04:56.506901] [Steps   30890] [G 0.0016008]
[2023-05-12 01:05:13.253698] [Steps   30900] [G 0.0015988]
[2023-05-12 01:05:29.936284] [Steps   30910] [G 0.0014022]
[2023-05-12 01:05:46.624553] [Steps   30920] [G 0.0017296]
[2023-05-12 01:06:03.300477] [Steps   30930] [G 0.0019986]
[2023-05-12 01:06:20.052786] [Steps   30940] [G 0.0020517]
[2023-05-12 01:06:36.726347] [Steps   30950] [G 0.0016808]
[2023-05-12 01:06:53.403748] [Steps   30960] [G 0.0022255]
[2023-05-12 01:07:10.121884] [Steps   30970] [G 0.0013080]
[2023-05-12 01:07:26.800849] [Steps   30980] [G 0.0016073]
[2023-05-12 01:07:43.457426] [Steps   30990] [G 0.0017043]
[2023-05-12 01:08:00.233182] [Steps   31000] [G 0.0022946]
Steps 31001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 01:08:18.246668] [Steps   31010] [G 0.0019984]
[2023-05-12 01:08:34.996673] [Steps   31020] [G 0.0017443]
[2023-05-12 01:08:51.643701] [Steps   31030] [G 0.0016307]
[2023-05-12 01:09:08.290801] [Steps   31040] [G 0.0026744]
[2023-05-12 01:09:25.031017] [Steps   31050] [G 0.0022830]
[2023-05-12 01:09:41.709868] [Steps   31060] [G 0.0022467]
[2023-05-12 01:09:58.378853] [Steps   31070] [G 0.0016553]
[2023-05-12 01:10:15.043549] [Steps   31080] [G 0.0024344]
[2023-05-12 01:10:31.821356] [Steps   31090] [G 0.0014959]
[2023-05-12 01:10:48.436383] [Steps   31100] [G 0.0022020]
[2023-05-12 01:11:06.987290] [Steps   31110] [G 0.0017097]
[2023-05-12 01:11:23.692246] [Steps   31120] [G 0.0019154]
[2023-05-12 01:11:40.340964] [Steps   31130] [G 0.0015094]
[2023-05-12 01:11:56.966574] [Steps   31140] [G 0.0014675]
[2023-05-12 01:12:13.594715] [Steps   31150] [G 0.0018708]
[2023-05-12 01:12:30.309257] [Steps   31160] [G 0.0017961]
[2023-05-12 01:12:46.982791] [Steps   31170] [G 0.0013366]
[2023-05-12 01:13:03.653760] [Steps   31180] [G 0.0017459]
[2023-05-12 01:13:20.442312] [Steps   31190] [G 0.0019916]
[2023-05-12 01:13:37.154106] [Steps   31200] [G 0.0017793]
[2023-05-12 01:13:53.843822] [Steps   31210] [G 0.0016577]
[2023-05-12 01:14:10.586298] [Steps   31220] [G 0.0014360]
[2023-05-12 01:14:27.256423] [Steps   31230] [G 0.0018525]
[2023-05-12 01:14:43.951175] [Steps   31240] [G 0.0028096]
[2023-05-12 01:15:00.627406] [Steps   31250] [G 0.0014030]
[2023-05-12 01:15:17.338760] [Steps   31260] [G 0.0017340]
[2023-05-12 01:15:33.995988] [Steps   31270] [G 0.0019362]
[2023-05-12 01:15:50.707385] [Steps   31280] [G 0.0015427]
[2023-05-12 01:16:07.441375] [Steps   31290] [G 0.0024734]
[2023-05-12 01:16:24.109351] [Steps   31300] [G 0.0021658]
[2023-05-12 01:16:40.760882] [Steps   31310] [G 0.0015541]
[2023-05-12 01:16:57.458995] [Steps   31320] [G 0.0018395]
[2023-05-12 01:17:14.202453] [Steps   31330] [G 0.0018827]
[2023-05-12 01:17:30.831601] [Steps   31340] [G 0.0020456]
[2023-05-12 01:17:47.565702] [Steps   31350] [G 0.0023210]
[2023-05-12 01:18:04.294014] [Steps   31360] [G 0.0019590]
[2023-05-12 01:18:20.951927] [Steps   31370] [G 0.0012603]
[2023-05-12 01:18:37.609857] [Steps   31380] [G 0.0017978]
[2023-05-12 01:18:54.372425] [Steps   31390] [G 0.0019438]
[2023-05-12 01:19:11.029309] [Steps   31400] [G 0.0020509]
[2023-05-12 01:19:29.560481] [Steps   31410] [G 0.0018523]
[2023-05-12 01:19:46.179586] [Steps   31420] [G 0.0018952]
[2023-05-12 01:20:02.879893] [Steps   31430] [G 0.0014412]
[2023-05-12 01:20:19.536823] [Steps   31440] [G 0.0015130]
[2023-05-12 01:20:36.197156] [Steps   31450] [G 0.0019396]
[2023-05-12 01:20:52.908333] [Steps   31460] [G 0.0021856]
[2023-05-12 01:21:09.541238] [Steps   31470] [G 0.0022011]
[2023-05-12 01:21:26.227387] [Steps   31480] [G 0.0048378]
[2023-05-12 01:21:42.867599] [Steps   31490] [G 0.0019553]
[2023-05-12 01:21:59.662391] [Steps   31500] [G 0.0020941]
[2023-05-12 01:22:16.332908] [Steps   31510] [G 0.0018728]
[2023-05-12 01:22:32.988993] [Steps   31520] [G 0.0019033]
[2023-05-12 01:22:49.704352] [Steps   31530] [G 0.0012495]
[2023-05-12 01:23:06.313764] [Steps   31540] [G 0.0015188]
[2023-05-12 01:23:22.913879] [Steps   31550] [G 0.0017322]
[2023-05-12 01:23:39.628630] [Steps   31560] [G 0.0023094]
[2023-05-12 01:23:56.210636] [Steps   31570] [G 0.0017178]
[2023-05-12 01:24:12.818500] [Steps   31580] [G 0.0023370]
[2023-05-12 01:24:29.465596] [Steps   31590] [G 0.0014325]
[2023-05-12 01:24:46.199999] [Steps   31600] [G 0.0022988]
[2023-05-12 01:25:02.802295] [Steps   31610] [G 0.0018769]
[2023-05-12 01:25:19.434842] [Steps   31620] [G 0.0024674]
[2023-05-12 01:25:36.191905] [Steps   31630] [G 0.0015806]
[2023-05-12 01:25:52.807092] [Steps   31640] [G 0.0019362]
[2023-05-12 01:26:09.406038] [Steps   31650] [G 0.0020593]
[2023-05-12 01:26:26.008002] [Steps   31660] [G 0.0018597]
[2023-05-12 01:26:42.787647] [Steps   31670] [G 0.0023005]
[2023-05-12 01:26:59.473891] [Steps   31680] [G 0.0016223]
[2023-05-12 01:27:16.118978] [Steps   31690] [G 0.0024868]
[2023-05-12 01:27:32.782922] [Steps   31700] [G 0.0019103]
[2023-05-12 01:27:49.377736] [Steps   31710] [G 0.0021467]
[2023-05-12 01:28:07.949300] [Steps   31720] [G 0.0020374]
[2023-05-12 01:28:24.627703] [Steps   31730] [G 0.0013672]
[2023-05-12 01:28:41.212731] [Steps   31740] [G 0.0013442]
[2023-05-12 01:28:57.807741] [Steps   31750] [G 0.0014592]
[2023-05-12 01:29:14.388987] [Steps   31760] [G 0.0014016]
[2023-05-12 01:29:31.047862] [Steps   31770] [G 0.0019054]
[2023-05-12 01:29:47.625235] [Steps   31780] [G 0.0012042]
[2023-05-12 01:30:04.226706] [Steps   31790] [G 0.0020535]
[2023-05-12 01:30:20.886598] [Steps   31800] [G 0.0017162]
[2023-05-12 01:30:37.492354] [Steps   31810] [G 0.0021584]
[2023-05-12 01:30:54.086579] [Steps   31820] [G 0.0013408]
[2023-05-12 01:31:10.681828] [Steps   31830] [G 0.0027300]
[2023-05-12 01:31:27.356812] [Steps   31840] [G 0.0017353]
[2023-05-12 01:31:43.938118] [Steps   31850] [G 0.0018487]
[2023-05-12 01:32:00.528431] [Steps   31860] [G 0.0018811]
[2023-05-12 01:32:17.175502] [Steps   31870] [G 0.0021140]
[2023-05-12 01:32:33.758931] [Steps   31880] [G 0.0016541]
[2023-05-12 01:32:50.339070] [Steps   31890] [G 0.0020159]
[2023-05-12 01:33:06.992995] [Steps   31900] [G 0.0013969]
[2023-05-12 01:33:23.578281] [Steps   31910] [G 0.0030368]
[2023-05-12 01:33:40.160820] [Steps   31920] [G 0.0023858]
[2023-05-12 01:33:56.748824] [Steps   31930] [G 0.0016596]
[2023-05-12 01:34:13.381802] [Steps   31940] [G 0.0023666]
[2023-05-12 01:34:29.961178] [Steps   31950] [G 0.0016770]
[2023-05-12 01:34:46.539213] [Steps   31960] [G 0.0015911]
[2023-05-12 01:35:03.189938] [Steps   31970] [G 0.0015930]
[2023-05-12 01:35:19.787211] [Steps   31980] [G 0.0020603]
[2023-05-12 01:35:36.358009] [Steps   31990] [G 0.0013485]
[2023-05-12 01:35:52.942071] [Steps   32000] [G 0.0022902]
Steps 32001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 01:36:10.936324] [Steps   32010] [G 0.0018768]
[2023-05-12 01:36:29.535317] [Steps   32020] [G 0.0020997]
[2023-05-12 01:36:46.221242] [Steps   32030] [G 0.0021637]
[2023-05-12 01:37:02.834354] [Steps   32040] [G 0.0012290]
[2023-05-12 01:37:19.519537] [Steps   32050] [G 0.0022598]
[2023-05-12 01:37:36.207461] [Steps   32060] [G 0.0017294]
[2023-05-12 01:37:52.806103] [Steps   32070] [G 0.0015380]
[2023-05-12 01:38:09.562656] [Steps   32080] [G 0.0015122]
[2023-05-12 01:38:26.262727] [Steps   32090] [G 0.0030883]
[2023-05-12 01:38:42.880060] [Steps   32100] [G 0.0019694]
[2023-05-12 01:38:59.511414] [Steps   32110] [G 0.0015945]
[2023-05-12 01:39:16.255640] [Steps   32120] [G 0.0016568]
[2023-05-12 01:39:32.922331] [Steps   32130] [G 0.0014368]
[2023-05-12 01:39:49.588449] [Steps   32140] [G 0.0020408]
[2023-05-12 01:40:06.334596] [Steps   32150] [G 0.0014108]
[2023-05-12 01:40:22.965405] [Steps   32160] [G 0.0018748]
[2023-05-12 01:40:39.614986] [Steps   32170] [G 0.0014155]
[2023-05-12 01:40:56.356384] [Steps   32180] [G 0.0015808]
[2023-05-12 01:41:13.015023] [Steps   32190] [G 0.0020272]
[2023-05-12 01:41:29.686952] [Steps   32200] [G 0.0021931]
[2023-05-12 01:41:46.345377] [Steps   32210] [G 0.0019478]
[2023-05-12 01:42:03.078847] [Steps   32220] [G 0.0020080]
[2023-05-12 01:42:19.745236] [Steps   32230] [G 0.0013409]
[2023-05-12 01:42:36.415113] [Steps   32240] [G 0.0014969]
[2023-05-12 01:42:53.162569] [Steps   32250] [G 0.0016376]
[2023-05-12 01:43:09.828511] [Steps   32260] [G 0.0014583]
[2023-05-12 01:43:26.495260] [Steps   32270] [G 0.0013587]
[2023-05-12 01:43:43.149765] [Steps   32280] [G 0.0013323]
[2023-05-12 01:43:59.864237] [Steps   32290] [G 0.0019083]
[2023-05-12 01:44:16.511908] [Steps   32300] [G 0.0016447]
[2023-05-12 01:44:33.087633] [Steps   32310] [G 0.0019580]
[2023-05-12 01:44:51.853754] [Steps   32320] [G 0.0016286]
[2023-05-12 01:45:08.529364] [Steps   32330] [G 0.0021507]
[2023-05-12 01:45:25.138709] [Steps   32340] [G 0.0017767]
[2023-05-12 01:45:41.817904] [Steps   32350] [G 0.0018393]
[2023-05-12 01:45:58.441467] [Steps   32360] [G 0.0016880]
[2023-05-12 01:46:15.046919] [Steps   32370] [G 0.0022214]
[2023-05-12 01:46:31.669059] [Steps   32380] [G 0.0013600]
[2023-05-12 01:46:48.425430] [Steps   32390] [G 0.0014052]
[2023-05-12 01:47:05.076922] [Steps   32400] [G 0.0016813]
[2023-05-12 01:47:21.733036] [Steps   32410] [G 0.0017901]
[2023-05-12 01:47:38.454067] [Steps   32420] [G 0.0014309]
[2023-05-12 01:47:55.085352] [Steps   32430] [G 0.0013299]
[2023-05-12 01:48:11.759690] [Steps   32440] [G 0.0017501]
[2023-05-12 01:48:28.442643] [Steps   32450] [G 0.0013166]
[2023-05-12 01:48:45.134915] [Steps   32460] [G 0.0020274]
[2023-05-12 01:49:01.780612] [Steps   32470] [G 0.0017880]
[2023-05-12 01:49:18.454459] [Steps   32480] [G 0.0020849]
[2023-05-12 01:49:35.207083] [Steps   32490] [G 0.0017426]
[2023-05-12 01:49:51.848773] [Steps   32500] [G 0.0017964]
[2023-05-12 01:50:08.510637] [Steps   32510] [G 0.0018042]
[2023-05-12 01:50:25.265761] [Steps   32520] [G 0.0013075]
[2023-05-12 01:50:41.926479] [Steps   32530] [G 0.0018292]
[2023-05-12 01:50:58.577064] [Steps   32540] [G 0.0019564]
[2023-05-12 01:51:15.222319] [Steps   32550] [G 0.0017424]
[2023-05-12 01:51:31.973120] [Steps   32560] [G 0.0015050]
[2023-05-12 01:51:48.635054] [Steps   32570] [G 0.0021741]
[2023-05-12 01:52:05.272775] [Steps   32580] [G 0.0017735]
[2023-05-12 01:52:22.020255] [Steps   32590] [G 0.0011724]
[2023-05-12 01:52:38.658102] [Steps   32600] [G 0.0013657]
[2023-05-12 01:52:55.284070] [Steps   32610] [G 0.0015943]
[2023-05-12 01:53:13.853946] [Steps   32620] [G 0.0017216]
[2023-05-12 01:53:30.560587] [Steps   32630] [G 0.0015154]
[2023-05-12 01:53:47.162636] [Steps   32640] [G 0.0015832]
[2023-05-12 01:54:03.828879] [Steps   32650] [G 0.0015405]
[2023-05-12 01:54:20.530486] [Steps   32660] [G 0.0014511]
[2023-05-12 01:54:37.197734] [Steps   32670] [G 0.0021656]
[2023-05-12 01:54:53.874016] [Steps   32680] [G 0.0025334]
[2023-05-12 01:55:10.536338] [Steps   32690] [G 0.0014789]
[2023-05-12 01:55:27.251575] [Steps   32700] [G 0.0020521]
[2023-05-12 01:55:43.918887] [Steps   32710] [G 0.0017695]
[2023-05-12 01:56:00.607836] [Steps   32720] [G 0.0020131]
[2023-05-12 01:56:17.327811] [Steps   32730] [G 0.0020688]
[2023-05-12 01:56:33.978313] [Steps   32740] [G 0.0013769]
[2023-05-12 01:56:50.637796] [Steps   32750] [G 0.0015985]
[2023-05-12 01:57:07.340999] [Steps   32760] [G 0.0015897]
[2023-05-12 01:57:24.006204] [Steps   32770] [G 0.0017616]
[2023-05-12 01:57:40.649776] [Steps   32780] [G 0.0014255]
[2023-05-12 01:57:57.331963] [Steps   32790] [G 0.0014806]
[2023-05-12 01:58:14.122367] [Steps   32800] [G 0.0017858]
[2023-05-12 01:58:30.770286] [Steps   32810] [G 0.0020131]
[2023-05-12 01:58:47.444441] [Steps   32820] [G 0.0019522]
[2023-05-12 01:59:04.161028] [Steps   32830] [G 0.0016197]
[2023-05-12 01:59:20.836947] [Steps   32840] [G 0.0011702]
[2023-05-12 01:59:37.467579] [Steps   32850] [G 0.0019786]
[2023-05-12 01:59:54.132044] [Steps   32860] [G 0.0014678]
[2023-05-12 02:00:10.872774] [Steps   32870] [G 0.0014311]
[2023-05-12 02:00:27.506552] [Steps   32880] [G 0.0018763]
[2023-05-12 02:00:44.163256] [Steps   32890] [G 0.0017461]
[2023-05-12 02:01:00.879014] [Steps   32900] [G 0.0016783]
[2023-05-12 02:01:17.504649] [Steps   32910] [G 0.0019135]
[2023-05-12 02:01:36.024847] [Steps   32920] [G 0.0015198]
[2023-05-12 02:01:52.755990] [Steps   32930] [G 0.0014699]
[2023-05-12 02:02:09.410932] [Steps   32940] [G 0.0021482]
[2023-05-12 02:02:26.073218] [Steps   32950] [G 0.0017211]
[2023-05-12 02:02:42.747669] [Steps   32960] [G 0.0013062]
[2023-05-12 02:02:59.504386] [Steps   32970] [G 0.0017895]
[2023-05-12 02:03:16.196637] [Steps   32980] [G 0.0023482]
[2023-05-12 02:03:32.875349] [Steps   32990] [G 0.0019251]
[2023-05-12 02:03:49.626965] [Steps   33000] [G 0.0020173]
Steps 33001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 02:04:07.643351] [Steps   33010] [G 0.0018450]
[2023-05-12 02:04:24.313289] [Steps   33020] [G 0.0015546]
[2023-05-12 02:04:40.999944] [Steps   33030] [G 0.0019247]
[2023-05-12 02:04:57.610772] [Steps   33040] [G 0.0013913]
[2023-05-12 02:05:14.310933] [Steps   33050] [G 0.0012292]
[2023-05-12 02:05:30.911478] [Steps   33060] [G 0.0015079]
[2023-05-12 02:05:47.604526] [Steps   33070] [G 0.0021401]
[2023-05-12 02:06:04.218570] [Steps   33080] [G 0.0015176]
[2023-05-12 02:06:20.943732] [Steps   33090] [G 0.0029353]
[2023-05-12 02:06:37.615241] [Steps   33100] [G 0.0019056]
[2023-05-12 02:06:54.223016] [Steps   33110] [G 0.0013422]
[2023-05-12 02:07:10.925255] [Steps   33120] [G 0.0017685]
[2023-05-12 02:07:27.530490] [Steps   33130] [G 0.0029524]
[2023-05-12 02:07:44.157286] [Steps   33140] [G 0.0019827]
[2023-05-12 02:08:00.855780] [Steps   33150] [G 0.0014396]
[2023-05-12 02:08:17.483815] [Steps   33160] [G 0.0025042]
[2023-05-12 02:08:34.083415] [Steps   33170] [G 0.0014890]
[2023-05-12 02:08:50.697392] [Steps   33180] [G 0.0017911]
[2023-05-12 02:09:07.398079] [Steps   33190] [G 0.0015553]
[2023-05-12 02:09:24.020804] [Steps   33200] [G 0.0017866]
[2023-05-12 02:09:40.595114] [Steps   33210] [G 0.0016816]
[2023-05-12 02:09:57.184025] [Steps   33220] [G 0.0016925]
[2023-05-12 02:10:15.797850] [Steps   33230] [G 0.0014218]
[2023-05-12 02:10:32.459709] [Steps   33240] [G 0.0013779]
[2023-05-12 02:10:49.046708] [Steps   33250] [G 0.0019577]
[2023-05-12 02:11:05.702467] [Steps   33260] [G 0.0017846]
[2023-05-12 02:11:22.280505] [Steps   33270] [G 0.0022522]
[2023-05-12 02:11:38.880892] [Steps   33280] [G 0.0015231]
[2023-05-12 02:11:55.559961] [Steps   33290] [G 0.0015523]
[2023-05-12 02:12:12.151126] [Steps   33300] [G 0.0017014]
[2023-05-12 02:12:28.742824] [Steps   33310] [G 0.0012998]
[2023-05-12 02:12:45.325846] [Steps   33320] [G 0.0011240]
[2023-05-12 02:13:02.024487] [Steps   33330] [G 0.0024193]
[2023-05-12 02:13:18.602613] [Steps   33340] [G 0.0013789]
[2023-05-12 02:13:35.198823] [Steps   33350] [G 0.0017285]
[2023-05-12 02:13:51.873744] [Steps   33360] [G 0.0015731]
[2023-05-12 02:14:08.462464] [Steps   33370] [G 0.0013758]
[2023-05-12 02:14:25.061987] [Steps   33380] [G 0.0023937]
[2023-05-12 02:14:41.722096] [Steps   33390] [G 0.0026363]
[2023-05-12 02:14:58.324755] [Steps   33400] [G 0.0014650]
[2023-05-12 02:15:14.901120] [Steps   33410] [G 0.0013979]
[2023-05-12 02:15:31.498253] [Steps   33420] [G 0.0023056]
[2023-05-12 02:15:48.212894] [Steps   33430] [G 0.0019253]
[2023-05-12 02:16:04.818323] [Steps   33440] [G 0.0020575]
[2023-05-12 02:16:21.418939] [Steps   33450] [G 0.0016980]
[2023-05-12 02:16:38.132274] [Steps   33460] [G 0.0012845]
[2023-05-12 02:16:54.716515] [Steps   33470] [G 0.0028682]
[2023-05-12 02:17:11.305527] [Steps   33480] [G 0.0015112]
[2023-05-12 02:17:27.906923] [Steps   33490] [G 0.0014667]
[2023-05-12 02:17:44.577063] [Steps   33500] [G 0.0015050]
[2023-05-12 02:18:01.178846] [Steps   33510] [G 0.0014676]
[2023-05-12 02:18:17.757494] [Steps   33520] [G 0.0020511]
[2023-05-12 02:18:36.310711] [Steps   33530] [G 0.0018129]
[2023-05-12 02:18:52.914881] [Steps   33540] [G 0.0013681]
[2023-05-12 02:19:09.510709] [Steps   33550] [G 0.0015507]
[2023-05-12 02:19:26.211627] [Steps   33560] [G 0.0018170]
[2023-05-12 02:19:42.818928] [Steps   33570] [G 0.0020673]
[2023-05-12 02:19:59.436597] [Steps   33580] [G 0.0016145]
[2023-05-12 02:20:16.056020] [Steps   33590] [G 0.0015998]
[2023-05-12 02:20:32.747416] [Steps   33600] [G 0.0017610]
[2023-05-12 02:20:49.355964] [Steps   33610] [G 0.0022317]
[2023-05-12 02:21:05.959623] [Steps   33620] [G 0.0016191]
[2023-05-12 02:21:22.659636] [Steps   33630] [G 0.0024721]
[2023-05-12 02:21:39.261891] [Steps   33640] [G 0.0014757]
[2023-05-12 02:21:55.867680] [Steps   33650] [G 0.0014140]
[2023-05-12 02:22:12.489024] [Steps   33660] [G 0.0018809]
[2023-05-12 02:22:29.157377] [Steps   33670] [G 0.0023827]
[2023-05-12 02:22:45.745767] [Steps   33680] [G 0.0016496]
[2023-05-12 02:23:02.335575] [Steps   33690] [G 0.0017394]
[2023-05-12 02:23:19.029831] [Steps   33700] [G 0.0012033]
[2023-05-12 02:23:35.638563] [Steps   33710] [G 0.0027035]
[2023-05-12 02:23:52.225538] [Steps   33720] [G 0.0018082]
[2023-05-12 02:24:08.888313] [Steps   33730] [G 0.0014051]
[2023-05-12 02:24:25.479890] [Steps   33740] [G 0.0019078]
[2023-05-12 02:24:42.092253] [Steps   33750] [G 0.0014935]
[2023-05-12 02:24:58.686053] [Steps   33760] [G 0.0015253]
[2023-05-12 02:25:15.386004] [Steps   33770] [G 0.0021616]
[2023-05-12 02:25:31.990335] [Steps   33780] [G 0.0014241]
[2023-05-12 02:25:48.593264] [Steps   33790] [G 0.0015169]
[2023-05-12 02:26:05.276074] [Steps   33800] [G 0.0017140]
[2023-05-12 02:26:21.864431] [Steps   33810] [G 0.0021937]
[2023-05-12 02:26:38.460862] [Steps   33820] [G 0.0012589]
[2023-05-12 02:26:57.091971] [Steps   33830] [G 0.0017138]
[2023-05-12 02:27:13.790419] [Steps   33840] [G 0.0012931]
[2023-05-12 02:27:30.379210] [Steps   33850] [G 0.0018366]
[2023-05-12 02:27:46.969894] [Steps   33860] [G 0.0013035]
[2023-05-12 02:28:03.651240] [Steps   33870] [G 0.0014354]
[2023-05-12 02:28:20.221828] [Steps   33880] [G 0.0015231]
[2023-05-12 02:28:36.845481] [Steps   33890] [G 0.0014378]
[2023-05-12 02:28:53.504736] [Steps   33900] [G 0.0014917]
[2023-05-12 02:29:10.089638] [Steps   33910] [G 0.0015227]
[2023-05-12 02:29:26.671934] [Steps   33920] [G 0.0021891]
[2023-05-12 02:29:43.270758] [Steps   33930] [G 0.0016789]
[2023-05-12 02:29:59.957232] [Steps   33940] [G 0.0010804]
[2023-05-12 02:30:16.560052] [Steps   33950] [G 0.0015448]
[2023-05-12 02:30:33.158465] [Steps   33960] [G 0.0020200]
[2023-05-12 02:30:49.821511] [Steps   33970] [G 0.0013539]
[2023-05-12 02:31:06.426975] [Steps   33980] [G 0.0021630]
[2023-05-12 02:31:22.989895] [Steps   33990] [G 0.0014861]
[2023-05-12 02:31:39.630457] [Steps   34000] [G 0.0014999]
Steps 34001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 02:31:57.655824] [Steps   34010] [G 0.0018601]
[2023-05-12 02:32:14.286241] [Steps   34020] [G 0.0013218]
[2023-05-12 02:32:30.849613] [Steps   34030] [G 0.0013402]
[2023-05-12 02:32:47.493369] [Steps   34040] [G 0.0013200]
[2023-05-12 02:33:04.092042] [Steps   34050] [G 0.0021291]
[2023-05-12 02:33:20.734943] [Steps   34060] [G 0.0012345]
[2023-05-12 02:33:37.308865] [Steps   34070] [G 0.0020055]
[2023-05-12 02:33:53.971330] [Steps   34080] [G 0.0014118]
[2023-05-12 02:34:10.633355] [Steps   34090] [G 0.0015963]
[2023-05-12 02:34:27.226714] [Steps   34100] [G 0.0015184]
[2023-05-12 02:34:43.885433] [Steps   34110] [G 0.0019293]
[2023-05-12 02:35:00.469705] [Steps   34120] [G 0.0015218]
[2023-05-12 02:35:19.117479] [Steps   34130] [G 0.0015517]
[2023-05-12 02:35:35.787360] [Steps   34140] [G 0.0014231]
[2023-05-12 02:35:52.364055] [Steps   34150] [G 0.0013282]
[2023-05-12 02:36:09.046423] [Steps   34160] [G 0.0014503]
[2023-05-12 02:36:25.674515] [Steps   34170] [G 0.0014725]
[2023-05-12 02:36:42.266513] [Steps   34180] [G 0.0016701]
[2023-05-12 02:36:58.993909] [Steps   34190] [G 0.0033947]
[2023-05-12 02:37:15.620362] [Steps   34200] [G 0.0013047]
[2023-05-12 02:37:32.248247] [Steps   34210] [G 0.0015509]
[2023-05-12 02:37:48.877448] [Steps   34220] [G 0.0020983]
[2023-05-12 02:38:05.586368] [Steps   34230] [G 0.0015782]
[2023-05-12 02:38:22.225192] [Steps   34240] [G 0.0010485]
[2023-05-12 02:38:38.824310] [Steps   34250] [G 0.0015540]
[2023-05-12 02:38:55.513216] [Steps   34260] [G 0.0011475]
[2023-05-12 02:39:12.154660] [Steps   34270] [G 0.0014900]
[2023-05-12 02:39:28.752471] [Steps   34280] [G 0.0014411]
[2023-05-12 02:39:45.372881] [Steps   34290] [G 0.0013202]
[2023-05-12 02:40:02.118654] [Steps   34300] [G 0.0012604]
[2023-05-12 02:40:18.700314] [Steps   34310] [G 0.0016345]
[2023-05-12 02:40:35.315548] [Steps   34320] [G 0.0012338]
[2023-05-12 02:40:52.029577] [Steps   34330] [G 0.0019297]
[2023-05-12 02:41:08.647541] [Steps   34340] [G 0.0016423]
[2023-05-12 02:41:25.265150] [Steps   34350] [G 0.0015908]
[2023-05-12 02:41:41.963396] [Steps   34360] [G 0.0024327]
[2023-05-12 02:41:58.567432] [Steps   34370] [G 0.0014136]
[2023-05-12 02:42:15.180625] [Steps   34380] [G 0.0014683]
[2023-05-12 02:42:31.809445] [Steps   34390] [G 0.0014646]
[2023-05-12 02:42:48.537433] [Steps   34400] [G 0.0018700]
[2023-05-12 02:43:05.136867] [Steps   34410] [G 0.0018147]
[2023-05-12 02:43:21.701146] [Steps   34420] [G 0.0016650]
[2023-05-12 02:43:40.170045] [Steps   34430] [G 0.0013402]
[2023-05-12 02:43:56.758467] [Steps   34440] [G 0.0018935]
[2023-05-12 02:44:13.444346] [Steps   34450] [G 0.0011305]
[2023-05-12 02:44:30.031283] [Steps   34460] [G 0.0010695]
[2023-05-12 02:44:46.737783] [Steps   34470] [G 0.0013390]
[2023-05-12 02:45:03.375700] [Steps   34480] [G 0.0024401]
[2023-05-12 02:45:19.975054] [Steps   34490] [G 0.0015926]
[2023-05-12 02:45:36.680284] [Steps   34500] [G 0.0012684]
[2023-05-12 02:45:53.314852] [Steps   34510] [G 0.0015110]
[2023-05-12 02:46:09.961925] [Steps   34520] [G 0.0016684]
[2023-05-12 02:46:26.652623] [Steps   34530] [G 0.0011512]
[2023-05-12 02:46:43.271793] [Steps   34540] [G 0.0013734]
[2023-05-12 02:46:59.897690] [Steps   34550] [G 0.0017721]
[2023-05-12 02:47:16.513709] [Steps   34560] [G 0.0018896]
[2023-05-12 02:47:33.203584] [Steps   34570] [G 0.0012194]
[2023-05-12 02:47:49.828852] [Steps   34580] [G 0.0013302]
[2023-05-12 02:48:06.484002] [Steps   34590] [G 0.0013552]
[2023-05-12 02:48:23.181237] [Steps   34600] [G 0.0017614]
[2023-05-12 02:48:39.798979] [Steps   34610] [G 0.0017463]
[2023-05-12 02:48:56.436323] [Steps   34620] [G 0.0012013]
[2023-05-12 02:49:13.019808] [Steps   34630] [G 0.0014776]
[2023-05-12 02:49:29.735388] [Steps   34640] [G 0.0015582]
[2023-05-12 02:49:46.356435] [Steps   34650] [G 0.0013397]
[2023-05-12 02:50:02.970447] [Steps   34660] [G 0.0015891]
[2023-05-12 02:50:19.675433] [Steps   34670] [G 0.0018114]
[2023-05-12 02:50:36.321775] [Steps   34680] [G 0.0012493]
[2023-05-12 02:50:52.947002] [Steps   34690] [G 0.0013184]
[2023-05-12 02:51:09.645350] [Steps   34700] [G 0.0013075]
[2023-05-12 02:51:26.282779] [Steps   34710] [G 0.0013980]
[2023-05-12 02:51:42.917209] [Steps   34720] [G 0.0013814]
[2023-05-12 02:51:59.492093] [Steps   34730] [G 0.0016557]
[2023-05-12 02:52:18.064314] [Steps   34740] [G 0.0017883]
[2023-05-12 02:52:34.665352] [Steps   34750] [G 0.0014915]
[2023-05-12 02:52:51.285430] [Steps   34760] [G 0.0014402]
[2023-05-12 02:53:07.977851] [Steps   34770] [G 0.0016814]
[2023-05-12 02:53:24.596387] [Steps   34780] [G 0.0016001]
[2023-05-12 02:53:41.222778] [Steps   34790] [G 0.0013139]
[2023-05-12 02:53:57.847638] [Steps   34800] [G 0.0016754]
[2023-05-12 02:54:14.558254] [Steps   34810] [G 0.0021339]
[2023-05-12 02:54:31.192445] [Steps   34820] [G 0.0017099]
[2023-05-12 02:54:47.810704] [Steps   34830] [G 0.0016247]
[2023-05-12 02:55:04.509767] [Steps   34840] [G 0.0016375]
[2023-05-12 02:55:21.154661] [Steps   34850] [G 0.0017039]
[2023-05-12 02:55:37.768482] [Steps   34860] [G 0.0011718]
[2023-05-12 02:55:54.509660] [Steps   34870] [G 0.0014710]
[2023-05-12 02:56:11.101090] [Steps   34880] [G 0.0015312]
[2023-05-12 02:56:27.723242] [Steps   34890] [G 0.0016413]
[2023-05-12 02:56:44.358738] [Steps   34900] [G 0.0020926]
[2023-05-12 02:57:01.045514] [Steps   34910] [G 0.0017490]
[2023-05-12 02:57:17.644349] [Steps   34920] [G 0.0012619]
[2023-05-12 02:57:34.252509] [Steps   34930] [G 0.0014909]
[2023-05-12 02:57:50.975315] [Steps   34940] [G 0.0017842]
[2023-05-12 02:58:07.596348] [Steps   34950] [G 0.0021109]
[2023-05-12 02:58:24.231557] [Steps   34960] [G 0.0022718]
[2023-05-12 02:58:40.829519] [Steps   34970] [G 0.0017571]
[2023-05-12 02:58:57.533835] [Steps   34980] [G 0.0014577]
[2023-05-12 02:59:14.173246] [Steps   34990] [G 0.0014686]
[2023-05-12 02:59:30.801119] [Steps   35000] [G 0.0019095]
Steps 35001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 02:59:48.850967] [Steps   35010] [G 0.0012940]
[2023-05-12 03:00:05.427328] [Steps   35020] [G 0.0013509]
[2023-05-12 03:00:22.092254] [Steps   35030] [G 0.0015656]
[2023-05-12 03:00:40.610449] [Steps   35040] [G 0.0012148]
[2023-05-12 03:00:57.206756] [Steps   35050] [G 0.0024192]
[2023-05-12 03:01:13.901300] [Steps   35060] [G 0.0012808]
[2023-05-12 03:01:30.544200] [Steps   35070] [G 0.0012696]
[2023-05-12 03:01:47.181917] [Steps   35080] [G 0.0013445]
[2023-05-12 03:02:03.819061] [Steps   35090] [G 0.0015394]
[2023-05-12 03:02:20.517144] [Steps   35100] [G 0.0015758]
[2023-05-12 03:02:37.159139] [Steps   35110] [G 0.0030852]
[2023-05-12 03:02:53.815051] [Steps   35120] [G 0.0011422]
[2023-05-12 03:03:10.554805] [Steps   35130] [G 0.0015612]
[2023-05-12 03:03:27.160017] [Steps   35140] [G 0.0014982]
[2023-05-12 03:03:43.823195] [Steps   35150] [G 0.0015723]
[2023-05-12 03:04:00.583963] [Steps   35160] [G 0.0014081]
[2023-05-12 03:04:17.231617] [Steps   35170] [G 0.0011597]
[2023-05-12 03:04:33.909804] [Steps   35180] [G 0.0011568]
[2023-05-12 03:04:50.536625] [Steps   35190] [G 0.0013313]
[2023-05-12 03:05:07.275484] [Steps   35200] [G 0.0008670]
[2023-05-12 03:05:23.897233] [Steps   35210] [G 0.0020971]
[2023-05-12 03:05:40.556018] [Steps   35220] [G 0.0011417]
[2023-05-12 03:05:57.304538] [Steps   35230] [G 0.0015960]
[2023-05-12 03:06:13.930638] [Steps   35240] [G 0.0024571]
[2023-05-12 03:06:30.571569] [Steps   35250] [G 0.0015226]
[2023-05-12 03:06:47.247320] [Steps   35260] [G 0.0014804]
[2023-05-12 03:07:03.981155] [Steps   35270] [G 0.0022010]
[2023-05-12 03:07:20.620750] [Steps   35280] [G 0.0017842]
[2023-05-12 03:07:37.270312] [Steps   35290] [G 0.0020209]
[2023-05-12 03:07:53.998013] [Steps   35300] [G 0.0014506]
[2023-05-12 03:08:10.647269] [Steps   35310] [G 0.0014797]
[2023-05-12 03:08:27.272932] [Steps   35320] [G 0.0019688]
[2023-05-12 03:08:43.966907] [Steps   35330] [G 0.0012040]
[2023-05-12 03:09:02.551652] [Steps   35340] [G 0.0018975]
[2023-05-12 03:09:19.126543] [Steps   35350] [G 0.0016151]
[2023-05-12 03:09:35.725048] [Steps   35360] [G 0.0014535]
[2023-05-12 03:09:52.392179] [Steps   35370] [G 0.0017085]
[2023-05-12 03:10:09.014522] [Steps   35380] [G 0.0016153]
[2023-05-12 03:10:25.638079] [Steps   35390] [G 0.0013010]
[2023-05-12 03:10:42.349841] [Steps   35400] [G 0.0013035]
[2023-05-12 03:10:58.961685] [Steps   35410] [G 0.0012588]
[2023-05-12 03:11:15.587060] [Steps   35420] [G 0.0013157]
[2023-05-12 03:11:32.181733] [Steps   35430] [G 0.0012411]
[2023-05-12 03:11:48.848647] [Steps   35440] [G 0.0015869]
[2023-05-12 03:12:05.449074] [Steps   35450] [G 0.0011873]
[2023-05-12 03:12:22.061360] [Steps   35460] [G 0.0011552]
[2023-05-12 03:12:38.748554] [Steps   35470] [G 0.0018367]
[2023-05-12 03:12:55.343175] [Steps   35480] [G 0.0018219]
[2023-05-12 03:13:11.970485] [Steps   35490] [G 0.0015124]
[2023-05-12 03:13:28.561659] [Steps   35500] [G 0.0013511]
[2023-05-12 03:13:45.231645] [Steps   35510] [G 0.0012381]
[2023-05-12 03:14:01.831670] [Steps   35520] [G 0.0014495]
[2023-05-12 03:14:18.426377] [Steps   35530] [G 0.0015089]
[2023-05-12 03:14:35.090504] [Steps   35540] [G 0.0014557]
[2023-05-12 03:14:51.689458] [Steps   35550] [G 0.0016827]
[2023-05-12 03:15:08.256983] [Steps   35560] [G 0.0017108]
[2023-05-12 03:15:24.923672] [Steps   35570] [G 0.0014159]
[2023-05-12 03:15:41.478048] [Steps   35580] [G 0.0015987]
[2023-05-12 03:15:58.072839] [Steps   35590] [G 0.0016675]
[2023-05-12 03:16:14.659003] [Steps   35600] [G 0.0014035]
[2023-05-12 03:16:31.300102] [Steps   35610] [G 0.0013920]
[2023-05-12 03:16:47.881605] [Steps   35620] [G 0.0012183]
[2023-05-12 03:17:04.448702] [Steps   35630] [G 0.0010502]
[2023-05-12 03:17:23.098018] [Steps   35640] [G 0.0014939]
[2023-05-12 03:17:39.690707] [Steps   35650] [G 0.0031378]
[2023-05-12 03:17:56.372093] [Steps   35660] [G 0.0010057]
[2023-05-12 03:18:12.975191] [Steps   35670] [G 0.0014249]
[2023-05-12 03:18:29.667455] [Steps   35680] [G 0.0018482]
[2023-05-12 03:18:46.295360] [Steps   35690] [G 0.0014058]
[2023-05-12 03:19:02.912317] [Steps   35700] [G 0.0014134]
[2023-05-12 03:19:19.578184] [Steps   35710] [G 0.0025420]
[2023-05-12 03:19:36.189586] [Steps   35720] [G 0.0011484]
[2023-05-12 03:19:52.782935] [Steps   35730] [G 0.0012869]
[2023-05-12 03:20:09.465141] [Steps   35740] [G 0.0015515]
[2023-05-12 03:20:26.077447] [Steps   35750] [G 0.0012202]
[2023-05-12 03:20:42.718898] [Steps   35760] [G 0.0015407]
[2023-05-12 03:20:59.333868] [Steps   35770] [G 0.0011513]
[2023-05-12 03:21:16.029285] [Steps   35780] [G 0.0018774]
[2023-05-12 03:21:32.631559] [Steps   35790] [G 0.0011476]
[2023-05-12 03:21:49.245139] [Steps   35800] [G 0.0010901]
[2023-05-12 03:22:05.914777] [Steps   35810] [G 0.0021336]
[2023-05-12 03:22:22.511918] [Steps   35820] [G 0.0015021]
[2023-05-12 03:22:39.135371] [Steps   35830] [G 0.0021619]
[2023-05-12 03:22:55.752155] [Steps   35840] [G 0.0021565]
[2023-05-12 03:23:12.422766] [Steps   35850] [G 0.0023030]
[2023-05-12 03:23:29.026912] [Steps   35860] [G 0.0027144]
[2023-05-12 03:23:45.674336] [Steps   35870] [G 0.0012020]
[2023-05-12 03:24:02.372265] [Steps   35880] [G 0.0018081]
[2023-05-12 03:24:18.978899] [Steps   35890] [G 0.0017020]
[2023-05-12 03:24:35.637880] [Steps   35900] [G 0.0013416]
[2023-05-12 03:24:52.316936] [Steps   35910] [G 0.0015705]
[2023-05-12 03:25:08.917560] [Steps   35920] [G 0.0019215]
[2023-05-12 03:25:25.489111] [Steps   35930] [G 0.0015372]
[2023-05-12 03:25:44.020165] [Steps   35940] [G 0.0024632]
[2023-05-12 03:26:00.710558] [Steps   35950] [G 0.0015352]
[2023-05-12 03:26:17.314934] [Steps   35960] [G 0.0014530]
[2023-05-12 03:26:33.896878] [Steps   35970] [G 0.0015430]
[2023-05-12 03:26:50.560175] [Steps   35980] [G 0.0015121]
[2023-05-12 03:27:07.146549] [Steps   35990] [G 0.0017508]
[2023-05-12 03:27:23.718050] [Steps   36000] [G 0.0009321]
Steps 36001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 03:27:41.739477] [Steps   36010] [G 0.0012304]
[2023-05-12 03:27:58.408274] [Steps   36020] [G 0.0014381]
[2023-05-12 03:28:15.017816] [Steps   36030] [G 0.0012354]
[2023-05-12 03:28:31.685035] [Steps   36040] [G 0.0021474]
[2023-05-12 03:28:48.347634] [Steps   36050] [G 0.0011056]
[2023-05-12 03:29:04.945451] [Steps   36060] [G 0.0013757]
[2023-05-12 03:29:21.602036] [Steps   36070] [G 0.0021303]
[2023-05-12 03:29:38.206413] [Steps   36080] [G 0.0016640]
[2023-05-12 03:29:54.873499] [Steps   36090] [G 0.0012803]
[2023-05-12 03:30:11.539205] [Steps   36100] [G 0.0039425]
[2023-05-12 03:30:28.158343] [Steps   36110] [G 0.0015139]
[2023-05-12 03:30:44.853280] [Steps   36120] [G 0.0013758]
[2023-05-12 03:31:01.456161] [Steps   36130] [G 0.0013902]
[2023-05-12 03:31:18.116296] [Steps   36140] [G 0.0017180]
[2023-05-12 03:31:34.725976] [Steps   36150] [G 0.0016260]
[2023-05-12 03:31:51.389652] [Steps   36160] [G 0.0011341]
[2023-05-12 03:32:08.069979] [Steps   36170] [G 0.0016680]
[2023-05-12 03:32:24.667191] [Steps   36180] [G 0.0015968]
[2023-05-12 03:32:41.333883] [Steps   36190] [G 0.0014013]
[2023-05-12 03:32:57.931137] [Steps   36200] [G 0.0020079]
[2023-05-12 03:33:14.578283] [Steps   36210] [G 0.0016869]
[2023-05-12 03:33:31.238023] [Steps   36220] [G 0.0014580]
[2023-05-12 03:33:47.826145] [Steps   36230] [G 0.0012065]
[2023-05-12 03:34:04.453980] [Steps   36240] [G 0.0018963]
[2023-05-12 03:34:23.022276] [Steps   36250] [G 0.0013149]
[2023-05-12 03:34:39.686072] [Steps   36260] [G 0.0013232]
[2023-05-12 03:34:56.361062] [Steps   36270] [G 0.0020543]
[2023-05-12 03:35:12.961430] [Steps   36280] [G 0.0011743]
[2023-05-12 03:35:29.609259] [Steps   36290] [G 0.0014157]
[2023-05-12 03:35:46.210535] [Steps   36300] [G 0.0013617]
[2023-05-12 03:36:02.900624] [Steps   36310] [G 0.0012672]
[2023-05-12 03:36:19.536302] [Steps   36320] [G 0.0013206]
[2023-05-12 03:36:36.173582] [Steps   36330] [G 0.0011042]
[2023-05-12 03:36:52.855009] [Steps   36340] [G 0.0012055]
[2023-05-12 03:37:09.463114] [Steps   36350] [G 0.0012068]
[2023-05-12 03:37:26.083283] [Steps   36360] [G 0.0015714]
[2023-05-12 03:37:42.749922] [Steps   36370] [G 0.0010413]
[2023-05-12 03:37:59.482195] [Steps   36380] [G 0.0016862]
[2023-05-12 03:38:16.128612] [Steps   36390] [G 0.0014544]
[2023-05-12 03:38:32.783985] [Steps   36400] [G 0.0020773]
[2023-05-12 03:38:49.504587] [Steps   36410] [G 0.0016900]
[2023-05-12 03:39:06.156996] [Steps   36420] [G 0.0013048]
[2023-05-12 03:39:22.817698] [Steps   36430] [G 0.0012525]
[2023-05-12 03:39:39.551773] [Steps   36440] [G 0.0015376]
[2023-05-12 03:39:56.181642] [Steps   36450] [G 0.0015346]
[2023-05-12 03:40:12.818219] [Steps   36460] [G 0.0024046]
[2023-05-12 03:40:29.447775] [Steps   36470] [G 0.0020833]
[2023-05-12 03:40:46.171306] [Steps   36480] [G 0.0015811]
[2023-05-12 03:41:02.803606] [Steps   36490] [G 0.0011330]
[2023-05-12 03:41:19.470682] [Steps   36500] [G 0.0012938]
[2023-05-12 03:41:36.165817] [Steps   36510] [G 0.0017448]
[2023-05-12 03:41:52.827291] [Steps   36520] [G 0.0013120]
[2023-05-12 03:42:09.437480] [Steps   36530] [G 0.0012428]
[2023-05-12 03:42:26.034539] [Steps   36540] [G 0.0014987]
[2023-05-12 03:42:44.561840] [Steps   36550] [G 0.0012088]
[2023-05-12 03:43:01.137085] [Steps   36560] [G 0.0021115]
[2023-05-12 03:43:17.801804] [Steps   36570] [G 0.0013776]
[2023-05-12 03:43:34.510393] [Steps   36580] [G 0.0015331]
[2023-05-12 03:43:51.164706] [Steps   36590] [G 0.0014364]
[2023-05-12 03:44:07.783570] [Steps   36600] [G 0.0013996]
[2023-05-12 03:44:24.436350] [Steps   36610] [G 0.0019750]
[2023-05-12 03:44:41.110925] [Steps   36620] [G 0.0013419]
[2023-05-12 03:44:57.761746] [Steps   36630] [G 0.0011203]
[2023-05-12 03:45:14.402025] [Steps   36640] [G 0.0013824]
[2023-05-12 03:45:31.113732] [Steps   36650] [G 0.0016553]
[2023-05-12 03:45:47.783587] [Steps   36660] [G 0.0013673]
[2023-05-12 03:46:04.435744] [Steps   36670] [G 0.0016136]
[2023-05-12 03:46:21.223213] [Steps   36680] [G 0.0012326]
[2023-05-12 03:46:37.850883] [Steps   36690] [G 0.0017654]
[2023-05-12 03:46:54.504650] [Steps   36700] [G 0.0011848]
[2023-05-12 03:47:11.173029] [Steps   36710] [G 0.0017908]
[2023-05-12 03:47:27.919116] [Steps   36720] [G 0.0020232]
[2023-05-12 03:47:44.546484] [Steps   36730] [G 0.0012197]
[2023-05-12 03:48:01.156761] [Steps   36740] [G 0.0019814]
[2023-05-12 03:48:17.892897] [Steps   36750] [G 0.0021521]
[2023-05-12 03:48:34.547940] [Steps   36760] [G 0.0021827]
[2023-05-12 03:48:51.174938] [Steps   36770] [G 0.0021265]
[2023-05-12 03:49:07.912980] [Steps   36780] [G 0.0016039]
[2023-05-12 03:49:24.568878] [Steps   36790] [G 0.0013783]
[2023-05-12 03:49:41.214490] [Steps   36800] [G 0.0009929]
[2023-05-12 03:49:57.851393] [Steps   36810] [G 0.0021265]
[2023-05-12 03:50:14.593941] [Steps   36820] [G 0.0024478]
[2023-05-12 03:50:31.220260] [Steps   36830] [G 0.0016348]
[2023-05-12 03:50:47.816731] [Steps   36840] [G 0.0013306]
[2023-05-12 03:51:06.556009] [Steps   36850] [G 0.0016115]
[2023-05-12 03:51:23.090383] [Steps   36860] [G 0.0012851]
[2023-05-12 03:51:39.630432] [Steps   36870] [G 0.0021008]
[2023-05-12 03:51:56.176412] [Steps   36880] [G 0.0012008]
[2023-05-12 03:52:12.786819] [Steps   36890] [G 0.0017305]
[2023-05-12 03:52:29.341753] [Steps   36900] [G 0.0015319]
[2023-05-12 03:52:45.868053] [Steps   36910] [G 0.0017595]
[2023-05-12 03:53:02.484593] [Steps   36920] [G 0.0011573]
[2023-05-12 03:53:19.019214] [Steps   36930] [G 0.0016502]
[2023-05-12 03:53:35.560347] [Steps   36940] [G 0.0015268]
[2023-05-12 03:53:52.180558] [Steps   36950] [G 0.0013023]
[2023-05-12 03:54:08.727201] [Steps   36960] [G 0.0014825]
[2023-05-12 03:54:25.272573] [Steps   36970] [G 0.0013257]
[2023-05-12 03:54:41.803339] [Steps   36980] [G 0.0023931]
[2023-05-12 03:54:58.427837] [Steps   36990] [G 0.0013356]
[2023-05-12 03:55:14.965613] [Steps   37000] [G 0.0012124]
Steps 37001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 03:55:32.893325] [Steps   37010] [G 0.0012264]
[2023-05-12 03:55:49.510781] [Steps   37020] [G 0.0016136]
[2023-05-12 03:56:06.047016] [Steps   37030] [G 0.0026805]
[2023-05-12 03:56:22.652511] [Steps   37040] [G 0.0011704]
[2023-05-12 03:56:39.179646] [Steps   37050] [G 0.0015697]
[2023-05-12 03:56:55.725766] [Steps   37060] [G 0.0015568]
[2023-05-12 03:57:12.340915] [Steps   37070] [G 0.0012817]
[2023-05-12 03:57:28.896717] [Steps   37080] [G 0.0016437]
[2023-05-12 03:57:45.462402] [Steps   37090] [G 0.0020122]
[2023-05-12 03:58:02.010447] [Steps   37100] [G 0.0013421]
[2023-05-12 03:58:18.635464] [Steps   37110] [G 0.0012783]
[2023-05-12 03:58:35.177372] [Steps   37120] [G 0.0015295]
[2023-05-12 03:58:51.736589] [Steps   37130] [G 0.0011313]
[2023-05-12 03:59:08.341444] [Steps   37140] [G 0.0018420]
[2023-05-12 03:59:27.008707] [Steps   37150] [G 0.0012882]
[2023-05-12 03:59:43.701130] [Steps   37160] [G 0.0013812]
[2023-05-12 04:00:00.333708] [Steps   37170] [G 0.0009985]
[2023-05-12 04:00:17.055084] [Steps   37180] [G 0.0012777]
[2023-05-12 04:00:33.700251] [Steps   37190] [G 0.0012842]
[2023-05-12 04:00:50.357759] [Steps   37200] [G 0.0010748]
[2023-05-12 04:01:07.110505] [Steps   37210] [G 0.0013109]
[2023-05-12 04:01:23.737401] [Steps   37220] [G 0.0013531]
[2023-05-12 04:01:40.411861] [Steps   37230] [G 0.0011571]
[2023-05-12 04:01:57.138904] [Steps   37240] [G 0.0020378]
[2023-05-12 04:02:13.795860] [Steps   37250] [G 0.0015750]
[2023-05-12 04:02:30.411752] [Steps   37260] [G 0.0015615]
[2023-05-12 04:02:47.040616] [Steps   37270] [G 0.0013224]
[2023-05-12 04:03:03.747222] [Steps   37280] [G 0.0015397]
[2023-05-12 04:03:20.425228] [Steps   37290] [G 0.0023242]
[2023-05-12 04:03:37.065482] [Steps   37300] [G 0.0016670]
[2023-05-12 04:03:53.788771] [Steps   37310] [G 0.0017827]
[2023-05-12 04:04:10.439108] [Steps   37320] [G 0.0010347]
[2023-05-12 04:04:27.085389] [Steps   37330] [G 0.0016549]
[2023-05-12 04:04:43.741476] [Steps   37340] [G 0.0013883]
[2023-05-12 04:05:00.467209] [Steps   37350] [G 0.0014528]
[2023-05-12 04:05:17.143151] [Steps   37360] [G 0.0011987]
[2023-05-12 04:05:33.831400] [Steps   37370] [G 0.0015949]
[2023-05-12 04:05:50.489457] [Steps   37380] [G 0.0019084]
[2023-05-12 04:06:07.131289] [Steps   37390] [G 0.0014071]
[2023-05-12 04:06:23.783950] [Steps   37400] [G 0.0015627]
[2023-05-12 04:06:40.490643] [Steps   37410] [G 0.0016663]
[2023-05-12 04:06:57.112793] [Steps   37420] [G 0.0015318]
[2023-05-12 04:07:13.741708] [Steps   37430] [G 0.0012528]
[2023-05-12 04:07:30.373897] [Steps   37440] [G 0.0013785]
[2023-05-12 04:07:48.997281] [Steps   37450] [G 0.0008117]
[2023-05-12 04:08:05.609939] [Steps   37460] [G 0.0011391]
[2023-05-12 04:08:22.206524] [Steps   37470] [G 0.0015974]
[2023-05-12 04:08:38.923940] [Steps   37480] [G 0.0012947]
[2023-05-12 04:08:55.535095] [Steps   37490] [G 0.0014220]
[2023-05-12 04:09:12.202917] [Steps   37500] [G 0.0017355]
[2023-05-12 04:09:28.876709] [Steps   37510] [G 0.0013519]
[2023-05-12 04:09:45.610925] [Steps   37520] [G 0.0015019]
[2023-05-12 04:10:02.268564] [Steps   37530] [G 0.0024017]
[2023-05-12 04:10:18.929302] [Steps   37540] [G 0.0008775]
[2023-05-12 04:10:35.709911] [Steps   37550] [G 0.0016389]
[2023-05-12 04:10:52.381404] [Steps   37560] [G 0.0015142]
[2023-05-12 04:11:09.050906] [Steps   37570] [G 0.0022551]
[2023-05-12 04:11:25.739588] [Steps   37580] [G 0.0014273]
[2023-05-12 04:11:42.450913] [Steps   37590] [G 0.0012686]
[2023-05-12 04:11:59.159180] [Steps   37600] [G 0.0016438]
[2023-05-12 04:12:15.815399] [Steps   37610] [G 0.0013323]
[2023-05-12 04:12:32.542316] [Steps   37620] [G 0.0010914]
[2023-05-12 04:12:49.204783] [Steps   37630] [G 0.0018902]
[2023-05-12 04:13:05.882781] [Steps   37640] [G 0.0024744]
[2023-05-12 04:13:22.635392] [Steps   37650] [G 0.0014117]
[2023-05-12 04:13:39.297935] [Steps   37660] [G 0.0014669]
[2023-05-12 04:13:55.981743] [Steps   37670] [G 0.0015108]
[2023-05-12 04:14:12.644266] [Steps   37680] [G 0.0012860]
[2023-05-12 04:14:29.391013] [Steps   37690] [G 0.0011095]
[2023-05-12 04:14:46.065381] [Steps   37700] [G 0.0014556]
[2023-05-12 04:15:02.732964] [Steps   37710] [G 0.0021541]
[2023-05-12 04:15:19.490416] [Steps   37720] [G 0.0014113]
[2023-05-12 04:15:36.165692] [Steps   37730] [G 0.0012168]
[2023-05-12 04:15:52.845651] [Steps   37740] [G 0.0012963]
[2023-05-12 04:16:09.442733] [Steps   37750] [G 0.0014581]
[2023-05-12 04:16:28.236685] [Steps   37760] [G 0.0015632]
[2023-05-12 04:16:44.942015] [Steps   37770] [G 0.0013766]
[2023-05-12 04:17:01.585300] [Steps   37780] [G 0.0013576]
[2023-05-12 04:17:18.312483] [Steps   37790] [G 0.0010967]
[2023-05-12 04:17:34.903453] [Steps   37800] [G 0.0012185]
[2023-05-12 04:17:51.568919] [Steps   37810] [G 0.0011958]
[2023-05-12 04:18:08.320161] [Steps   37820] [G 0.0012862]
[2023-05-12 04:18:25.027770] [Steps   37830] [G 0.0011044]
[2023-05-12 04:18:41.741710] [Steps   37840] [G 0.0016898]
[2023-05-12 04:18:58.397042] [Steps   37850] [G 0.0012692]
[2023-05-12 04:19:15.158361] [Steps   37860] [G 0.0011159]
[2023-05-12 04:19:31.831800] [Steps   37870] [G 0.0012650]
[2023-05-12 04:19:48.553056] [Steps   37880] [G 0.0012476]
[2023-05-12 04:20:05.271793] [Steps   37890] [G 0.0012720]
[2023-05-12 04:20:21.931205] [Steps   37900] [G 0.0013970]
[2023-05-12 04:20:38.607204] [Steps   37910] [G 0.0015094]
[2023-05-12 04:20:55.286442] [Steps   37920] [G 0.0012344]
[2023-05-12 04:21:12.075638] [Steps   37930] [G 0.0014444]
[2023-05-12 04:21:28.752020] [Steps   37940] [G 0.0016677]
[2023-05-12 04:21:45.474519] [Steps   37950] [G 0.0011135]
[2023-05-12 04:22:02.200596] [Steps   37960] [G 0.0016794]
[2023-05-12 04:22:18.849729] [Steps   37970] [G 0.0015377]
[2023-05-12 04:22:35.545705] [Steps   37980] [G 0.0013179]
[2023-05-12 04:22:52.283411] [Steps   37990] [G 0.0011834]
[2023-05-12 04:23:08.978262] [Steps   38000] [G 0.0012705]
Steps 38001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 04:23:27.054502] [Steps   38010] [G 0.0012794]
[2023-05-12 04:23:43.754667] [Steps   38020] [G 0.0016924]
[2023-05-12 04:24:00.448838] [Steps   38030] [G 0.0010193]
[2023-05-12 04:24:17.132599] [Steps   38040] [G 0.0015900]
[2023-05-12 04:24:33.796606] [Steps   38050] [G 0.0015564]
[2023-05-12 04:24:52.359601] [Steps   38060] [G 0.0012226]
[2023-05-12 04:25:09.098283] [Steps   38070] [G 0.0013801]
[2023-05-12 04:25:25.742217] [Steps   38080] [G 0.0015178]
[2023-05-12 04:25:42.419496] [Steps   38090] [G 0.0012424]
[2023-05-12 04:25:59.141290] [Steps   38100] [G 0.0013912]
[2023-05-12 04:26:15.856200] [Steps   38110] [G 0.0023760]
[2023-05-12 04:26:32.503986] [Steps   38120] [G 0.0015492]
[2023-05-12 04:26:49.175652] [Steps   38130] [G 0.0017032]
[2023-05-12 04:27:05.937427] [Steps   38140] [G 0.0010373]
[2023-05-12 04:27:22.661492] [Steps   38150] [G 0.0018964]
[2023-05-12 04:27:39.326113] [Steps   38160] [G 0.0009327]
[2023-05-12 04:27:56.032345] [Steps   38170] [G 0.0014721]
[2023-05-12 04:28:12.718771] [Steps   38180] [G 0.0013252]
[2023-05-12 04:28:29.404034] [Steps   38190] [G 0.0012756]
[2023-05-12 04:28:46.132655] [Steps   38200] [G 0.0012705]
[2023-05-12 04:29:02.810791] [Steps   38210] [G 0.0018459]
[2023-05-12 04:29:19.484299] [Steps   38220] [G 0.0012101]
[2023-05-12 04:29:36.142044] [Steps   38230] [G 0.0011484]
[2023-05-12 04:29:52.873046] [Steps   38240] [G 0.0013309]
[2023-05-12 04:30:09.565665] [Steps   38250] [G 0.0011022]
[2023-05-12 04:30:26.209704] [Steps   38260] [G 0.0023550]
[2023-05-12 04:30:42.953398] [Steps   38270] [G 0.0012861]
[2023-05-12 04:30:59.621966] [Steps   38280] [G 0.0015008]
[2023-05-12 04:31:16.276984] [Steps   38290] [G 0.0017891]
[2023-05-12 04:31:32.945490] [Steps   38300] [G 0.0013688]
[2023-05-12 04:31:49.700294] [Steps   38310] [G 0.0012718]
[2023-05-12 04:32:06.375918] [Steps   38320] [G 0.0020392]
[2023-05-12 04:32:23.039932] [Steps   38330] [G 0.0018438]
[2023-05-12 04:32:39.758425] [Steps   38340] [G 0.0020064]
[2023-05-12 04:32:56.377449] [Steps   38350] [G 0.0019813]
[2023-05-12 04:33:14.846957] [Steps   38360] [G 0.0014324]
[2023-05-12 04:33:31.532954] [Steps   38370] [G 0.0017141]
[2023-05-12 04:33:48.130597] [Steps   38380] [G 0.0009844]
[2023-05-12 04:34:04.734004] [Steps   38390] [G 0.0014550]
[2023-05-12 04:34:21.358055] [Steps   38400] [G 0.0011389]
[2023-05-12 04:34:38.060975] [Steps   38410] [G 0.0017146]
[2023-05-12 04:34:54.693016] [Steps   38420] [G 0.0015942]
[2023-05-12 04:35:11.323164] [Steps   38430] [G 0.0013796]
[2023-05-12 04:35:28.071447] [Steps   38440] [G 0.0009749]
[2023-05-12 04:35:44.725351] [Steps   38450] [G 0.0012979]
[2023-05-12 04:36:01.357152] [Steps   38460] [G 0.0010903]
[2023-05-12 04:36:17.975314] [Steps   38470] [G 0.0011359]
[2023-05-12 04:36:34.721334] [Steps   38480] [G 0.0011905]
[2023-05-12 04:36:51.368884] [Steps   38490] [G 0.0014889]
[2023-05-12 04:37:07.982359] [Steps   38500] [G 0.0018404]
[2023-05-12 04:37:24.676035] [Steps   38510] [G 0.0012504]
[2023-05-12 04:37:41.324885] [Steps   38520] [G 0.0013037]
[2023-05-12 04:37:57.978625] [Steps   38530] [G 0.0014306]
[2023-05-12 04:38:14.676514] [Steps   38540] [G 0.0012902]
[2023-05-12 04:38:31.321384] [Steps   38550] [G 0.0013138]
[2023-05-12 04:38:47.992918] [Steps   38560] [G 0.0014181]
[2023-05-12 04:39:04.658688] [Steps   38570] [G 0.0012258]
[2023-05-12 04:39:21.374312] [Steps   38580] [G 0.0014915]
[2023-05-12 04:39:38.002569] [Steps   38590] [G 0.0012350]
[2023-05-12 04:39:54.673106] [Steps   38600] [G 0.0010917]
[2023-05-12 04:40:11.411312] [Steps   38610] [G 0.0013209]
[2023-05-12 04:40:28.081870] [Steps   38620] [G 0.0014214]
[2023-05-12 04:40:44.727508] [Steps   38630] [G 0.0015426]
[2023-05-12 04:41:01.388816] [Steps   38640] [G 0.0016142]
[2023-05-12 04:41:18.072483] [Steps   38650] [G 0.0013159]
[2023-05-12 04:41:36.685687] [Steps   38660] [G 0.0010503]
[2023-05-12 04:41:53.297240] [Steps   38670] [G 0.0016523]
[2023-05-12 04:42:10.007339] [Steps   38680] [G 0.0022323]
[2023-05-12 04:42:26.639606] [Steps   38690] [G 0.0015611]
[2023-05-12 04:42:43.287564] [Steps   38700] [G 0.0010907]
[2023-05-12 04:43:00.030732] [Steps   38710] [G 0.0013358]
[2023-05-12 04:43:16.694279] [Steps   38720] [G 0.0012727]
[2023-05-12 04:43:33.358960] [Steps   38730] [G 0.0018923]
[2023-05-12 04:43:50.030376] [Steps   38740] [G 0.0013743]
[2023-05-12 04:44:06.766924] [Steps   38750] [G 0.0015748]
[2023-05-12 04:44:23.405166] [Steps   38760] [G 0.0014268]
[2023-05-12 04:44:40.072880] [Steps   38770] [G 0.0012465]
[2023-05-12 04:44:56.800502] [Steps   38780] [G 0.0014221]
[2023-05-12 04:45:13.529118] [Steps   38790] [G 0.0015105]
[2023-05-12 04:45:30.190631] [Steps   38800] [G 0.0013122]
[2023-05-12 04:45:46.844446] [Steps   38810] [G 0.0013059]
[2023-05-12 04:46:03.536929] [Steps   38820] [G 0.0017514]
[2023-05-12 04:46:20.138090] [Steps   38830] [G 0.0008718]
[2023-05-12 04:46:36.818731] [Steps   38840] [G 0.0012748]
[2023-05-12 04:46:53.584087] [Steps   38850] [G 0.0015108]
[2023-05-12 04:47:10.207734] [Steps   38860] [G 0.0013455]
[2023-05-12 04:47:26.838136] [Steps   38870] [G 0.0017266]
[2023-05-12 04:47:43.570189] [Steps   38880] [G 0.0017111]
[2023-05-12 04:48:00.218251] [Steps   38890] [G 0.0015836]
[2023-05-12 04:48:16.868807] [Steps   38900] [G 0.0013391]
[2023-05-12 04:48:33.553358] [Steps   38910] [G 0.0014630]
[2023-05-12 04:48:50.304736] [Steps   38920] [G 0.0011506]
[2023-05-12 04:49:06.974306] [Steps   38930] [G 0.0013092]
[2023-05-12 04:49:23.639139] [Steps   38940] [G 0.0020796]
[2023-05-12 04:49:40.315845] [Steps   38950] [G 0.0013579]
[2023-05-12 04:49:58.772686] [Steps   38960] [G 0.0010972]
[2023-05-12 04:50:15.405881] [Steps   38970] [G 0.0012561]
[2023-05-12 04:50:32.039818] [Steps   38980] [G 0.0018793]
[2023-05-12 04:50:48.744575] [Steps   38990] [G 0.0012413]
[2023-05-12 04:51:05.360098] [Steps   39000] [G 0.0011024]
Steps 39001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 04:51:23.329889] [Steps   39010] [G 0.0010404]
[2023-05-12 04:51:39.972707] [Steps   39020] [G 0.0013556]
[2023-05-12 04:51:56.612607] [Steps   39030] [G 0.0011717]
[2023-05-12 04:52:13.375289] [Steps   39040] [G 0.0012009]
[2023-05-12 04:52:30.074940] [Steps   39050] [G 0.0013623]
[2023-05-12 04:52:46.744157] [Steps   39060] [G 0.0012084]
[2023-05-12 04:53:03.520393] [Steps   39070] [G 0.0018679]
[2023-05-12 04:53:20.160637] [Steps   39080] [G 0.0013788]
[2023-05-12 04:53:36.854213] [Steps   39090] [G 0.0012323]
[2023-05-12 04:53:53.524308] [Steps   39100] [G 0.0013401]
[2023-05-12 04:54:10.277932] [Steps   39110] [G 0.0013239]
[2023-05-12 04:54:26.951351] [Steps   39120] [G 0.0008891]
[2023-05-12 04:54:43.632212] [Steps   39130] [G 0.0010570]
[2023-05-12 04:55:00.317609] [Steps   39140] [G 0.0016677]
[2023-05-12 04:55:16.981633] [Steps   39150] [G 0.0010346]
[2023-05-12 04:55:33.651392] [Steps   39160] [G 0.0010808]
[2023-05-12 04:55:50.422355] [Steps   39170] [G 0.0014000]
[2023-05-12 04:56:07.086668] [Steps   39180] [G 0.0013592]
[2023-05-12 04:56:23.790556] [Steps   39190] [G 0.0012391]
[2023-05-12 04:56:40.459889] [Steps   39200] [G 0.0022272]
[2023-05-12 04:56:57.213936] [Steps   39210] [G 0.0011914]
[2023-05-12 04:57:13.906118] [Steps   39220] [G 0.0016499]
[2023-05-12 04:57:30.615961] [Steps   39230] [G 0.0010012]
[2023-05-12 04:57:47.347229] [Steps   39240] [G 0.0011600]
[2023-05-12 04:58:04.021025] [Steps   39250] [G 0.0012711]
[2023-05-12 04:58:20.615413] [Steps   39260] [G 0.0020228]
[2023-05-12 04:58:39.244077] [Steps   39270] [G 0.0011583]
[2023-05-12 04:58:55.979132] [Steps   39280] [G 0.0011736]
[2023-05-12 04:59:12.684566] [Steps   39290] [G 0.0012899]
[2023-05-12 04:59:29.335197] [Steps   39300] [G 0.0010202]
[2023-05-12 04:59:46.029307] [Steps   39310] [G 0.0013738]
[2023-05-12 05:00:02.690965] [Steps   39320] [G 0.0008917]
[2023-05-12 05:00:19.369831] [Steps   39330] [G 0.0013953]
[2023-05-12 05:00:36.088813] [Steps   39340] [G 0.0012324]
[2023-05-12 05:00:52.737803] [Steps   39350] [G 0.0009746]
[2023-05-12 05:01:09.367242] [Steps   39360] [G 0.0015365]
[2023-05-12 05:01:26.044042] [Steps   39370] [G 0.0015839]
[2023-05-12 05:01:42.779593] [Steps   39380] [G 0.0010578]
[2023-05-12 05:01:59.406003] [Steps   39390] [G 0.0015423]
[2023-05-12 05:02:16.069858] [Steps   39400] [G 0.0015477]
[2023-05-12 05:02:32.760830] [Steps   39410] [G 0.0014969]
[2023-05-12 05:02:49.414781] [Steps   39420] [G 0.0010499]
[2023-05-12 05:03:06.048900] [Steps   39430] [G 0.0014427]
[2023-05-12 05:03:22.728087] [Steps   39440] [G 0.0016952]
[2023-05-12 05:03:39.454871] [Steps   39450] [G 0.0009368]
[2023-05-12 05:03:56.107598] [Steps   39460] [G 0.0011639]
[2023-05-12 05:04:12.794936] [Steps   39470] [G 0.0012461]
[2023-05-12 05:04:29.526206] [Steps   39480] [G 0.0015072]
[2023-05-12 05:04:46.200777] [Steps   39490] [G 0.0011313]
[2023-05-12 05:05:02.829656] [Steps   39500] [G 0.0019126]
[2023-05-12 05:05:19.519313] [Steps   39510] [G 0.0016967]
[2023-05-12 05:05:36.154227] [Steps   39520] [G 0.0015211]
[2023-05-12 05:05:52.816868] [Steps   39530] [G 0.0017736]
[2023-05-12 05:06:09.444138] [Steps   39540] [G 0.0016873]
[2023-05-12 05:06:26.173953] [Steps   39550] [G 0.0016988]
[2023-05-12 05:06:42.789600] [Steps   39560] [G 0.0019109]
[2023-05-12 05:07:01.418549] [Steps   39570] [G 0.0012312]
[2023-05-12 05:07:18.114559] [Steps   39580] [G 0.0012603]
[2023-05-12 05:07:34.718033] [Steps   39590] [G 0.0013410]
[2023-05-12 05:07:51.398811] [Steps   39600] [G 0.0013787]
[2023-05-12 05:08:08.072857] [Steps   39610] [G 0.0017624]
[2023-05-12 05:08:24.814590] [Steps   39620] [G 0.0015910]
[2023-05-12 05:08:41.472407] [Steps   39630] [G 0.0019269]
[2023-05-12 05:08:58.101109] [Steps   39640] [G 0.0015185]
[2023-05-12 05:09:14.816684] [Steps   39650] [G 0.0009598]
[2023-05-12 05:09:31.481874] [Steps   39660] [G 0.0015998]
[2023-05-12 05:09:48.156613] [Steps   39670] [G 0.0011681]
[2023-05-12 05:10:04.787683] [Steps   39680] [G 0.0014043]
[2023-05-12 05:10:21.450854] [Steps   39690] [G 0.0012279]
[2023-05-12 05:10:38.081608] [Steps   39700] [G 0.0015413]
[2023-05-12 05:10:54.690327] [Steps   39710] [G 0.0018377]
[2023-05-12 05:11:11.380760] [Steps   39720] [G 0.0012917]
[2023-05-12 05:11:27.986567] [Steps   39730] [G 0.0016499]
[2023-05-12 05:11:44.608969] [Steps   39740] [G 0.0009902]
[2023-05-12 05:12:01.289137] [Steps   39750] [G 0.0011019]
[2023-05-12 05:12:17.907158] [Steps   39760] [G 0.0010250]
[2023-05-12 05:12:34.531950] [Steps   39770] [G 0.0011466]
[2023-05-12 05:12:51.125567] [Steps   39780] [G 0.0011329]
[2023-05-12 05:13:07.812870] [Steps   39790] [G 0.0010171]
[2023-05-12 05:13:24.413371] [Steps   39800] [G 0.0013011]
[2023-05-12 05:13:41.044452] [Steps   39810] [G 0.0008622]
[2023-05-12 05:13:57.826453] [Steps   39820] [G 0.0016597]
[2023-05-12 05:14:14.479612] [Steps   39830] [G 0.0011798]
[2023-05-12 05:14:31.128899] [Steps   39840] [G 0.0011120]
[2023-05-12 05:14:47.740249] [Steps   39850] [G 0.0017310]
[2023-05-12 05:15:04.382272] [Steps   39860] [G 0.0022017]
[2023-05-12 05:15:23.049738] [Steps   39870] [G 0.0014059]
[2023-05-12 05:15:39.658839] [Steps   39880] [G 0.0010060]
[2023-05-12 05:15:56.333972] [Steps   39890] [G 0.0008828]
[2023-05-12 05:16:12.947435] [Steps   39900] [G 0.0018329]
[2023-05-12 05:16:29.601392] [Steps   39910] [G 0.0010157]
[2023-05-12 05:16:46.329678] [Steps   39920] [G 0.0009639]
[2023-05-12 05:17:02.993183] [Steps   39930] [G 0.0012719]
[2023-05-12 05:17:19.678885] [Steps   39940] [G 0.0013677]
[2023-05-12 05:17:36.353911] [Steps   39950] [G 0.0020180]
[2023-05-12 05:17:53.072422] [Steps   39960] [G 0.0013739]
[2023-05-12 05:18:09.709330] [Steps   39970] [G 0.0013055]
[2023-05-12 05:18:26.392746] [Steps   39980] [G 0.0012251]
[2023-05-12 05:18:43.158769] [Steps   39990] [G 0.0013872]
[2023-05-12 05:18:59.866356] [Steps   40000] [G 0.0009554]
Steps 40001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 05:19:19.072935] [Steps   40010] [G 0.0012061]
[2023-05-12 05:19:35.742417] [Steps   40020] [G 0.0014328]
[2023-05-12 05:19:52.511160] [Steps   40030] [G 0.0009898]
[2023-05-12 05:20:09.218560] [Steps   40040] [G 0.0017745]
[2023-05-12 05:20:25.977044] [Steps   40050] [G 0.0016182]
[2023-05-12 05:20:42.663025] [Steps   40060] [G 0.0008150]
[2023-05-12 05:20:59.358964] [Steps   40070] [G 0.0011283]
[2023-05-12 05:21:16.109749] [Steps   40080] [G 0.0011579]
[2023-05-12 05:21:32.795972] [Steps   40090] [G 0.0011439]
[2023-05-12 05:21:49.517800] [Steps   40100] [G 0.0011384]
[2023-05-12 05:22:06.201532] [Steps   40110] [G 0.0017920]
[2023-05-12 05:22:22.951503] [Steps   40120] [G 0.0007250]
[2023-05-12 05:22:39.607396] [Steps   40130] [G 0.0014045]
[2023-05-12 05:22:56.312361] [Steps   40140] [G 0.0009473]
[2023-05-12 05:23:13.085095] [Steps   40150] [G 0.0030281]
[2023-05-12 05:23:29.691818] [Steps   40160] [G 0.0010282]
[2023-05-12 05:23:48.245883] [Steps   40170] [G 0.0011383]
[2023-05-12 05:24:04.953724] [Steps   40180] [G 0.0012428]
[2023-05-12 05:24:21.600218] [Steps   40190] [G 0.0013222]
[2023-05-12 05:24:38.256205] [Steps   40200] [G 0.0016899]
[2023-05-12 05:24:54.923164] [Steps   40210] [G 0.0010910]
[2023-05-12 05:25:11.652973] [Steps   40220] [G 0.0013872]
[2023-05-12 05:25:28.355512] [Steps   40230] [G 0.0013676]
[2023-05-12 05:25:45.026425] [Steps   40240] [G 0.0009461]
[2023-05-12 05:26:01.783632] [Steps   40250] [G 0.0021556]
[2023-05-12 05:26:18.475506] [Steps   40260] [G 0.0013602]
[2023-05-12 05:26:35.140482] [Steps   40270] [G 0.0010358]
[2023-05-12 05:26:51.838398] [Steps   40280] [G 0.0013582]
[2023-05-12 05:27:08.632000] [Steps   40290] [G 0.0012710]
[2023-05-12 05:27:25.282906] [Steps   40300] [G 0.0009823]
[2023-05-12 05:27:41.942582] [Steps   40310] [G 0.0012874]
[2023-05-12 05:27:58.644369] [Steps   40320] [G 0.0020712]
[2023-05-12 05:28:15.336595] [Steps   40330] [G 0.0018645]
[2023-05-12 05:28:32.012988] [Steps   40340] [G 0.0021931]
[2023-05-12 05:28:48.771390] [Steps   40350] [G 0.0014367]
[2023-05-12 05:29:05.411783] [Steps   40360] [G 0.0013180]
[2023-05-12 05:29:22.051507] [Steps   40370] [G 0.0011030]
[2023-05-12 05:29:38.722392] [Steps   40380] [G 0.0009428]
[2023-05-12 05:29:55.428382] [Steps   40390] [G 0.0012975]
[2023-05-12 05:30:12.093934] [Steps   40400] [G 0.0010444]
[2023-05-12 05:30:28.760092] [Steps   40410] [G 0.0012530]
[2023-05-12 05:30:45.501424] [Steps   40420] [G 0.0012845]
[2023-05-12 05:31:02.202028] [Steps   40430] [G 0.0008883]
[2023-05-12 05:31:18.857458] [Steps   40440] [G 0.0016813]
[2023-05-12 05:31:35.553634] [Steps   40450] [G 0.0013241]
[2023-05-12 05:31:52.270897] [Steps   40460] [G 0.0012037]
[2023-05-12 05:32:10.834525] [Steps   40470] [G 0.0010064]
[2023-05-12 05:32:27.424807] [Steps   40480] [G 0.0014397]
[2023-05-12 05:32:44.129175] [Steps   40490] [G 0.0014568]
[2023-05-12 05:33:00.733595] [Steps   40500] [G 0.0026162]
[2023-05-12 05:33:17.336482] [Steps   40510] [G 0.0012924]
[2023-05-12 05:33:34.041866] [Steps   40520] [G 0.0012254]
[2023-05-12 05:33:50.655037] [Steps   40530] [G 0.0018438]
[2023-05-12 05:34:07.307899] [Steps   40540] [G 0.0011031]
[2023-05-12 05:34:23.950124] [Steps   40550] [G 0.0014077]
[2023-05-12 05:34:40.685007] [Steps   40560] [G 0.0013073]
[2023-05-12 05:34:57.349091] [Steps   40570] [G 0.0010523]
[2023-05-12 05:35:14.034904] [Steps   40580] [G 0.0012272]
[2023-05-12 05:35:30.791094] [Steps   40590] [G 0.0012000]
[2023-05-12 05:35:47.445431] [Steps   40600] [G 0.0014697]
[2023-05-12 05:36:04.133757] [Steps   40610] [G 0.0010608]
[2023-05-12 05:36:20.790829] [Steps   40620] [G 0.0010975]
[2023-05-12 05:36:37.531657] [Steps   40630] [G 0.0014405]
[2023-05-12 05:36:54.182079] [Steps   40640] [G 0.0012215]
[2023-05-12 05:37:10.849752] [Steps   40650] [G 0.0013235]
[2023-05-12 05:37:27.608660] [Steps   40660] [G 0.0017350]
[2023-05-12 05:37:44.287618] [Steps   40670] [G 0.0018508]
[2023-05-12 05:38:00.955431] [Steps   40680] [G 0.0011697]
[2023-05-12 05:38:17.667155] [Steps   40690] [G 0.0011770]
[2023-05-12 05:38:34.347844] [Steps   40700] [G 0.0012657]
[2023-05-12 05:38:51.026133] [Steps   40710] [G 0.0015020]
[2023-05-12 05:39:07.673620] [Steps   40720] [G 0.0014173]
[2023-05-12 05:39:24.364399] [Steps   40730] [G 0.0013036]
[2023-05-12 05:39:41.056112] [Steps   40740] [G 0.0008752]
[2023-05-12 05:39:57.723714] [Steps   40750] [G 0.0014522]
[2023-05-12 05:40:14.416378] [Steps   40760] [G 0.0016459]
[2023-05-12 05:40:31.010991] [Steps   40770] [G 0.0015402]
[2023-05-12 05:40:49.740141] [Steps   40780] [G 0.0012338]
[2023-05-12 05:41:06.283599] [Steps   40790] [G 0.0024539]
[2023-05-12 05:41:22.916214] [Steps   40800] [G 0.0010945]
[2023-05-12 05:41:39.471767] [Steps   40810] [G 0.0009267]
[2023-05-12 05:41:56.046730] [Steps   40820] [G 0.0010657]
[2023-05-12 05:42:12.702089] [Steps   40830] [G 0.0011347]
[2023-05-12 05:42:29.291625] [Steps   40840] [G 0.0015455]
[2023-05-12 05:42:45.858167] [Steps   40850] [G 0.0013480]
[2023-05-12 05:43:02.512952] [Steps   40860] [G 0.0017173]
[2023-05-12 05:43:19.138263] [Steps   40870] [G 0.0010980]
[2023-05-12 05:43:35.746184] [Steps   40880] [G 0.0014222]
[2023-05-12 05:43:52.336945] [Steps   40890] [G 0.0019901]
[2023-05-12 05:44:09.001790] [Steps   40900] [G 0.0015828]
[2023-05-12 05:44:25.626766] [Steps   40910] [G 0.0011150]
[2023-05-12 05:44:42.239886] [Steps   40920] [G 0.0013016]
[2023-05-12 05:44:58.923882] [Steps   40930] [G 0.0009949]
[2023-05-12 05:45:15.534307] [Steps   40940] [G 0.0016163]
[2023-05-12 05:45:32.157603] [Steps   40950] [G 0.0011163]
[2023-05-12 05:45:48.773169] [Steps   40960] [G 0.0009687]
[2023-05-12 05:46:05.436804] [Steps   40970] [G 0.0015642]
[2023-05-12 05:46:22.049487] [Steps   40980] [G 0.0012468]
[2023-05-12 05:46:38.673326] [Steps   40990] [G 0.0010535]
[2023-05-12 05:46:55.343014] [Steps   41000] [G 0.0016855]
Steps 41001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 05:47:13.280597] [Steps   41010] [G 0.0013757]
[2023-05-12 05:47:29.928770] [Steps   41020] [G 0.0012404]
[2023-05-12 05:47:46.620400] [Steps   41030] [G 0.0014256]
[2023-05-12 05:48:03.205563] [Steps   41040] [G 0.0019813]
[2023-05-12 05:48:19.865555] [Steps   41050] [G 0.0019272]
[2023-05-12 05:48:36.461328] [Steps   41060] [G 0.0016712]
[2023-05-12 05:48:53.041598] [Steps   41070] [G 0.0010439]
[2023-05-12 05:49:11.547968] [Steps   41080] [G 0.0017843]
[2023-05-12 05:49:28.269312] [Steps   41090] [G 0.0011786]
[2023-05-12 05:49:44.945061] [Steps   41100] [G 0.0014865]
[2023-05-12 05:50:01.631042] [Steps   41110] [G 0.0012217]
[2023-05-12 05:50:18.344212] [Steps   41120] [G 0.0013941]
[2023-05-12 05:50:35.014998] [Steps   41130] [G 0.0010811]
[2023-05-12 05:50:51.701781] [Steps   41140] [G 0.0014939]
[2023-05-12 05:51:08.442275] [Steps   41150] [G 0.0010057]
[2023-05-12 05:51:25.105768] [Steps   41160] [G 0.0010679]
[2023-05-12 05:51:41.720742] [Steps   41170] [G 0.0010433]
[2023-05-12 05:51:58.363076] [Steps   41180] [G 0.0015156]
[2023-05-12 05:52:15.120104] [Steps   41190] [G 0.0011042]
[2023-05-12 05:52:31.802196] [Steps   41200] [G 0.0016213]
[2023-05-12 05:52:48.472064] [Steps   41210] [G 0.0013720]
[2023-05-12 05:53:05.243358] [Steps   41220] [G 0.0012670]
[2023-05-12 05:53:21.903506] [Steps   41230] [G 0.0012177]
[2023-05-12 05:53:38.596221] [Steps   41240] [G 0.0015083]
[2023-05-12 05:53:55.264671] [Steps   41250] [G 0.0013878]
[2023-05-12 05:54:12.043194] [Steps   41260] [G 0.0011151]
[2023-05-12 05:54:28.701396] [Steps   41270] [G 0.0022720]
[2023-05-12 05:54:45.372157] [Steps   41280] [G 0.0010228]
[2023-05-12 05:55:02.098779] [Steps   41290] [G 0.0024405]
[2023-05-12 05:55:18.763129] [Steps   41300] [G 0.0010271]
[2023-05-12 05:55:35.415614] [Steps   41310] [G 0.0008018]
[2023-05-12 05:55:52.175723] [Steps   41320] [G 0.0009746]
[2023-05-12 05:56:08.809581] [Steps   41330] [G 0.0012062]
[2023-05-12 05:56:25.451852] [Steps   41340] [G 0.0009274]
[2023-05-12 05:56:42.134964] [Steps   41350] [G 0.0013374]
[2023-05-12 05:56:58.926323] [Steps   41360] [G 0.0025038]
[2023-05-12 05:57:15.538386] [Steps   41370] [G 0.0009760]
[2023-05-12 05:57:34.168012] [Steps   41380] [G 0.0008567]
[2023-05-12 05:57:50.933031] [Steps   41390] [G 0.0013578]
[2023-05-12 05:58:07.553977] [Steps   41400] [G 0.0016264]
[2023-05-12 05:58:24.310875] [Steps   41410] [G 0.0012416]
[2023-05-12 05:58:41.001816] [Steps   41420] [G 0.0009291]
[2023-05-12 05:58:57.738716] [Steps   41430] [G 0.0013414]
[2023-05-12 05:59:14.428103] [Steps   41440] [G 0.0009229]
[2023-05-12 05:59:31.093609] [Steps   41450] [G 0.0010252]
[2023-05-12 05:59:47.865441] [Steps   41460] [G 0.0013932]
[2023-05-12 06:00:04.543058] [Steps   41470] [G 0.0018036]
[2023-05-12 06:00:21.215038] [Steps   41480] [G 0.0009075]
[2023-05-12 06:00:37.954286] [Steps   41490] [G 0.0013396]
[2023-05-12 06:00:54.607583] [Steps   41500] [G 0.0012937]
[2023-05-12 06:01:11.311883] [Steps   41510] [G 0.0008177]
[2023-05-12 06:01:28.024654] [Steps   41520] [G 0.0015654]
[2023-05-12 06:01:44.766076] [Steps   41530] [G 0.0010003]
[2023-05-12 06:02:01.460731] [Steps   41540] [G 0.0010071]
[2023-05-12 06:02:18.153948] [Steps   41550] [G 0.0012209]
[2023-05-12 06:02:34.884380] [Steps   41560] [G 0.0011943]
[2023-05-12 06:02:51.551644] [Steps   41570] [G 0.0012569]
[2023-05-12 06:03:08.219833] [Steps   41580] [G 0.0011194]
[2023-05-12 06:03:24.827973] [Steps   41590] [G 0.0017244]
[2023-05-12 06:03:41.532129] [Steps   41600] [G 0.0009892]
[2023-05-12 06:03:58.226769] [Steps   41610] [G 0.0013644]
[2023-05-12 06:04:14.921015] [Steps   41620] [G 0.0012265]
[2023-05-12 06:04:31.668226] [Steps   41630] [G 0.0008346]
[2023-05-12 06:04:48.353045] [Steps   41640] [G 0.0011736]
[2023-05-12 06:05:04.979938] [Steps   41650] [G 0.0014280]
[2023-05-12 06:05:21.711694] [Steps   41660] [G 0.0021869]
[2023-05-12 06:05:38.327551] [Steps   41670] [G 0.0013106]
[2023-05-12 06:05:56.900191] [Steps   41680] [G 0.0018285]
[2023-05-12 06:06:13.503617] [Steps   41690] [G 0.0012492]
[2023-05-12 06:06:30.181921] [Steps   41700] [G 0.0014249]
[2023-05-12 06:06:46.795392] [Steps   41710] [G 0.0012096]
[2023-05-12 06:07:03.440452] [Steps   41720] [G 0.0010969]
[2023-05-12 06:07:20.174835] [Steps   41730] [G 0.0016773]
[2023-05-12 06:07:36.785648] [Steps   41740] [G 0.0007151]
[2023-05-12 06:07:53.417918] [Steps   41750] [G 0.0009591]
[2023-05-12 06:08:10.053038] [Steps   41760] [G 0.0012177]
[2023-05-12 06:08:26.762935] [Steps   41770] [G 0.0008891]
[2023-05-12 06:08:43.394523] [Steps   41780] [G 0.0011329]
[2023-05-12 06:09:00.024924] [Steps   41790] [G 0.0007479]
[2023-05-12 06:09:16.752965] [Steps   41800] [G 0.0008850]
[2023-05-12 06:09:33.370718] [Steps   41810] [G 0.0010119]
[2023-05-12 06:09:50.011966] [Steps   41820] [G 0.0011249]
[2023-05-12 06:10:06.677527] [Steps   41830] [G 0.0009772]
[2023-05-12 06:10:23.311644] [Steps   41840] [G 0.0014624]
[2023-05-12 06:10:39.960429] [Steps   41850] [G 0.0009954]
[2023-05-12 06:10:56.611915] [Steps   41860] [G 0.0014375]
[2023-05-12 06:11:13.323914] [Steps   41870] [G 0.0013293]
[2023-05-12 06:11:30.003150] [Steps   41880] [G 0.0008659]
[2023-05-12 06:11:46.659749] [Steps   41890] [G 0.0020532]
[2023-05-12 06:12:03.447692] [Steps   41900] [G 0.0010318]
[2023-05-12 06:12:20.077228] [Steps   41910] [G 0.0009530]
[2023-05-12 06:12:36.744543] [Steps   41920] [G 0.0011836]
[2023-05-12 06:12:53.428801] [Steps   41930] [G 0.0015716]
[2023-05-12 06:13:10.143032] [Steps   41940] [G 0.0010893]
[2023-05-12 06:13:26.791201] [Steps   41950] [G 0.0009916]
[2023-05-12 06:13:43.445895] [Steps   41960] [G 0.0008768]
[2023-05-12 06:14:00.132527] [Steps   41970] [G 0.0014905]
[2023-05-12 06:14:18.742276] [Steps   41980] [G 0.0009159]
[2023-05-12 06:14:35.354233] [Steps   41990] [G 0.0011013]
[2023-05-12 06:14:52.083398] [Steps   42000] [G 0.0010567]
Steps 42001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 06:15:10.138774] [Steps   42010] [G 0.0012776]
[2023-05-12 06:15:26.861096] [Steps   42020] [G 0.0013839]
[2023-05-12 06:15:43.506432] [Steps   42030] [G 0.0011573]
[2023-05-12 06:16:00.231643] [Steps   42040] [G 0.0014427]
[2023-05-12 06:16:16.905738] [Steps   42050] [G 0.0012519]
[2023-05-12 06:16:33.662727] [Steps   42060] [G 0.0018910]
[2023-05-12 06:16:50.398527] [Steps   42070] [G 0.0009461]
[2023-05-12 06:17:07.099693] [Steps   42080] [G 0.0010805]
[2023-05-12 06:17:23.817752] [Steps   42090] [G 0.0007304]
[2023-05-12 06:17:40.513257] [Steps   42100] [G 0.0015834]
[2023-05-12 06:17:57.238914] [Steps   42110] [G 0.0008742]
[2023-05-12 06:18:13.989175] [Steps   42120] [G 0.0021394]
[2023-05-12 06:18:30.675386] [Steps   42130] [G 0.0007500]
[2023-05-12 06:18:47.416886] [Steps   42140] [G 0.0012214]
[2023-05-12 06:19:04.096279] [Steps   42150] [G 0.0014208]
[2023-05-12 06:19:20.811547] [Steps   42160] [G 0.0013260]
[2023-05-12 06:19:37.517111] [Steps   42170] [G 0.0010783]
[2023-05-12 06:19:54.223557] [Steps   42180] [G 0.0012689]
[2023-05-12 06:20:10.949754] [Steps   42190] [G 0.0011927]
[2023-05-12 06:20:27.627850] [Steps   42200] [G 0.0013838]
[2023-05-12 06:20:44.353217] [Steps   42210] [G 0.0008592]
[2023-05-12 06:21:01.017838] [Steps   42220] [G 0.0010642]
[2023-05-12 06:21:17.728959] [Steps   42230] [G 0.0014007]
[2023-05-12 06:21:34.463497] [Steps   42240] [G 0.0015323]
[2023-05-12 06:21:51.110882] [Steps   42250] [G 0.0012141]
[2023-05-12 06:22:07.882786] [Steps   42260] [G 0.0011213]
[2023-05-12 06:22:24.557186] [Steps   42270] [G 0.0014404]
[2023-05-12 06:22:41.199034] [Steps   42280] [G 0.0016382]
[2023-05-12 06:22:59.869314] [Steps   42290] [G 0.0008762]
[2023-05-12 06:23:16.459041] [Steps   42300] [G 0.0012654]
[2023-05-12 06:23:33.149270] [Steps   42310] [G 0.0012847]
[2023-05-12 06:23:49.778622] [Steps   42320] [G 0.0010509]
[2023-05-12 06:24:06.481749] [Steps   42330] [G 0.0010368]
[2023-05-12 06:24:23.117937] [Steps   42340] [G 0.0011957]
[2023-05-12 06:24:39.822266] [Steps   42350] [G 0.0016166]
[2023-05-12 06:24:56.545704] [Steps   42360] [G 0.0011369]
[2023-05-12 06:25:13.186245] [Steps   42370] [G 0.0018120]
[2023-05-12 06:25:29.879446] [Steps   42380] [G 0.0008439]
[2023-05-12 06:25:46.562126] [Steps   42390] [G 0.0020276]
[2023-05-12 06:26:03.361608] [Steps   42400] [G 0.0011690]
[2023-05-12 06:26:20.127274] [Steps   42410] [G 0.0008922]
[2023-05-12 06:26:36.764800] [Steps   42420] [G 0.0020649]
[2023-05-12 06:26:53.478330] [Steps   42430] [G 0.0009408]
[2023-05-12 06:27:10.151036] [Steps   42440] [G 0.0010469]
[2023-05-12 06:27:26.870043] [Steps   42450] [G 0.0010061]
[2023-05-12 06:27:43.581021] [Steps   42460] [G 0.0009806]
[2023-05-12 06:28:00.248089] [Steps   42470] [G 0.0010127]
[2023-05-12 06:28:17.003790] [Steps   42480] [G 0.0013567]
[2023-05-12 06:28:33.674962] [Steps   42490] [G 0.0012686]
[2023-05-12 06:28:50.394633] [Steps   42500] [G 0.0011647]
[2023-05-12 06:29:07.061534] [Steps   42510] [G 0.0010300]
[2023-05-12 06:29:23.805612] [Steps   42520] [G 0.0010642]
[2023-05-12 06:29:40.529836] [Steps   42530] [G 0.0009485]
[2023-05-12 06:29:57.177851] [Steps   42540] [G 0.0014043]
[2023-05-12 06:30:13.912303] [Steps   42550] [G 0.0013963]
[2023-05-12 06:30:30.589616] [Steps   42560] [G 0.0009765]
[2023-05-12 06:30:47.379363] [Steps   42570] [G 0.0014832]
[2023-05-12 06:31:04.079171] [Steps   42580] [G 0.0010782]
[2023-05-12 06:31:22.747734] [Steps   42590] [G 0.0008666]
[2023-05-12 06:31:39.463983] [Steps   42600] [G 0.0008601]
[2023-05-12 06:31:56.104543] [Steps   42610] [G 0.0009379]
[2023-05-12 06:32:12.793631] [Steps   42620] [G 0.0017913]
[2023-05-12 06:32:29.514813] [Steps   42630] [G 0.0011915]
[2023-05-12 06:32:46.182447] [Steps   42640] [G 0.0009290]
[2023-05-12 06:33:02.850145] [Steps   42650] [G 0.0009180]
[2023-05-12 06:33:19.522361] [Steps   42660] [G 0.0013168]
[2023-05-12 06:33:36.245326] [Steps   42670] [G 0.0010809]
[2023-05-12 06:33:52.946412] [Steps   42680] [G 0.0008466]
[2023-05-12 06:34:09.627514] [Steps   42690] [G 0.0011031]
[2023-05-12 06:34:26.361079] [Steps   42700] [G 0.0011875]
[2023-05-12 06:34:43.043570] [Steps   42710] [G 0.0010682]
[2023-05-12 06:34:59.723170] [Steps   42720] [G 0.0015843]
[2023-05-12 06:35:16.382117] [Steps   42730] [G 0.0013139]
[2023-05-12 06:35:33.128733] [Steps   42740] [G 0.0009766]
[2023-05-12 06:35:49.777459] [Steps   42750] [G 0.0010063]
[2023-05-12 06:36:06.444932] [Steps   42760] [G 0.0010013]
[2023-05-12 06:36:23.181218] [Steps   42770] [G 0.0010893]
[2023-05-12 06:36:39.849039] [Steps   42780] [G 0.0013614]
[2023-05-12 06:36:56.492648] [Steps   42790] [G 0.0018516]
[2023-05-12 06:37:13.234815] [Steps   42800] [G 0.0015422]
[2023-05-12 06:37:29.890800] [Steps   42810] [G 0.0017686]
[2023-05-12 06:37:46.555171] [Steps   42820] [G 0.0008625]
[2023-05-12 06:38:03.202493] [Steps   42830] [G 0.0010814]
[2023-05-12 06:38:19.949491] [Steps   42840] [G 0.0017739]
[2023-05-12 06:38:36.608200] [Steps   42850] [G 0.0009217]
[2023-05-12 06:38:53.227863] [Steps   42860] [G 0.0015057]
[2023-05-12 06:39:09.987413] [Steps   42870] [G 0.0015582]
[2023-05-12 06:39:26.578503] [Steps   42880] [G 0.0011233]
[2023-05-12 06:39:45.164752] [Steps   42890] [G 0.0010017]
[2023-05-12 06:40:01.804437] [Steps   42900] [G 0.0011305]
[2023-05-12 06:40:18.520544] [Steps   42910] [G 0.0012229]
[2023-05-12 06:40:35.099433] [Steps   42920] [G 0.0011330]
[2023-05-12 06:40:51.689849] [Steps   42930] [G 0.0010595]
[2023-05-12 06:41:08.437835] [Steps   42940] [G 0.0012233]
[2023-05-12 06:41:25.076576] [Steps   42950] [G 0.0011705]
[2023-05-12 06:41:41.698690] [Steps   42960] [G 0.0016817]
[2023-05-12 06:41:58.331000] [Steps   42970] [G 0.0014776]
[2023-05-12 06:42:15.042291] [Steps   42980] [G 0.0011942]
[2023-05-12 06:42:31.663876] [Steps   42990] [G 0.0008526]
[2023-05-12 06:42:48.269535] [Steps   43000] [G 0.0016795]
Steps 43001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 06:43:06.288351] [Steps   43010] [G 0.0009537]
[2023-05-12 06:43:22.879759] [Steps   43020] [G 0.0019811]
[2023-05-12 06:43:39.577054] [Steps   43030] [G 0.0009121]
[2023-05-12 06:43:56.204689] [Steps   43040] [G 0.0013643]
[2023-05-12 06:44:12.817806] [Steps   43050] [G 0.0009581]
[2023-05-12 06:44:29.526972] [Steps   43060] [G 0.0014304]
[2023-05-12 06:44:46.134573] [Steps   43070] [G 0.0011534]
[2023-05-12 06:45:02.773585] [Steps   43080] [G 0.0008753]
[2023-05-12 06:45:19.461898] [Steps   43090] [G 0.0010311]
[2023-05-12 06:45:36.070543] [Steps   43100] [G 0.0011623]
[2023-05-12 06:45:52.723400] [Steps   43110] [G 0.0009880]
[2023-05-12 06:46:09.333779] [Steps   43120] [G 0.0011270]
[2023-05-12 06:46:26.053244] [Steps   43130] [G 0.0012039]
[2023-05-12 06:46:42.664603] [Steps   43140] [G 0.0010611]
[2023-05-12 06:46:59.287791] [Steps   43150] [G 0.0009579]
[2023-05-12 06:47:15.970802] [Steps   43160] [G 0.0010415]
[2023-05-12 06:47:32.600643] [Steps   43170] [G 0.0015852]
[2023-05-12 06:47:49.221520] [Steps   43180] [G 0.0015142]
[2023-05-12 06:48:07.866372] [Steps   43190] [G 0.0010578]
[2023-05-12 06:48:24.537925] [Steps   43200] [G 0.0011339]
[2023-05-12 06:48:41.111140] [Steps   43210] [G 0.0007404]
[2023-05-12 06:48:57.697675] [Steps   43220] [G 0.0018792]
[2023-05-12 06:49:14.392333] [Steps   43230] [G 0.0012991]
[2023-05-12 06:49:30.978373] [Steps   43240] [G 0.0012631]
[2023-05-12 06:49:47.582049] [Steps   43250] [G 0.0008784]
[2023-05-12 06:50:04.237009] [Steps   43260] [G 0.0009483]
[2023-05-12 06:50:20.836280] [Steps   43270] [G 0.0010075]
[2023-05-12 06:50:37.434115] [Steps   43280] [G 0.0011129]
[2023-05-12 06:50:54.042151] [Steps   43290] [G 0.0010894]
[2023-05-12 06:51:10.729553] [Steps   43300] [G 0.0010287]
[2023-05-12 06:51:27.340189] [Steps   43310] [G 0.0009015]
[2023-05-12 06:51:43.950483] [Steps   43320] [G 0.0016011]
[2023-05-12 06:52:00.631124] [Steps   43330] [G 0.0012089]
[2023-05-12 06:52:17.242500] [Steps   43340] [G 0.0009134]
[2023-05-12 06:52:33.837304] [Steps   43350] [G 0.0010018]
[2023-05-12 06:52:50.426832] [Steps   43360] [G 0.0007315]
[2023-05-12 06:53:07.102658] [Steps   43370] [G 0.0011896]
[2023-05-12 06:53:23.690105] [Steps   43380] [G 0.0013923]
[2023-05-12 06:53:40.303848] [Steps   43390] [G 0.0013240]
[2023-05-12 06:53:56.968810] [Steps   43400] [G 0.0007885]
[2023-05-12 06:54:13.569967] [Steps   43410] [G 0.0015123]
[2023-05-12 06:54:30.157931] [Steps   43420] [G 0.0010552]
[2023-05-12 06:54:46.836262] [Steps   43430] [G 0.0007982]
[2023-05-12 06:55:03.429343] [Steps   43440] [G 0.0011622]
[2023-05-12 06:55:20.018658] [Steps   43450] [G 0.0008196]
[2023-05-12 06:55:36.599348] [Steps   43460] [G 0.0015050]
[2023-05-12 06:55:53.273519] [Steps   43470] [G 0.0010576]
[2023-05-12 06:56:09.873100] [Steps   43480] [G 0.0009439]
[2023-05-12 06:56:28.391573] [Steps   43490] [G 0.0011851]
[2023-05-12 06:56:45.095470] [Steps   43500] [G 0.0007461]
[2023-05-12 06:57:01.684696] [Steps   43510] [G 0.0021418]
[2023-05-12 06:57:18.354593] [Steps   43520] [G 0.0013568]
[2023-05-12 06:57:34.976197] [Steps   43530] [G 0.0018082]
[2023-05-12 06:57:51.681271] [Steps   43540] [G 0.0015190]
[2023-05-12 06:58:08.279571] [Steps   43550] [G 0.0012489]
[2023-05-12 06:58:24.880187] [Steps   43560] [G 0.0014811]
[2023-05-12 06:58:41.570437] [Steps   43570] [G 0.0011320]
[2023-05-12 06:58:58.163773] [Steps   43580] [G 0.0016185]
[2023-05-12 06:59:14.782476] [Steps   43590] [G 0.0011367]
[2023-05-12 06:59:31.401246] [Steps   43600] [G 0.0009143]
[2023-05-12 06:59:48.108969] [Steps   43610] [G 0.0011374]
[2023-05-12 07:00:04.731781] [Steps   43620] [G 0.0013911]
[2023-05-12 07:00:21.326694] [Steps   43630] [G 0.0014871]
[2023-05-12 07:00:38.008623] [Steps   43640] [G 0.0014248]
[2023-05-12 07:00:54.635808] [Steps   43650] [G 0.0011621]
[2023-05-12 07:01:11.224575] [Steps   43660] [G 0.0008060]
[2023-05-12 07:01:27.908292] [Steps   43670] [G 0.0018463]
[2023-05-12 07:01:44.496922] [Steps   43680] [G 0.0010594]
[2023-05-12 07:02:01.098430] [Steps   43690] [G 0.0009645]
[2023-05-12 07:02:17.700521] [Steps   43700] [G 0.0011526]
[2023-05-12 07:02:34.399221] [Steps   43710] [G 0.0013093]
[2023-05-12 07:02:50.983729] [Steps   43720] [G 0.0009912]
[2023-05-12 07:03:07.609324] [Steps   43730] [G 0.0008272]
[2023-05-12 07:03:24.303238] [Steps   43740] [G 0.0012481]
[2023-05-12 07:03:40.907715] [Steps   43750] [G 0.0009217]
[2023-05-12 07:03:57.517589] [Steps   43760] [G 0.0010925]
[2023-05-12 07:04:14.119814] [Steps   43770] [G 0.0015843]
[2023-05-12 07:04:30.814804] [Steps   43780] [G 0.0010136]
[2023-05-12 07:04:47.384689] [Steps   43790] [G 0.0009291]
[2023-05-12 07:05:05.861767] [Steps   43800] [G 0.0011675]
[2023-05-12 07:05:22.587362] [Steps   43810] [G 0.0021556]
[2023-05-12 07:05:39.216344] [Steps   43820] [G 0.0010191]
[2023-05-12 07:05:55.859029] [Steps   43830] [G 0.0014532]
[2023-05-12 07:06:12.559462] [Steps   43840] [G 0.0020461]
[2023-05-12 07:06:29.199348] [Steps   43850] [G 0.0010308]
[2023-05-12 07:06:45.857191] [Steps   43860] [G 0.0008973]
[2023-05-12 07:07:02.480700] [Steps   43870] [G 0.0011020]
[2023-05-12 07:07:19.200306] [Steps   43880] [G 0.0012362]
[2023-05-12 07:07:35.800770] [Steps   43890] [G 0.0008593]
[2023-05-12 07:07:52.448512] [Steps   43900] [G 0.0012700]
[2023-05-12 07:08:09.142359] [Steps   43910] [G 0.0022238]
[2023-05-12 07:08:25.754461] [Steps   43920] [G 0.0010258]
[2023-05-12 07:08:42.409996] [Steps   43930] [G 0.0014876]
[2023-05-12 07:08:59.025085] [Steps   43940] [G 0.0009070]
[2023-05-12 07:09:15.770485] [Steps   43950] [G 0.0011870]
[2023-05-12 07:09:32.397296] [Steps   43960] [G 0.0009399]
[2023-05-12 07:09:49.018309] [Steps   43970] [G 0.0009391]
[2023-05-12 07:10:05.784249] [Steps   43980] [G 0.0010218]
[2023-05-12 07:10:22.408837] [Steps   43990] [G 0.0011438]
[2023-05-12 07:10:39.008013] [Steps   44000] [G 0.0008334]
Steps 44001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 07:10:57.145757] [Steps   44010] [G 0.0011792]
[2023-05-12 07:11:13.789673] [Steps   44020] [G 0.0015638]
[2023-05-12 07:11:30.515614] [Steps   44030] [G 0.0010352]
[2023-05-12 07:11:47.144490] [Steps   44040] [G 0.0008245]
[2023-05-12 07:12:03.782211] [Steps   44050] [G 0.0018129]
[2023-05-12 07:12:20.428360] [Steps   44060] [G 0.0008677]
[2023-05-12 07:12:37.086196] [Steps   44070] [G 0.0010147]
[2023-05-12 07:12:53.670416] [Steps   44080] [G 0.0011673]
[2023-05-12 07:13:10.309549] [Steps   44090] [G 0.0010212]
[2023-05-12 07:13:28.921139] [Steps   44100] [G 0.0018182]
[2023-05-12 07:13:45.521285] [Steps   44110] [G 0.0008728]
[2023-05-12 07:14:02.084738] [Steps   44120] [G 0.0012063]
[2023-05-12 07:14:18.764164] [Steps   44130] [G 0.0010653]
[2023-05-12 07:14:35.370844] [Steps   44140] [G 0.0011520]
[2023-05-12 07:14:51.993277] [Steps   44150] [G 0.0011310]
[2023-05-12 07:15:08.621957] [Steps   44160] [G 0.0016088]
[2023-05-12 07:15:25.324673] [Steps   44170] [G 0.0010093]
[2023-05-12 07:15:41.959934] [Steps   44180] [G 0.0012750]
[2023-05-12 07:15:58.571021] [Steps   44190] [G 0.0008567]
[2023-05-12 07:16:15.276399] [Steps   44200] [G 0.0012391]
[2023-05-12 07:16:31.880765] [Steps   44210] [G 0.0013342]
[2023-05-12 07:16:48.485883] [Steps   44220] [G 0.0009047]
[2023-05-12 07:17:05.121300] [Steps   44230] [G 0.0012021]
[2023-05-12 07:17:21.816336] [Steps   44240] [G 0.0012078]
[2023-05-12 07:17:38.410103] [Steps   44250] [G 0.0008836]
[2023-05-12 07:17:55.009755] [Steps   44260] [G 0.0013954]
[2023-05-12 07:18:11.729520] [Steps   44270] [G 0.0013481]
[2023-05-12 07:18:28.403218] [Steps   44280] [G 0.0009959]
[2023-05-12 07:18:44.981138] [Steps   44290] [G 0.0009601]
[2023-05-12 07:19:01.674151] [Steps   44300] [G 0.0010030]
[2023-05-12 07:19:18.365122] [Steps   44310] [G 0.0019652]
[2023-05-12 07:19:34.992817] [Steps   44320] [G 0.0012373]
[2023-05-12 07:19:51.587120] [Steps   44330] [G 0.0007325]
[2023-05-12 07:20:08.284158] [Steps   44340] [G 0.0015429]
[2023-05-12 07:20:24.860099] [Steps   44350] [G 0.0010501]
[2023-05-12 07:20:41.478556] [Steps   44360] [G 0.0013690]
[2023-05-12 07:20:58.160570] [Steps   44370] [G 0.0010704]
[2023-05-12 07:21:14.756106] [Steps   44380] [G 0.0013733]
[2023-05-12 07:21:31.359463] [Steps   44390] [G 0.0009410]
[2023-05-12 07:21:50.043267] [Steps   44400] [G 0.0008992]
[2023-05-12 07:22:06.719685] [Steps   44410] [G 0.0015109]
[2023-05-12 07:22:23.333026] [Steps   44420] [G 0.0010283]
[2023-05-12 07:22:39.941045] [Steps   44430] [G 0.0007425]
[2023-05-12 07:22:56.621982] [Steps   44440] [G 0.0010642]
[2023-05-12 07:23:13.251565] [Steps   44450] [G 0.0014002]
[2023-05-12 07:23:29.879785] [Steps   44460] [G 0.0009037]
[2023-05-12 07:23:46.558255] [Steps   44470] [G 0.0010912]
[2023-05-12 07:24:03.158419] [Steps   44480] [G 0.0013055]
[2023-05-12 07:24:19.742709] [Steps   44490] [G 0.0008237]
[2023-05-12 07:24:36.363530] [Steps   44500] [G 0.0009498]
[2023-05-12 07:24:53.063740] [Steps   44510] [G 0.0021708]
[2023-05-12 07:25:09.659077] [Steps   44520] [G 0.0016197]
[2023-05-12 07:25:26.272463] [Steps   44530] [G 0.0009494]
[2023-05-12 07:25:42.944990] [Steps   44540] [G 0.0018630]
[2023-05-12 07:25:59.562065] [Steps   44550] [G 0.0010597]
[2023-05-12 07:26:16.167062] [Steps   44560] [G 0.0013807]
[2023-05-12 07:26:32.780213] [Steps   44570] [G 0.0009024]
[2023-05-12 07:26:49.460662] [Steps   44580] [G 0.0008488]
[2023-05-12 07:27:06.085049] [Steps   44590] [G 0.0016212]
[2023-05-12 07:27:22.678889] [Steps   44600] [G 0.0007695]
[2023-05-12 07:27:39.375274] [Steps   44610] [G 0.0011964]
[2023-05-12 07:27:55.982763] [Steps   44620] [G 0.0007357]
[2023-05-12 07:28:12.584210] [Steps   44630] [G 0.0008720]
[2023-05-12 07:28:29.258719] [Steps   44640] [G 0.0011280]
[2023-05-12 07:28:45.866215] [Steps   44650] [G 0.0010799]
[2023-05-12 07:29:02.456614] [Steps   44660] [G 0.0010319]
[2023-05-12 07:29:19.050856] [Steps   44670] [G 0.0011662]
[2023-05-12 07:29:35.726290] [Steps   44680] [G 0.0020325]
[2023-05-12 07:29:52.310664] [Steps   44690] [G 0.0008370]
[2023-05-12 07:30:10.741149] [Steps   44700] [G 0.0010301]
[2023-05-12 07:30:27.434534] [Steps   44710] [G 0.0012359]
[2023-05-12 07:30:44.007886] [Steps   44720] [G 0.0011445]
[2023-05-12 07:31:00.600275] [Steps   44730] [G 0.0012593]
[2023-05-12 07:31:17.194649] [Steps   44740] [G 0.0008300]
[2023-05-12 07:31:33.867169] [Steps   44750] [G 0.0009432]
[2023-05-12 07:31:50.452830] [Steps   44760] [G 0.0009645]
[2023-05-12 07:32:07.054867] [Steps   44770] [G 0.0008925]
[2023-05-12 07:32:23.740383] [Steps   44780] [G 0.0015301]
[2023-05-12 07:32:40.349306] [Steps   44790] [G 0.0008974]
[2023-05-12 07:32:56.937607] [Steps   44800] [G 0.0009222]
[2023-05-12 07:33:13.629997] [Steps   44810] [G 0.0015050]
[2023-05-12 07:33:30.239568] [Steps   44820] [G 0.0015404]
[2023-05-12 07:33:46.930701] [Steps   44830] [G 0.0008098]
[2023-05-12 07:34:03.528682] [Steps   44840] [G 0.0006037]
[2023-05-12 07:34:20.217304] [Steps   44850] [G 0.0009661]
[2023-05-12 07:34:36.824235] [Steps   44860] [G 0.0014123]
[2023-05-12 07:34:53.395855] [Steps   44870] [G 0.0026838]
[2023-05-12 07:35:10.127666] [Steps   44880] [G 0.0012655]
[2023-05-12 07:35:26.705365] [Steps   44890] [G 0.0009739]
[2023-05-12 07:35:43.306464] [Steps   44900] [G 0.0008979]
[2023-05-12 07:35:59.914134] [Steps   44910] [G 0.0016434]
[2023-05-12 07:36:16.587469] [Steps   44920] [G 0.0007642]
[2023-05-12 07:36:33.189014] [Steps   44930] [G 0.0008563]
[2023-05-12 07:36:49.769182] [Steps   44940] [G 0.0010259]
[2023-05-12 07:37:06.454613] [Steps   44950] [G 0.0021530]
[2023-05-12 07:37:23.043638] [Steps   44960] [G 0.0008306]
[2023-05-12 07:37:39.647274] [Steps   44970] [G 0.0010993]
[2023-05-12 07:37:56.308085] [Steps   44980] [G 0.0008893]
[2023-05-12 07:38:12.877252] [Steps   44990] [G 0.0015134]
[2023-05-12 07:38:31.323635] [Steps   45000] [G 0.0009111]
Steps 45001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 07:38:49.290721] [Steps   45010] [G 0.0008631]
[2023-05-12 07:39:05.984173] [Steps   45020] [G 0.0016860]
[2023-05-12 07:39:22.596041] [Steps   45030] [G 0.0010290]
[2023-05-12 07:39:39.274623] [Steps   45040] [G 0.0008603]
[2023-05-12 07:39:55.873905] [Steps   45050] [G 0.0015477]
[2023-05-12 07:40:12.566435] [Steps   45060] [G 0.0013730]
[2023-05-12 07:40:29.265838] [Steps   45070] [G 0.0008608]
[2023-05-12 07:40:45.895124] [Steps   45080] [G 0.0014592]
[2023-05-12 07:41:02.594581] [Steps   45090] [G 0.0010020]
[2023-05-12 07:41:19.198968] [Steps   45100] [G 0.0020834]
[2023-05-12 07:41:35.825824] [Steps   45110] [G 0.0007390]
[2023-05-12 07:41:52.453425] [Steps   45120] [G 0.0011099]
[2023-05-12 07:42:09.139228] [Steps   45130] [G 0.0011780]
[2023-05-12 07:42:25.747614] [Steps   45140] [G 0.0010606]
[2023-05-12 07:42:42.353754] [Steps   45150] [G 0.0007794]
[2023-05-12 07:42:59.038994] [Steps   45160] [G 0.0006873]
[2023-05-12 07:43:15.633643] [Steps   45170] [G 0.0011540]
[2023-05-12 07:43:32.231241] [Steps   45180] [G 0.0015512]
[2023-05-12 07:43:48.991171] [Steps   45190] [G 0.0015901]
[2023-05-12 07:44:05.581453] [Steps   45200] [G 0.0012314]
[2023-05-12 07:44:22.223273] [Steps   45210] [G 0.0010524]
[2023-05-12 07:44:38.841138] [Steps   45220] [G 0.0008041]
[2023-05-12 07:44:55.521957] [Steps   45230] [G 0.0010574]
[2023-05-12 07:45:12.120600] [Steps   45240] [G 0.0011992]
[2023-05-12 07:45:28.738188] [Steps   45250] [G 0.0011954]
[2023-05-12 07:45:45.460785] [Steps   45260] [G 0.0008488]
[2023-05-12 07:46:02.066444] [Steps   45270] [G 0.0009426]
[2023-05-12 07:46:18.690057] [Steps   45280] [G 0.0017717]
[2023-05-12 07:46:35.324074] [Steps   45290] [G 0.0012529]
[2023-05-12 07:46:51.954876] [Steps   45300] [G 0.0010466]
[2023-05-12 07:47:10.396752] [Steps   45310] [G 0.0008680]
[2023-05-12 07:47:27.033082] [Steps   45320] [G 0.0011061]
[2023-05-12 07:47:43.693048] [Steps   45330] [G 0.0007912]
[2023-05-12 07:48:00.267713] [Steps   45340] [G 0.0009676]
[2023-05-12 07:48:16.899495] [Steps   45350] [G 0.0010253]
[2023-05-12 07:48:33.516222] [Steps   45360] [G 0.0010261]
[2023-05-12 07:48:50.216573] [Steps   45370] [G 0.0009074]
[2023-05-12 07:49:06.809979] [Steps   45380] [G 0.0010028]
[2023-05-12 07:49:23.491548] [Steps   45390] [G 0.0014194]
[2023-05-12 07:49:40.193447] [Steps   45400] [G 0.0012328]
[2023-05-12 07:49:56.792069] [Steps   45410] [G 0.0009631]
[2023-05-12 07:50:13.411312] [Steps   45420] [G 0.0007782]
[2023-05-12 07:50:30.105720] [Steps   45430] [G 0.0010561]
[2023-05-12 07:50:46.704396] [Steps   45440] [G 0.0010051]
[2023-05-12 07:51:03.319174] [Steps   45450] [G 0.0007705]
[2023-05-12 07:51:19.903622] [Steps   45460] [G 0.0010940]
[2023-05-12 07:51:36.600464] [Steps   45470] [G 0.0011868]
[2023-05-12 07:51:53.225507] [Steps   45480] [G 0.0007256]
[2023-05-12 07:52:09.872710] [Steps   45490] [G 0.0014123]
[2023-05-12 07:52:26.537999] [Steps   45500] [G 0.0010527]
[2023-05-12 07:52:43.177428] [Steps   45510] [G 0.0011716]
[2023-05-12 07:52:59.769312] [Steps   45520] [G 0.0006111]
[2023-05-12 07:53:16.396612] [Steps   45530] [G 0.0015205]
[2023-05-12 07:53:33.077767] [Steps   45540] [G 0.0013820]
[2023-05-12 07:53:49.691680] [Steps   45550] [G 0.0010562]
[2023-05-12 07:54:06.325227] [Steps   45560] [G 0.0007307]
[2023-05-12 07:54:23.008141] [Steps   45570] [G 0.0011497]
[2023-05-12 07:54:39.630968] [Steps   45580] [G 0.0011787]
[2023-05-12 07:54:56.239280] [Steps   45590] [G 0.0014305]
[2023-05-12 07:55:12.904754] [Steps   45600] [G 0.0012636]
[2023-05-12 07:55:31.645893] [Steps   45610] [G 0.0007326]
[2023-05-12 07:55:48.324731] [Steps   45620] [G 0.0008991]
[2023-05-12 07:56:04.937273] [Steps   45630] [G 0.0014140]
[2023-05-12 07:56:21.609897] [Steps   45640] [G 0.0010248]
[2023-05-12 07:56:38.210075] [Steps   45650] [G 0.0007573]
[2023-05-12 07:56:54.794459] [Steps   45660] [G 0.0013195]
[2023-05-12 07:57:11.530026] [Steps   45670] [G 0.0009065]
[2023-05-12 07:57:28.119221] [Steps   45680] [G 0.0008509]
[2023-05-12 07:57:44.696983] [Steps   45690] [G 0.0016144]
[2023-05-12 07:58:01.303304] [Steps   45700] [G 0.0010914]
[2023-05-12 07:58:17.968974] [Steps   45710] [G 0.0009966]
[2023-05-12 07:58:34.566366] [Steps   45720] [G 0.0010126]
[2023-05-12 07:58:51.137734] [Steps   45730] [G 0.0017733]
[2023-05-12 07:59:07.820383] [Steps   45740] [G 0.0011659]
[2023-05-12 07:59:24.405707] [Steps   45750] [G 0.0013891]
[2023-05-12 07:59:41.006413] [Steps   45760] [G 0.0007027]
[2023-05-12 07:59:57.672281] [Steps   45770] [G 0.0008681]
[2023-05-12 08:00:14.288321] [Steps   45780] [G 0.0022921]
[2023-05-12 08:00:30.897974] [Steps   45790] [G 0.0019756]
[2023-05-12 08:00:47.485177] [Steps   45800] [G 0.0011837]
[2023-05-12 08:01:04.150913] [Steps   45810] [G 0.0008353]
[2023-05-12 08:01:20.729636] [Steps   45820] [G 0.0008884]
[2023-05-12 08:01:37.307725] [Steps   45830] [G 0.0009188]
[2023-05-12 08:01:53.956800] [Steps   45840] [G 0.0012008]
[2023-05-12 08:02:10.534865] [Steps   45850] [G 0.0008267]
[2023-05-12 08:02:27.121217] [Steps   45860] [G 0.0008586]
[2023-05-12 08:02:43.689574] [Steps   45870] [G 0.0012777]
[2023-05-12 08:03:00.354896] [Steps   45880] [G 0.0008674]
[2023-05-12 08:03:16.921587] [Steps   45890] [G 0.0009878]
[2023-05-12 08:03:33.462183] [Steps   45900] [G 0.0010181]
[2023-05-12 08:03:52.057269] [Steps   45910] [G 0.0010540]
[2023-05-12 08:04:08.645296] [Steps   45920] [G 0.0009556]
[2023-05-12 08:04:25.235415] [Steps   45930] [G 0.0011348]
[2023-05-12 08:04:41.961373] [Steps   45940] [G 0.0011972]
[2023-05-12 08:04:58.579829] [Steps   45950] [G 0.0009230]
[2023-05-12 08:05:15.183296] [Steps   45960] [G 0.0009788]
[2023-05-12 08:05:31.787971] [Steps   45970] [G 0.0006617]
[2023-05-12 08:05:48.498539] [Steps   45980] [G 0.0012294]
[2023-05-12 08:06:05.124163] [Steps   45990] [G 0.0022903]
[2023-05-12 08:06:21.751611] [Steps   46000] [G 0.0018958]
Steps 46001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 08:06:39.918181] [Steps   46010] [G 0.0011155]
[2023-05-12 08:06:56.543884] [Steps   46020] [G 0.0007714]
[2023-05-12 08:07:13.309280] [Steps   46030] [G 0.0013722]
[2023-05-12 08:07:30.074718] [Steps   46040] [G 0.0008278]
[2023-05-12 08:07:46.765070] [Steps   46050] [G 0.0010975]
[2023-05-12 08:08:03.557339] [Steps   46060] [G 0.0014103]
[2023-05-12 08:08:20.244378] [Steps   46070] [G 0.0026925]
[2023-05-12 08:08:36.919172] [Steps   46080] [G 0.0014935]
[2023-05-12 08:08:53.603177] [Steps   46090] [G 0.0012133]
[2023-05-12 08:09:10.375073] [Steps   46100] [G 0.0011516]
[2023-05-12 08:09:27.061270] [Steps   46110] [G 0.0026686]
[2023-05-12 08:09:43.759711] [Steps   46120] [G 0.0006444]
[2023-05-12 08:10:00.475997] [Steps   46130] [G 0.0011709]
[2023-05-12 08:10:17.138494] [Steps   46140] [G 0.0010276]
[2023-05-12 08:10:33.831589] [Steps   46150] [G 0.0007780]
[2023-05-12 08:10:50.459404] [Steps   46160] [G 0.0015511]
[2023-05-12 08:11:07.168220] [Steps   46170] [G 0.0013335]
[2023-05-12 08:11:23.834890] [Steps   46180] [G 0.0013827]
[2023-05-12 08:11:40.556812] [Steps   46190] [G 0.0011315]
[2023-05-12 08:11:57.278397] [Steps   46200] [G 0.0008944]
[2023-05-12 08:12:16.029748] [Steps   46210] [G 0.0011168]
[2023-05-12 08:12:32.569398] [Steps   46220] [G 0.0009996]
[2023-05-12 08:12:49.224916] [Steps   46230] [G 0.0016462]
[2023-05-12 08:13:05.804732] [Steps   46240] [G 0.0009913]
[2023-05-12 08:13:22.376826] [Steps   46250] [G 0.0009622]
[2023-05-12 08:13:38.974165] [Steps   46260] [G 0.0005771]
[2023-05-12 08:13:55.636381] [Steps   46270] [G 0.0019373]
[2023-05-12 08:14:12.221202] [Steps   46280] [G 0.0009358]
[2023-05-12 08:14:28.811490] [Steps   46290] [G 0.0007937]
[2023-05-12 08:14:45.483682] [Steps   46300] [G 0.0010773]
[2023-05-12 08:15:02.063271] [Steps   46310] [G 0.0006319]
[2023-05-12 08:15:18.644735] [Steps   46320] [G 0.0011354]
[2023-05-12 08:15:35.255736] [Steps   46330] [G 0.0008975]
[2023-05-12 08:15:51.934191] [Steps   46340] [G 0.0010741]
[2023-05-12 08:16:08.575800] [Steps   46350] [G 0.0015643]
[2023-05-12 08:16:25.285062] [Steps   46360] [G 0.0006681]
[2023-05-12 08:16:42.000295] [Steps   46370] [G 0.0007902]
[2023-05-12 08:16:58.593072] [Steps   46380] [G 0.0007475]
[2023-05-12 08:17:15.173033] [Steps   46390] [G 0.0009022]
[2023-05-12 08:17:31.846253] [Steps   46400] [G 0.0017434]
[2023-05-12 08:17:48.453447] [Steps   46410] [G 0.0012610]
[2023-05-12 08:18:05.045364] [Steps   46420] [G 0.0008676]
[2023-05-12 08:18:21.636160] [Steps   46430] [G 0.0015172]
[2023-05-12 08:18:38.294158] [Steps   46440] [G 0.0012854]
[2023-05-12 08:18:54.901142] [Steps   46450] [G 0.0007097]
[2023-05-12 08:19:11.476961] [Steps   46460] [G 0.0013763]
[2023-05-12 08:19:28.137925] [Steps   46470] [G 0.0007692]
[2023-05-12 08:19:44.729743] [Steps   46480] [G 0.0015240]
[2023-05-12 08:20:01.353521] [Steps   46490] [G 0.0010113]
[2023-05-12 08:20:17.925471] [Steps   46500] [G 0.0012196]
[2023-05-12 08:20:36.520873] [Steps   46510] [G 0.0009776]
[2023-05-12 08:20:53.138394] [Steps   46520] [G 0.0010162]
[2023-05-12 08:21:09.777744] [Steps   46530] [G 0.0023404]
[2023-05-12 08:21:26.536680] [Steps   46540] [G 0.0007610]
[2023-05-12 08:21:43.178982] [Steps   46550] [G 0.0015763]
[2023-05-12 08:21:59.832702] [Steps   46560] [G 0.0007774]
[2023-05-12 08:22:16.596958] [Steps   46570] [G 0.0006392]
[2023-05-12 08:22:33.215025] [Steps   46580] [G 0.0007476]
[2023-05-12 08:22:49.904803] [Steps   46590] [G 0.0009690]
[2023-05-12 08:23:06.575250] [Steps   46600] [G 0.0012016]
[2023-05-12 08:23:23.346507] [Steps   46610] [G 0.0009014]
[2023-05-12 08:23:39.967554] [Steps   46620] [G 0.0008519]
[2023-05-12 08:23:56.617389] [Steps   46630] [G 0.0008884]
[2023-05-12 08:24:13.335043] [Steps   46640] [G 0.0008268]
[2023-05-12 08:24:30.010349] [Steps   46650] [G 0.0012063]
[2023-05-12 08:24:46.631721] [Steps   46660] [G 0.0009925]
[2023-05-12 08:25:03.258692] [Steps   46670] [G 0.0011291]
[2023-05-12 08:25:20.024449] [Steps   46680] [G 0.0008554]
[2023-05-12 08:25:36.655731] [Steps   46690] [G 0.0008299]
[2023-05-12 08:25:53.293029] [Steps   46700] [G 0.0007918]
[2023-05-12 08:26:10.010296] [Steps   46710] [G 0.0012219]
[2023-05-12 08:26:26.699530] [Steps   46720] [G 0.0008041]
[2023-05-12 08:26:43.327643] [Steps   46730] [G 0.0008120]
[2023-05-12 08:27:00.041963] [Steps   46740] [G 0.0012385]
[2023-05-12 08:27:16.704739] [Steps   46750] [G 0.0012614]
[2023-05-12 08:27:33.344619] [Steps   46760] [G 0.0008949]
[2023-05-12 08:27:50.016175] [Steps   46770] [G 0.0009086]
[2023-05-12 08:28:06.771411] [Steps   46780] [G 0.0007603]
[2023-05-12 08:28:23.427884] [Steps   46790] [G 0.0025521]
[2023-05-12 08:28:40.082872] [Steps   46800] [G 0.0012621]
[2023-05-12 08:28:56.742336] [Steps   46810] [G 0.0009849]
[2023-05-12 08:29:15.463542] [Steps   46820] [G 0.0014456]
[2023-05-12 08:29:32.167103] [Steps   46830] [G 0.0010986]
[2023-05-12 08:29:48.808222] [Steps   46840] [G 0.0011437]
[2023-05-12 08:30:05.543148] [Steps   46850] [G 0.0013792]
[2023-05-12 08:30:22.231836] [Steps   46860] [G 0.0006624]
[2023-05-12 08:30:38.918594] [Steps   46870] [G 0.0010465]
[2023-05-12 08:30:55.690939] [Steps   46880] [G 0.0013230]
[2023-05-12 08:31:12.410852] [Steps   46890] [G 0.0007581]
[2023-05-12 08:31:29.111317] [Steps   46900] [G 0.0011170]
[2023-05-12 08:31:45.823107] [Steps   46910] [G 0.0007708]
[2023-05-12 08:32:02.626942] [Steps   46920] [G 0.0010188]
[2023-05-12 08:32:19.343834] [Steps   46930] [G 0.0009424]
[2023-05-12 08:32:36.028675] [Steps   46940] [G 0.0013497]
[2023-05-12 08:32:52.762186] [Steps   46950] [G 0.0015737]
[2023-05-12 08:33:09.471619] [Steps   46960] [G 0.0008319]
[2023-05-12 08:33:26.153979] [Steps   46970] [G 0.0012613]
[2023-05-12 08:33:42.891453] [Steps   46980] [G 0.0012232]
[2023-05-12 08:33:59.566570] [Steps   46990] [G 0.0012214]
[2023-05-12 08:34:16.304418] [Steps   47000] [G 0.0010357]
Steps 47001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 08:34:34.397312] [Steps   47010] [G 0.0007815]
[2023-05-12 08:34:51.168215] [Steps   47020] [G 0.0007702]
[2023-05-12 08:35:07.844527] [Steps   47030] [G 0.0009222]
[2023-05-12 08:35:24.592840] [Steps   47040] [G 0.0010044]
[2023-05-12 08:35:41.328291] [Steps   47050] [G 0.0009040]
[2023-05-12 08:35:58.025954] [Steps   47060] [G 0.0014599]
[2023-05-12 08:36:14.777752] [Steps   47070] [G 0.0010738]
[2023-05-12 08:36:31.469289] [Steps   47080] [G 0.0008247]
[2023-05-12 08:36:48.240778] [Steps   47090] [G 0.0010644]
[2023-05-12 08:37:04.998816] [Steps   47100] [G 0.0012932]
[2023-05-12 08:37:21.622451] [Steps   47110] [G 0.0008793]
[2023-05-12 08:37:40.295160] [Steps   47120] [G 0.0014851]
[2023-05-12 08:37:56.927625] [Steps   47130] [G 0.0008414]
[2023-05-12 08:38:13.700518] [Steps   47140] [G 0.0008639]
[2023-05-12 08:38:30.399219] [Steps   47150] [G 0.0009497]
[2023-05-12 08:38:47.118049] [Steps   47160] [G 0.0008534]
[2023-05-12 08:39:03.876487] [Steps   47170] [G 0.0006870]
[2023-05-12 08:39:20.547574] [Steps   47180] [G 0.0007719]
[2023-05-12 08:39:37.263183] [Steps   47190] [G 0.0009915]
[2023-05-12 08:39:53.881097] [Steps   47200] [G 0.0009554]
[2023-05-12 08:40:10.592392] [Steps   47210] [G 0.0011987]
[2023-05-12 08:40:27.293882] [Steps   47220] [G 0.0007723]
[2023-05-12 08:40:44.005526] [Steps   47230] [G 0.0009554]
[2023-05-12 08:41:00.773910] [Steps   47240] [G 0.0009905]
[2023-05-12 08:41:17.428875] [Steps   47250] [G 0.0009539]
[2023-05-12 08:41:34.128533] [Steps   47260] [G 0.0007700]
[2023-05-12 08:41:50.877308] [Steps   47270] [G 0.0008546]
[2023-05-12 08:42:07.578584] [Steps   47280] [G 0.0016255]
[2023-05-12 08:42:24.269597] [Steps   47290] [G 0.0006657]
[2023-05-12 08:42:40.934578] [Steps   47300] [G 0.0008163]
[2023-05-12 08:42:57.744883] [Steps   47310] [G 0.0007578]
[2023-05-12 08:43:14.423742] [Steps   47320] [G 0.0015979]
[2023-05-12 08:43:31.118561] [Steps   47330] [G 0.0013894]
[2023-05-12 08:43:47.830918] [Steps   47340] [G 0.0008976]
[2023-05-12 08:44:04.537589] [Steps   47350] [G 0.0004976]
[2023-05-12 08:44:21.197708] [Steps   47360] [G 0.0011051]
[2023-05-12 08:44:37.872450] [Steps   47370] [G 0.0010357]
[2023-05-12 08:44:54.629289] [Steps   47380] [G 0.0023926]
[2023-05-12 08:45:11.340416] [Steps   47390] [G 0.0010115]
[2023-05-12 08:45:28.020974] [Steps   47400] [G 0.0010872]
[2023-05-12 08:45:44.701630] [Steps   47410] [G 0.0013273]
[2023-05-12 08:46:03.283508] [Steps   47420] [G 0.0008644]
[2023-05-12 08:46:19.901886] [Steps   47430] [G 0.0007216]
[2023-05-12 08:46:36.616087] [Steps   47440] [G 0.0013216]
[2023-05-12 08:46:53.247207] [Steps   47450] [G 0.0006927]
[2023-05-12 08:47:09.853860] [Steps   47460] [G 0.0008027]
[2023-05-12 08:47:26.539582] [Steps   47470] [G 0.0007460]
[2023-05-12 08:47:43.235760] [Steps   47480] [G 0.0005611]
[2023-05-12 08:47:59.890296] [Steps   47490] [G 0.0008003]
[2023-05-12 08:48:16.552234] [Steps   47500] [G 0.0006271]
[2023-05-12 08:48:33.361397] [Steps   47510] [G 0.0006312]
[2023-05-12 08:48:50.034727] [Steps   47520] [G 0.0010260]
[2023-05-12 08:49:06.682745] [Steps   47530] [G 0.0015932]
[2023-05-12 08:49:23.328210] [Steps   47540] [G 0.0008278]
[2023-05-12 08:49:40.069216] [Steps   47550] [G 0.0015454]
[2023-05-12 08:49:56.759775] [Steps   47560] [G 0.0008059]
[2023-05-12 08:50:13.398422] [Steps   47570] [G 0.0009074]
[2023-05-12 08:50:30.108742] [Steps   47580] [G 0.0009826]
[2023-05-12 08:50:46.769010] [Steps   47590] [G 0.0014144]
[2023-05-12 08:51:03.441729] [Steps   47600] [G 0.0014389]
[2023-05-12 08:51:20.146052] [Steps   47610] [G 0.0010023]
[2023-05-12 08:51:36.778137] [Steps   47620] [G 0.0012539]
[2023-05-12 08:51:53.459279] [Steps   47630] [G 0.0007616]
[2023-05-12 08:52:10.112999] [Steps   47640] [G 0.0011617]
[2023-05-12 08:52:26.838272] [Steps   47650] [G 0.0012175]
[2023-05-12 08:52:43.477249] [Steps   47660] [G 0.0014261]
[2023-05-12 08:53:00.169232] [Steps   47670] [G 0.0006845]
[2023-05-12 08:53:16.958262] [Steps   47680] [G 0.0008888]
[2023-05-12 08:53:33.618108] [Steps   47690] [G 0.0011611]
[2023-05-12 08:53:50.270874] [Steps   47700] [G 0.0006703]
[2023-05-12 08:54:06.879918] [Steps   47710] [G 0.0010587]
[2023-05-12 08:54:25.540797] [Steps   47720] [G 0.0008888]
[2023-05-12 08:54:42.171353] [Steps   47730] [G 0.0007358]
[2023-05-12 08:54:58.846355] [Steps   47740] [G 0.0007621]
[2023-05-12 08:55:15.558307] [Steps   47750] [G 0.0010945]
[2023-05-12 08:55:32.211708] [Steps   47760] [G 0.0011029]
[2023-05-12 08:55:48.868479] [Steps   47770] [G 0.0008282]
[2023-05-12 08:56:05.615760] [Steps   47780] [G 0.0009643]
[2023-05-12 08:56:22.250020] [Steps   47790] [G 0.0015482]
[2023-05-12 08:56:38.937441] [Steps   47800] [G 0.0015606]
[2023-05-12 08:56:55.602680] [Steps   47810] [G 0.0006549]
[2023-05-12 08:57:12.375750] [Steps   47820] [G 0.0007880]
[2023-05-12 08:57:29.045459] [Steps   47830] [G 0.0015624]
[2023-05-12 08:57:45.758968] [Steps   47840] [G 0.0006558]
[2023-05-12 08:58:02.517683] [Steps   47850] [G 0.0008429]
[2023-05-12 08:58:19.210256] [Steps   47860] [G 0.0010650]
[2023-05-12 08:58:35.894333] [Steps   47870] [G 0.0008624]
[2023-05-12 08:58:52.615201] [Steps   47880] [G 0.0008449]
[2023-05-12 08:59:09.446043] [Steps   47890] [G 0.0011103]
[2023-05-12 08:59:26.158786] [Steps   47900] [G 0.0012564]
[2023-05-12 08:59:42.867271] [Steps   47910] [G 0.0006510]
[2023-05-12 08:59:59.567916] [Steps   47920] [G 0.0006975]
[2023-05-12 09:00:16.183132] [Steps   47930] [G 0.0007312]
[2023-05-12 09:00:32.808809] [Steps   47940] [G 0.0006285]
[2023-05-12 09:00:49.522595] [Steps   47950] [G 0.0007111]
[2023-05-12 09:01:06.193962] [Steps   47960] [G 0.0022049]
[2023-05-12 09:01:22.839556] [Steps   47970] [G 0.0011139]
[2023-05-12 09:01:39.544537] [Steps   47980] [G 0.0010737]
[2023-05-12 09:01:56.320053] [Steps   47990] [G 0.0008928]
[2023-05-12 09:02:13.007178] [Steps   48000] [G 0.0011133]
Steps 48001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 09:02:30.987096] [Steps   48010] [G 0.0005323]
[2023-05-12 09:02:49.573193] [Steps   48020] [G 0.0011012]
[2023-05-12 09:03:06.240460] [Steps   48030] [G 0.0008573]
[2023-05-12 09:03:22.943882] [Steps   48040] [G 0.0007077]
[2023-05-12 09:03:39.619740] [Steps   48050] [G 0.0011090]
[2023-05-12 09:03:56.319343] [Steps   48060] [G 0.0008943]
[2023-05-12 09:04:13.080167] [Steps   48070] [G 0.0009665]
[2023-05-12 09:04:29.799907] [Steps   48080] [G 0.0008103]
[2023-05-12 09:04:46.503241] [Steps   48090] [G 0.0010257]
[2023-05-12 09:05:03.224205] [Steps   48100] [G 0.0017873]
[2023-05-12 09:05:20.003530] [Steps   48110] [G 0.0015773]
[2023-05-12 09:05:36.730062] [Steps   48120] [G 0.0008360]
[2023-05-12 09:05:53.439350] [Steps   48130] [G 0.0009131]
[2023-05-12 09:06:10.183770] [Steps   48140] [G 0.0004983]
[2023-05-12 09:06:26.845607] [Steps   48150] [G 0.0011898]
[2023-05-12 09:06:43.456574] [Steps   48160] [G 0.0005486]
[2023-05-12 09:07:00.202978] [Steps   48170] [G 0.0006908]
[2023-05-12 09:07:16.888730] [Steps   48180] [G 0.0006896]
[2023-05-12 09:07:33.587434] [Steps   48190] [G 0.0007322]
[2023-05-12 09:07:50.300079] [Steps   48200] [G 0.0009667]
[2023-05-12 09:08:07.045183] [Steps   48210] [G 0.0010798]
[2023-05-12 09:08:23.744260] [Steps   48220] [G 0.0016081]
[2023-05-12 09:08:40.442713] [Steps   48230] [G 0.0008849]
[2023-05-12 09:08:57.197282] [Steps   48240] [G 0.0013106]
[2023-05-12 09:09:13.883228] [Steps   48250] [G 0.0014595]
[2023-05-12 09:09:30.597341] [Steps   48260] [G 0.0011559]
[2023-05-12 09:09:47.289889] [Steps   48270] [G 0.0011780]
[2023-05-12 09:10:04.054935] [Steps   48280] [G 0.0010022]
[2023-05-12 09:10:20.775662] [Steps   48290] [G 0.0007121]
[2023-05-12 09:10:37.459633] [Steps   48300] [G 0.0007223]
[2023-05-12 09:10:54.200827] [Steps   48310] [G 0.0010858]
[2023-05-12 09:11:10.808581] [Steps   48320] [G 0.0010957]
[2023-05-12 09:11:29.604326] [Steps   48330] [G 0.0020302]
[2023-05-12 09:11:46.228915] [Steps   48340] [G 0.0008888]
[2023-05-12 09:12:02.905767] [Steps   48350] [G 0.0006058]
[2023-05-12 09:12:19.590113] [Steps   48360] [G 0.0005020]
[2023-05-12 09:12:36.258600] [Steps   48370] [G 0.0007400]
[2023-05-12 09:12:53.000697] [Steps   48380] [G 0.0008059]
[2023-05-12 09:13:09.697164] [Steps   48390] [G 0.0005434]
[2023-05-12 09:13:26.391924] [Steps   48400] [G 0.0006496]
[2023-05-12 09:13:43.177054] [Steps   48410] [G 0.0007255]
[2023-05-12 09:13:59.827797] [Steps   48420] [G 0.0008945]
[2023-05-12 09:14:16.521611] [Steps   48430] [G 0.0008269]
[2023-05-12 09:14:33.172717] [Steps   48440] [G 0.0007699]
[2023-05-12 09:14:49.899157] [Steps   48450] [G 0.0008581]
[2023-05-12 09:15:06.571883] [Steps   48460] [G 0.0010413]
[2023-05-12 09:15:23.273738] [Steps   48470] [G 0.0012139]
[2023-05-12 09:15:39.976646] [Steps   48480] [G 0.0009222]
[2023-05-12 09:15:56.676061] [Steps   48490] [G 0.0008439]
[2023-05-12 09:16:13.338492] [Steps   48500] [G 0.0006142]
[2023-05-12 09:16:30.011204] [Steps   48510] [G 0.0012966]
[2023-05-12 09:16:46.765217] [Steps   48520] [G 0.0008303]
[2023-05-12 09:17:03.450987] [Steps   48530] [G 0.0009934]
[2023-05-12 09:17:20.146254] [Steps   48540] [G 0.0009247]
[2023-05-12 09:17:36.885157] [Steps   48550] [G 0.0006367]
[2023-05-12 09:17:53.571035] [Steps   48560] [G 0.0005858]
[2023-05-12 09:18:10.267722] [Steps   48570] [G 0.0009335]
[2023-05-12 09:18:27.028199] [Steps   48580] [G 0.0011965]
[2023-05-12 09:18:43.740203] [Steps   48590] [G 0.0010246]
[2023-05-12 09:19:00.409804] [Steps   48600] [G 0.0011128]
[2023-05-12 09:19:17.105903] [Steps   48610] [G 0.0006700]
[2023-05-12 09:19:33.812929] [Steps   48620] [G 0.0007162]
[2023-05-12 09:19:52.411096] [Steps   48630] [G 0.0012968]
[2023-05-12 09:20:08.999961] [Steps   48640] [G 0.0013944]
[2023-05-12 09:20:25.634788] [Steps   48650] [G 0.0007465]
[2023-05-12 09:20:42.204650] [Steps   48660] [G 0.0009636]
[2023-05-12 09:20:58.771199] [Steps   48670] [G 0.0009573]
[2023-05-12 09:21:15.355299] [Steps   48680] [G 0.0005885]
[2023-05-12 09:21:32.001671] [Steps   48690] [G 0.0019121]
[2023-05-12 09:21:48.593234] [Steps   48700] [G 0.0017423]
[2023-05-12 09:22:05.214853] [Steps   48710] [G 0.0016041]
[2023-05-12 09:22:21.877113] [Steps   48720] [G 0.0014272]
[2023-05-12 09:22:38.481978] [Steps   48730] [G 0.0006123]
[2023-05-12 09:22:55.064113] [Steps   48740] [G 0.0007838]
[2023-05-12 09:23:11.750117] [Steps   48750] [G 0.0008290]
[2023-05-12 09:23:28.343668] [Steps   48760] [G 0.0006644]
[2023-05-12 09:23:44.942708] [Steps   48770] [G 0.0011101]
[2023-05-12 09:24:01.550066] [Steps   48780] [G 0.0008117]
[2023-05-12 09:24:18.236568] [Steps   48790] [G 0.0009993]
[2023-05-12 09:24:34.839972] [Steps   48800] [G 0.0007733]
[2023-05-12 09:24:51.438659] [Steps   48810] [G 0.0007641]
[2023-05-12 09:25:08.119510] [Steps   48820] [G 0.0011903]
[2023-05-12 09:25:24.712369] [Steps   48830] [G 0.0008685]
[2023-05-12 09:25:41.296931] [Steps   48840] [G 0.0015190]
[2023-05-12 09:25:57.899048] [Steps   48850] [G 0.0007872]
[2023-05-12 09:26:14.564240] [Steps   48860] [G 0.0011252]
[2023-05-12 09:26:31.164604] [Steps   48870] [G 0.0007088]
[2023-05-12 09:26:47.752559] [Steps   48880] [G 0.0006866]
[2023-05-12 09:27:04.422318] [Steps   48890] [G 0.0005813]
[2023-05-12 09:27:21.021262] [Steps   48900] [G 0.0011798]
[2023-05-12 09:27:37.604188] [Steps   48910] [G 0.0013170]
[2023-05-12 09:27:54.254672] [Steps   48920] [G 0.0012296]
[2023-05-12 09:28:12.849620] [Steps   48930] [G 0.0010651]
[2023-05-12 09:28:29.491278] [Steps   48940] [G 0.0011567]
[2023-05-12 09:28:46.105715] [Steps   48950] [G 0.0007956]
[2023-05-12 09:29:02.903742] [Steps   48960] [G 0.0009628]
[2023-05-12 09:29:19.564151] [Steps   48970] [G 0.0007415]
[2023-05-12 09:29:36.239431] [Steps   48980] [G 0.0006343]
[2023-05-12 09:29:53.008195] [Steps   48990] [G 0.0006405]
[2023-05-12 09:30:09.710241] [Steps   49000] [G 0.0006264]
Steps 49001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 09:30:27.829644] [Steps   49010] [G 0.0006800]
[2023-05-12 09:30:44.516254] [Steps   49020] [G 0.0012244]
[2023-05-12 09:31:01.181804] [Steps   49030] [G 0.0009475]
[2023-05-12 09:31:17.925018] [Steps   49040] [G 0.0010651]
[2023-05-12 09:31:34.631006] [Steps   49050] [G 0.0010092]
[2023-05-12 09:31:51.305650] [Steps   49060] [G 0.0010525]
[2023-05-12 09:32:07.994181] [Steps   49070] [G 0.0010837]
[2023-05-12 09:32:24.735019] [Steps   49080] [G 0.0006385]
[2023-05-12 09:32:41.420909] [Steps   49090] [G 0.0012707]
[2023-05-12 09:32:58.103977] [Steps   49100] [G 0.0007082]
[2023-05-12 09:33:14.842195] [Steps   49110] [G 0.0018849]
[2023-05-12 09:33:31.533608] [Steps   49120] [G 0.0010255]
[2023-05-12 09:33:48.231763] [Steps   49130] [G 0.0005590]
[2023-05-12 09:34:04.983149] [Steps   49140] [G 0.0014445]
[2023-05-12 09:34:21.644857] [Steps   49150] [G 0.0006223]
[2023-05-12 09:34:38.298400] [Steps   49160] [G 0.0010593]
[2023-05-12 09:34:54.980342] [Steps   49170] [G 0.0018607]
[2023-05-12 09:35:11.733585] [Steps   49180] [G 0.0014960]
[2023-05-12 09:35:28.388016] [Steps   49190] [G 0.0016703]
[2023-05-12 09:35:45.075222] [Steps   49200] [G 0.0007353]
[2023-05-12 09:36:01.830884] [Steps   49210] [G 0.0006539]
[2023-05-12 09:36:18.454554] [Steps   49220] [G 0.0009137]
[2023-05-12 09:36:37.152653] [Steps   49230] [G 0.0012016]
[2023-05-12 09:36:53.823857] [Steps   49240] [G 0.0009819]
[2023-05-12 09:37:10.556457] [Steps   49250] [G 0.0013132]
[2023-05-12 09:37:27.223618] [Steps   49260] [G 0.0013590]
[2023-05-12 09:37:43.915115] [Steps   49270] [G 0.0020653]
[2023-05-12 09:38:00.624557] [Steps   49280] [G 0.0008225]
[2023-05-12 09:38:17.325128] [Steps   49290] [G 0.0009279]
[2023-05-12 09:38:33.987425] [Steps   49300] [G 0.0009922]
[2023-05-12 09:38:50.740921] [Steps   49310] [G 0.0007774]
[2023-05-12 09:39:07.421676] [Steps   49320] [G 0.0011264]
[2023-05-12 09:39:24.102319] [Steps   49330] [G 0.0011900]
[2023-05-12 09:39:40.783815] [Steps   49340] [G 0.0012142]
[2023-05-12 09:39:57.536743] [Steps   49350] [G 0.0007920]
[2023-05-12 09:40:14.210101] [Steps   49360] [G 0.0005190]
[2023-05-12 09:40:30.875918] [Steps   49370] [G 0.0008794]
[2023-05-12 09:40:47.641019] [Steps   49380] [G 0.0009129]
[2023-05-12 09:41:04.309812] [Steps   49390] [G 0.0020393]
[2023-05-12 09:41:20.958692] [Steps   49400] [G 0.0011127]
[2023-05-12 09:41:37.643242] [Steps   49410] [G 0.0017893]
[2023-05-12 09:41:54.409827] [Steps   49420] [G 0.0019891]
[2023-05-12 09:42:11.113633] [Steps   49430] [G 0.0010377]
[2023-05-12 09:42:27.810185] [Steps   49440] [G 0.0004853]
[2023-05-12 09:42:44.621456] [Steps   49450] [G 0.0020121]
[2023-05-12 09:43:01.281382] [Steps   49460] [G 0.0007827]
[2023-05-12 09:43:17.967788] [Steps   49470] [G 0.0010639]
[2023-05-12 09:43:34.735276] [Steps   49480] [G 0.0008650]
[2023-05-12 09:43:51.407923] [Steps   49490] [G 0.0010231]
[2023-05-12 09:44:08.084278] [Steps   49500] [G 0.0012014]
[2023-05-12 09:44:24.763157] [Steps   49510] [G 0.0009096]
[2023-05-12 09:44:41.481426] [Steps   49520] [G 0.0008569]
[2023-05-12 09:45:00.085278] [Steps   49530] [G 0.0011576]
[2023-05-12 09:45:16.620993] [Steps   49540] [G 0.0008637]
[2023-05-12 09:45:33.325227] [Steps   49550] [G 0.0010421]
[2023-05-12 09:45:49.882716] [Steps   49560] [G 0.0014811]
[2023-05-12 09:46:06.449869] [Steps   49570] [G 0.0007332]
[2023-05-12 09:46:23.018444] [Steps   49580] [G 0.0007804]
[2023-05-12 09:46:39.712831] [Steps   49590] [G 0.0005610]
[2023-05-12 09:46:56.298695] [Steps   49600] [G 0.0007820]
[2023-05-12 09:47:12.880394] [Steps   49610] [G 0.0008649]
[2023-05-12 09:47:29.544187] [Steps   49620] [G 0.0011909]
[2023-05-12 09:47:46.122757] [Steps   49630] [G 0.0008691]
[2023-05-12 09:48:02.693719] [Steps   49640] [G 0.0011378]
[2023-05-12 09:48:19.339078] [Steps   49650] [G 0.0008035]
[2023-05-12 09:48:35.897631] [Steps   49660] [G 0.0010607]
[2023-05-12 09:48:52.473813] [Steps   49670] [G 0.0011095]
[2023-05-12 09:49:09.028577] [Steps   49680] [G 0.0008187]
[2023-05-12 09:49:25.702965] [Steps   49690] [G 0.0006958]
[2023-05-12 09:49:42.278797] [Steps   49700] [G 0.0006275]
[2023-05-12 09:49:58.934598] [Steps   49710] [G 0.0011158]
[2023-05-12 09:50:15.683662] [Steps   49720] [G 0.0009350]
[2023-05-12 09:50:32.354275] [Steps   49730] [G 0.0008651]
[2023-05-12 09:50:48.954810] [Steps   49740] [G 0.0008889]
[2023-05-12 09:51:05.544979] [Steps   49750] [G 0.0008416]
[2023-05-12 09:51:22.219421] [Steps   49760] [G 0.0008214]
[2023-05-12 09:51:38.792542] [Steps   49770] [G 0.0008189]
[2023-05-12 09:51:55.356273] [Steps   49780] [G 0.0012742]
[2023-05-12 09:52:12.005453] [Steps   49790] [G 0.0009820]
[2023-05-12 09:52:28.605022] [Steps   49800] [G 0.0016898]
[2023-05-12 09:52:45.218343] [Steps   49810] [G 0.0006429]
[2023-05-12 09:53:01.977761] [Steps   49820] [G 0.0008317]
[2023-05-12 09:53:18.549060] [Steps   49830] [G 0.0018484]
[2023-05-12 09:53:37.369584] [Steps   49840] [G 0.0012195]
[2023-05-12 09:53:53.984423] [Steps   49850] [G 0.0011761]
[2023-05-12 09:54:10.702864] [Steps   49860] [G 0.0007184]
[2023-05-12 09:54:27.385495] [Steps   49870] [G 0.0007799]
[2023-05-12 09:54:44.055129] [Steps   49880] [G 0.0008960]
[2023-05-12 09:55:00.807734] [Steps   49890] [G 0.0009241]
[2023-05-12 09:55:17.527455] [Steps   49900] [G 0.0009394]
[2023-05-12 09:55:34.244761] [Steps   49910] [G 0.0008120]
[2023-05-12 09:55:50.939231] [Steps   49920] [G 0.0007817]
[2023-05-12 09:56:07.677120] [Steps   49930] [G 0.0010035]
[2023-05-12 09:56:24.365399] [Steps   49940] [G 0.0010262]
[2023-05-12 09:56:41.070831] [Steps   49950] [G 0.0007107]
[2023-05-12 09:56:57.813658] [Steps   49960] [G 0.0008794]
[2023-05-12 09:57:14.515144] [Steps   49970] [G 0.0006886]
[2023-05-12 09:57:31.214701] [Steps   49980] [G 0.0014502]
[2023-05-12 09:57:47.909585] [Steps   49990] [G 0.0009652]
[2023-05-12 09:58:04.674979] [Steps   50000] [G 0.0007996]
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9328)
[2023-05-12 09:58:23.900222] [Steps   50010] [G 0.0010421]
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f92c78abb50>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-14 20:57:27.146791] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-14 20:57:27.148328] # Params - G: 43378727
[2023-05-14 20:57:27.148357] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-14 20:57:27.148383] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/4193)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fbc0e4d8ad0>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-14 21:01:09.844899] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-14 21:01:09.846370] # Params - G: 43378727
[2023-05-14 21:01:09.846397] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-14 21:01:09.846420] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9370)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-14 21:30:04.053024] [Steps    1000] [G 0.0156817]
Traceback (most recent call last):
  File "train2_vitGenerator.py", line 379, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 369, in worker
    pair_loader=pair_loader,val_pair_loader=val_pair_loader,logger=logger)
  File "train2_vitGenerator.py", line 229, in train
    val_gen_images = _sample_generator(generator, val_images.size(0),enable_grad=True,imgs=val_images,illus=val_illus)
  File "train2_vitGenerator.py", line 117, in _sample_generator
    generated_data = G(x=imgs,input=latent_samples,illu=illus)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/vit_generator_skip.py", line 571, in forward
    x = conv_layer(x, latent)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/stylegan2/generator.py", line 135, in forward
    out = self.activate(out)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/stylegan2/op/fused_act.py", line 112, in forward
    return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)
  File "/home/xsx/dino/stylegan2/op/fused_act.py", line 117, in fused_leaky_relu
    negative_slope=negative_slope)
RuntimeError: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 11.77 GiB total capacity; 9.67 GiB already allocated; 209.50 MiB free; 10.40 GiB reserved in total by PyTorch)

get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f4c607da390>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 08:18:56.029491] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 08:18:56.030925] # Params - G: 43378727
[2023-05-15 08:18:56.030951] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 08:18:56.030972] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/8390)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 08:47:19.225762] [Steps    1000] [G 0.0155425]
Traceback (most recent call last):
  File "train2_vitGenerator.py", line 380, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 370, in worker
    pair_loader=pair_loader,val_pair_loader=val_pair_loader,logger=logger)
  File "train2_vitGenerator.py", line 230, in train
    val_gen_images = _sample_generator(generator, val_images.size(0),enable_grad=True,imgs=val_images,illus=val_illus)
  File "train2_vitGenerator.py", line 117, in _sample_generator
    generated_data = G(x=imgs,input=latent_samples,illu=illus)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/vit_generator_skip.py", line 571, in forward
    x = conv_layer(x, latent)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/stylegan2/generator.py", line 135, in forward
    out = self.activate(out)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/stylegan2/op/fused_act.py", line 112, in forward
    return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)
  File "/home/xsx/dino/stylegan2/op/fused_act.py", line 117, in fused_leaky_relu
    negative_slope=negative_slope)
RuntimeError: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 11.77 GiB total capacity; 9.67 GiB already allocated; 151.50 MiB free; 10.46 GiB reserved in total by PyTorch)

get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f557af81c10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 09:06:47.580171] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 09:06:47.581596] # Params - G: 43378727
[2023-05-15 09:06:47.581620] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 09:06:47.581641] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/3936)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f02af7f9b10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 09:09:23.980974] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 09:09:23.982388] # Params - G: 43378727
[2023-05-15 09:09:23.982413] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 09:09:23.982434] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2850)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 09:12:24.160508] [Steps     100] [G 0.0332887]
Steps 101 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2850)
Traceback (most recent call last):
  File "train2_vitGenerator.py", line 380, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 370, in worker
    pair_loader=pair_loader,val_pair_loader=val_pair_loader,logger=logger)
  File "train2_vitGenerator.py", line 199, in train
    gen_images = _sample_generator(generator, images.size(0),enable_grad=True,imgs=images,illus=illus)
  File "train2_vitGenerator.py", line 117, in _sample_generator
    generated_data = G(x=imgs,input=latent_samples,illu=illus)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/vit_generator_skip.py", line 571, in forward
    x = conv_layer(x, latent)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/stylegan2/generator.py", line 135, in forward
    out = self.activate(out)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/stylegan2/op/fused_act.py", line 112, in forward
    return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)
  File "/home/xsx/dino/stylegan2/op/fused_act.py", line 117, in fused_leaky_relu
    negative_slope=negative_slope)
RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.77 GiB total capacity; 10.14 GiB already allocated; 155.50 MiB free; 10.45 GiB reserved in total by PyTorch)

get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f20a457fb10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 09:22:16.678053] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 09:22:16.679497] # Params - G: 43378727
[2023-05-15 09:22:16.679523] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 09:22:16.679545] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/4750)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 09:22:45.537830] [Steps      10] [G 0.0489078]
Steps 11 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/4750)
Traceback (most recent call last):
  File "train2_vitGenerator.py", line 380, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 370, in worker
    pair_loader=pair_loader,val_pair_loader=val_pair_loader,logger=logger)
  File "train2_vitGenerator.py", line 199, in train
    gen_images = _sample_generator(generator, images.size(0),enable_grad=True,imgs=images,illus=illus)
  File "train2_vitGenerator.py", line 117, in _sample_generator
    generated_data = G(x=imgs,input=latent_samples,illu=illus)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/vit_generator_skip.py", line 571, in forward
    x = conv_layer(x, latent)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/stylegan2/generator.py", line 135, in forward
    out = self.activate(out)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/stylegan2/op/fused_act.py", line 112, in forward
    return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)
  File "/home/xsx/dino/stylegan2/op/fused_act.py", line 117, in fused_leaky_relu
    negative_slope=negative_slope)
RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.77 GiB total capacity; 10.14 GiB already allocated; 155.50 MiB free; 10.45 GiB reserved in total by PyTorch)

get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fd22c99ba10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 09:27:27.972662] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 09:27:27.974130] # Params - G: 43378727
[2023-05-15 09:27:27.974156] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 09:27:27.974180] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/8196)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 09:27:56.619661] [Steps      10] [G 0.0489228]
Steps 11 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/8196)
[2023-05-15 09:28:15.023055] [Steps      20] [G 0.0566299]
Steps 21 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/8196)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fe784582b50>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 09:29:25.949224] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 09:29:25.950809] # Params - G: 43378727
[2023-05-15 09:29:25.950844] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 09:29:25.950875] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/7544)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f4f4bdfdc50>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 09:31:36.174231] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 09:31:36.175692] # Params - G: 43378727
[2023-05-15 09:31:36.175717] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 09:31:36.175740] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2975)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 09:32:06.053617] [Steps      10] [G 0.0489253]
Steps 11 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2975)
[2023-05-15 09:32:24.949450] [Steps      20] [G 0.0566584]
Steps 21 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2975)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f62abd67d90>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 09:36:53.804101] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 09:36:53.805529] # Params - G: 43378727
[2023-05-15 09:36:53.805553] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 09:36:53.805578] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 09:42:40.834997] [Steps     200] [G 0.0249914]
Steps 201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 09:48:21.821155] [Steps     400] [G 0.0180869]
Steps 401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 09:54:02.662006] [Steps     600] [G 0.0142154]
Steps 601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 09:59:46.238425] [Steps     800] [G 0.0174644]
Steps 801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:05:29.297818] [Steps    1000] [G 0.0127881]
Steps 1001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:11:11.449869] [Steps    1200] [G 0.0118980]
Steps 1201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:16:53.853130] [Steps    1400] [G 0.0144174]
Steps 1401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:22:36.164484] [Steps    1600] [G 0.0121865]
Steps 1601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:28:17.964550] [Steps    1800] [G 0.0099802]
Steps 1801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:34:01.002510] [Steps    2000] [G 0.0097984]
Steps 2001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:39:44.195216] [Steps    2200] [G 0.0113687]
Steps 2201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:45:25.093749] [Steps    2400] [G 0.0096006]
Steps 2401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:51:08.694422] [Steps    2600] [G 0.0145034]
Steps 2601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 10:56:51.138311] [Steps    2800] [G 0.0097205]
Steps 2801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
[2023-05-15 11:02:31.886516] [Steps    3000] [G 0.0103918]
Steps 3001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/2836)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f91cff45c50>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 11:04:40.342508] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 11:04:40.343939] # Params - G: 43378727
[2023-05-15 11:04:40.343964] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 11:04:40.343988] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 11:10:31.912561] [Steps     200] [G 0.0246325]
Steps 201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 11:16:16.338084] [Steps     400] [G 0.0221881]
Steps 401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 11:21:59.221203] [Steps     600] [G 0.0157210]
Steps 601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 11:27:45.089389] [Steps     800] [G 0.0170627]
Steps 801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 11:33:29.505758] [Steps    1000] [G 0.0131187]
Steps 1001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 11:39:12.952095] [Steps    1200] [G 0.0118888]
Steps 1201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 11:44:58.014748] [Steps    1400] [G 0.0140611]
Steps 1401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 11:50:43.464804] [Steps    1600] [G 0.0123841]
Steps 1601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 11:56:26.889862] [Steps    1800] [G 0.0102450]
Steps 1801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:02:11.988071] [Steps    2000] [G 0.0100805]
Steps 2001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:07:57.044425] [Steps    2200] [G 0.0113334]
Steps 2201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:13:40.077935] [Steps    2400] [G 0.0097822]
Steps 2401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:19:25.246014] [Steps    2600] [G 0.0144112]
Steps 2601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:25:11.025501] [Steps    2800] [G 0.0098833]
Steps 2801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:30:54.635803] [Steps    3000] [G 0.0103785]
Steps 3001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:36:40.934601] [Steps    3200] [G 0.0085187]
Steps 3201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:42:26.602633] [Steps    3400] [G 0.0078665]
Steps 3401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:48:10.536078] [Steps    3600] [G 0.0095455]
Steps 3601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:53:55.484460] [Steps    3800] [G 0.0076107]
Steps 3801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 12:59:40.258007] [Steps    4000] [G 0.0077339]
Steps 4001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:05:24.544745] [Steps    4200] [G 0.0077792]
Steps 4201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:11:10.367035] [Steps    4400] [G 0.0095769]
Steps 4401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:16:56.336495] [Steps    4600] [G 0.0067863]
Steps 4601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:22:40.665073] [Steps    4800] [G 0.0063832]
Steps 4801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:28:26.742100] [Steps    5000] [G 0.0079511]
Steps 5001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:34:11.720099] [Steps    5200] [G 0.0069525]
Steps 5201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:39:55.104263] [Steps    5400] [G 0.0081607]
Steps 5401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:45:40.337249] [Steps    5600] [G 0.0078800]
Steps 5601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:51:25.536556] [Steps    5800] [G 0.0062685]
Steps 5801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 13:57:09.042237] [Steps    6000] [G 0.0067254]
Steps 6001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:02:53.548034] [Steps    6200] [G 0.0112158]
Steps 6201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:08:38.154909] [Steps    6400] [G 0.0072622]
Steps 6401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:14:21.060285] [Steps    6600] [G 0.0080719]
Steps 6601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:20:06.092037] [Steps    6800] [G 0.0075106]
Steps 6801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:25:51.495051] [Steps    7000] [G 0.0065080]
Steps 7001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:31:35.264719] [Steps    7200] [G 0.0069597]
Steps 7201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:37:20.415881] [Steps    7400] [G 0.0077044]
Steps 7401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:43:05.673374] [Steps    7600] [G 0.0059153]
Steps 7601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:48:49.080710] [Steps    7800] [G 0.0069842]
Steps 7801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 14:54:33.335330] [Steps    8000] [G 0.0066444]
Steps 8001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 15:00:18.107166] [Steps    8200] [G 0.0070583]
Steps 8201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 15:06:02.280287] [Steps    8400] [G 0.0055423]
Steps 8401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 15:11:46.839162] [Steps    8600] [G 0.0060016]
Steps 8601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 15:17:31.224147] [Steps    8800] [G 0.0054060]
Steps 8801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
[2023-05-15 15:23:14.770455] [Steps    9000] [G 0.0055332]
Steps 9001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5194)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f5c9f148b50>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 15:24:41.734831] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 15:24:41.736394] # Params - G: 43378727
[2023-05-15 15:24:41.736424] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 15:24:41.736450] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1267)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
lr为0.0001
Traceback (most recent call last):
  File "train2_vitGenerator.py", line 381, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 371, in worker
    pair_loader=pair_loader,val_pair_loader=val_pair_loader,logger=logger)
  File "train2_vitGenerator.py", line 162, in train
    val_images = val_images.cuda()#batch_size除以4
UnboundLocalError: local variable 'val_images' referenced before assignment
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f6ce636bb10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 15:25:01.064635] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 15:25:01.066082] # Params - G: 43378727
[2023-05-15 15:25:01.066109] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 15:25:01.066134] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5182)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
lr为0.0001
Traceback (most recent call last):
  File "train2_vitGenerator.py", line 381, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 371, in worker
    pair_loader=pair_loader,val_pair_loader=val_pair_loader,logger=logger)
  File "train2_vitGenerator.py", line 162, in train
    val_images = val_images.cuda()#batch_size除以4
UnboundLocalError: local variable 'val_images' referenced before assignment
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f6a6686f710>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 15:27:10.199337] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 15:27:10.200849] # Params - G: 43378727
[2023-05-15 15:27:10.200876] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 15:27:10.200898] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1317)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 15:27:50.134902] [Steps      10] [G 0.0523434]
Steps 11 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1317)
[2023-05-15 15:28:18.177126] [Steps      20] [G 0.0464152]
Steps 21 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1317)
[2023-05-15 15:28:46.597604] [Steps      30] [G 0.0400892]
Steps 31 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1317)
[2023-05-15 15:29:15.227625] [Steps      40] [G 0.0465381]
Steps 41 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1317)
[2023-05-15 15:29:43.790664] [Steps      50] [G 0.0383152]
Steps 51 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1317)
[2023-05-15 15:30:12.220628] [Steps      60] [G 0.0353736]
Steps 61 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1317)
[2023-05-15 15:30:40.730645] [Steps      70] [G 0.0287443]
Steps 71 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1317)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fe35f6e2cd0>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 15:32:40.997193] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 15:32:40.998637] # Params - G: 43378727
[2023-05-15 15:32:40.998663] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 15:32:40.998688] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/8149)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fc187de7bd0>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-15 15:37:50.416124] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-15 15:37:50.417549] # Params - G: 43378727
[2023-05-15 15:37:50.417576] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-15 15:37:50.417599] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-15 15:47:12.468288] [Steps     200] [G 0.0244925]
Steps 201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 15:56:25.348746] [Steps     400] [G 0.0164248]
Steps 401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 16:05:35.198704] [Steps     600] [G 0.0140136]
Steps 601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 16:14:47.065185] [Steps     800] [G 0.0148657]
Steps 801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 16:24:00.211400] [Steps    1000] [G 0.0201270]
Steps 1001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 16:33:09.267382] [Steps    1200] [G 0.0119553]
Steps 1201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 16:42:22.466650] [Steps    1400] [G 0.0160154]
Steps 1401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 16:51:35.693161] [Steps    1600] [G 0.0119489]
Steps 1601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 17:00:46.123950] [Steps    1800] [G 0.0099964]
Steps 1801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 17:09:57.426769] [Steps    2000] [G 0.0096269]
Steps 2001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 17:19:08.680110] [Steps    2200] [G 0.0113933]
Steps 2201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 17:28:17.162512] [Steps    2400] [G 0.0095501]
Steps 2401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 17:37:27.834521] [Steps    2600] [G 0.0389285]
Steps 2601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 17:46:40.700557] [Steps    2800] [G 0.0097765]
Steps 2801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 17:55:50.285036] [Steps    3000] [G 0.0102513]
Steps 3001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 18:05:02.364594] [Steps    3200] [G 0.0086372]
Steps 3201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 18:14:15.228021] [Steps    3400] [G 0.0076244]
Steps 3401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 18:23:25.427284] [Steps    3600] [G 0.0092797]
Steps 3601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 18:32:37.714788] [Steps    3800] [G 0.0075644]
Steps 3801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 18:41:48.065440] [Steps    4000] [G 0.0077514]
Steps 4001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 18:50:57.231840] [Steps    4200] [G 0.0077354]
Steps 4201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 19:00:09.701182] [Steps    4400] [G 0.0096634]
Steps 4401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 19:09:21.405108] [Steps    4600] [G 0.0067428]
Steps 4601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 19:18:29.358722] [Steps    4800] [G 0.0064129]
Steps 4801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 19:27:41.323954] [Steps    5000] [G 0.0079862]
Steps 5001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 19:36:53.823728] [Steps    5200] [G 0.0070043]
Steps 5201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 19:46:05.024163] [Steps    5400] [G 0.0077878]
Steps 5401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 19:55:17.572621] [Steps    5600] [G 0.0077328]
Steps 5601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 20:04:30.066273] [Steps    5800] [G 0.0060879]
Steps 5801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 20:13:41.382127] [Steps    6000] [G 0.0070276]
Steps 6001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 20:22:53.421582] [Steps    6200] [G 0.0111442]
Steps 6201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 20:32:05.723773] [Steps    6400] [G 0.0074211]
Steps 6401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 20:41:15.321540] [Steps    6600] [G 0.0080933]
Steps 6601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 20:50:27.181810] [Steps    6800] [G 0.0073209]
Steps 6801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 20:59:37.774555] [Steps    7000] [G 0.0061627]
Steps 7001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 21:08:47.543174] [Steps    7200] [G 0.0066740]
Steps 7201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 21:17:59.461151] [Steps    7400] [G 0.0079495]
Steps 7401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 21:27:11.347330] [Steps    7600] [G 0.0058122]
Steps 7601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 21:36:21.785764] [Steps    7800] [G 0.0064469]
Steps 7801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 21:45:33.730498] [Steps    8000] [G 0.0059471]
Steps 8001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 21:54:45.928329] [Steps    8200] [G 0.0069084]
Steps 8201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 22:03:56.291576] [Steps    8400] [G 0.0061533]
Steps 8401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 22:13:08.519871] [Steps    8600] [G 0.0060139]
Steps 8601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 22:22:20.676361] [Steps    8800] [G 0.0051879]
Steps 8801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 22:31:30.620329] [Steps    9000] [G 0.0055766]
Steps 9001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 22:40:41.485143] [Steps    9200] [G 0.0067398]
Steps 9201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 22:49:53.657150] [Steps    9400] [G 0.0055540]
Steps 9401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 22:59:03.956012] [Steps    9600] [G 0.0050790]
Steps 9601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 23:08:15.892155] [Steps    9800] [G 0.0084834]
Steps 9801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 23:17:27.046277] [Steps   10000] [G 0.0049892]
Steps 10001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 23:26:38.025756] [Steps   10200] [G 0.0072454]
Steps 10201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 23:35:48.262802] [Steps   10400] [G 0.0057400]
Steps 10401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 23:44:59.912818] [Steps   10600] [G 0.0046183]
Steps 10601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-15 23:54:10.358068] [Steps   10800] [G 0.0046030]
Steps 10801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 00:03:22.718002] [Steps   11000] [G 0.0042161]
Steps 11001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 00:12:35.897621] [Steps   11200] [G 0.0050913]
Steps 11201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 00:21:46.290998] [Steps   11400] [G 0.0047953]
Steps 11401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 00:30:58.049387] [Steps   11600] [G 0.0063399]
Steps 11601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 00:40:10.722982] [Steps   11800] [G 0.0040706]
Steps 11801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 00:49:21.297523] [Steps   12000] [G 0.0039738]
Steps 12001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 00:58:32.593900] [Steps   12200] [G 0.0082172]
Steps 12201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 01:07:43.200783] [Steps   12400] [G 0.0049791]
Steps 12401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 01:16:51.817747] [Steps   12600] [G 0.0067427]
Steps 12601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 01:26:03.590491] [Steps   12800] [G 0.0037229]
Steps 12801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 01:35:15.363369] [Steps   13000] [G 0.0038903]
Steps 13001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 01:44:24.681764] [Steps   13200] [G 0.0041098]
Steps 13201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 01:53:37.053357] [Steps   13400] [G 0.0040271]
Steps 13401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 02:02:49.240820] [Steps   13600] [G 0.0029551]
Steps 13601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 02:11:59.164859] [Steps   13800] [G 0.0053207]
Steps 13801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 02:21:11.130812] [Steps   14000] [G 0.0031957]
Steps 14001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 02:30:22.609013] [Steps   14200] [G 0.0065414]
Steps 14201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 02:39:31.985411] [Steps   14400] [G 0.0056196]
Steps 14401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 02:48:44.636637] [Steps   14600] [G 0.0040595]
Steps 14601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 02:57:57.219670] [Steps   14800] [G 0.0048120]
Steps 14801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 03:07:04.938990] [Steps   15000] [G 0.0040151]
Steps 15001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 03:16:16.371808] [Steps   15200] [G 0.0039870]
Steps 15201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 03:25:26.558567] [Steps   15400] [G 0.0040487]
Steps 15401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 03:34:36.968545] [Steps   15600] [G 0.0057451]
Steps 15601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 03:43:47.411722] [Steps   15800] [G 0.0051410]
Steps 15801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 03:52:56.266270] [Steps   16000] [G 0.0048747]
Steps 16001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 04:02:09.041263] [Steps   16200] [G 0.0042514]
Steps 16201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 04:11:20.672629] [Steps   16400] [G 0.0044480]
Steps 16401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 04:20:32.838002] [Steps   16600] [G 0.0047146]
Steps 16601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 04:29:45.290895] [Steps   16800] [G 0.0034997]
Steps 16801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 04:38:56.984985] [Steps   17000] [G 0.0031924]
Steps 17001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 04:48:06.882741] [Steps   17200] [G 0.0041401]
Steps 17201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 04:57:18.977963] [Steps   17400] [G 0.0039253]
Steps 17401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 05:06:31.519854] [Steps   17600] [G 0.0037387]
Steps 17601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 05:15:42.175288] [Steps   17800] [G 0.0041912]
Steps 17801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 05:24:54.259126] [Steps   18000] [G 0.0052786]
Steps 18001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 05:34:06.992657] [Steps   18200] [G 0.0037155]
Steps 18201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 05:43:17.883023] [Steps   18400] [G 0.0044850]
Steps 18401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 05:52:29.381744] [Steps   18600] [G 0.0034380]
Steps 18601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 06:01:40.618060] [Steps   18800] [G 0.0041783]
Steps 18801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 06:10:49.196891] [Steps   19000] [G 0.0023742]
Steps 19001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 06:20:00.762794] [Steps   19200] [G 0.0036075]
Steps 19201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 06:29:11.960500] [Steps   19400] [G 0.0028704]
Steps 19401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 06:38:21.820413] [Steps   19600] [G 0.0045490]
Steps 19601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 06:47:33.581391] [Steps   19800] [G 0.0037613]
Steps 19801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 06:56:44.251484] [Steps   20000] [G 0.0026598]
Steps 20001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 07:05:54.482867] [Steps   20200] [G 0.0034161]
Steps 20201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 07:15:05.071100] [Steps   20400] [G 0.0043909]
Steps 20401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 07:24:15.494628] [Steps   20600] [G 0.0029937]
Steps 20601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 07:33:23.427486] [Steps   20800] [G 0.0023145]
Steps 20801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 07:42:35.698447] [Steps   21000] [G 0.0024953]
Steps 21001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 07:51:48.318307] [Steps   21200] [G 0.0033338]
Steps 21201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 08:00:59.058413] [Steps   21400] [G 0.0022642]
Steps 21401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 08:10:12.521335] [Steps   21600] [G 0.0031675]
Steps 21601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 08:19:24.358759] [Steps   21800] [G 0.0022118]
Steps 21801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 08:28:34.265664] [Steps   22000] [G 0.0025501]
Steps 22001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 08:37:47.396176] [Steps   22200] [G 0.0027640]
Steps 22201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 08:47:01.214127] [Steps   22400] [G 0.0026371]
Steps 22401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 08:56:11.222003] [Steps   22600] [G 0.0032036]
Steps 22601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 09:05:24.568123] [Steps   22800] [G 0.0021372]
Steps 22801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 09:14:37.546206] [Steps   23000] [G 0.0021902]
Steps 23001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 09:23:49.222880] [Steps   23200] [G 0.0022236]
Steps 23201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 09:33:00.383833] [Steps   23400] [G 0.0024669]
Steps 23401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 09:42:10.943585] [Steps   23600] [G 0.0028939]
Steps 23601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 09:51:23.868551] [Steps   23800] [G 0.0025145]
Steps 23801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 10:00:37.992625] [Steps   24000] [G 0.0025527]
Steps 24001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 10:09:52.774685] [Steps   24200] [G 0.0028832]
Steps 24201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 10:19:05.459490] [Steps   24400] [G 0.0030087]
Steps 24401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 10:28:20.665986] [Steps   24600] [G 0.0018803]
Steps 24601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 10:37:36.734416] [Steps   24800] [G 0.0018398]
Steps 24801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 10:46:47.970109] [Steps   25000] [G 0.0037444]
Steps 25001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 10:55:59.679492] [Steps   25200] [G 0.0025572]
Steps 25201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 11:05:11.646719] [Steps   25400] [G 0.0026194]
Steps 25401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 11:14:23.106796] [Steps   25600] [G 0.0022207]
Steps 25601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 11:23:35.241245] [Steps   25800] [G 0.0024689]
Steps 25801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 11:32:48.472455] [Steps   26000] [G 0.0021533]
Steps 26001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 11:42:00.475894] [Steps   26200] [G 0.0017626]
Steps 26201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 11:51:13.824379] [Steps   26400] [G 0.0015804]
Steps 26401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 12:00:26.108081] [Steps   26600] [G 0.0017693]
Steps 26601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 12:09:37.025497] [Steps   26800] [G 0.0023866]
Steps 26801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 12:18:49.770766] [Steps   27000] [G 0.0018559]
Steps 27001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 12:28:01.639515] [Steps   27200] [G 0.0022866]
Steps 27201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 12:37:13.543660] [Steps   27400] [G 0.0019350]
Steps 27401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 12:46:27.118997] [Steps   27600] [G 0.0021422]
Steps 27601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 12:55:40.729827] [Steps   27800] [G 0.0019675]
Steps 27801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 13:04:51.719710] [Steps   28000] [G 0.0023994]
Steps 28001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 13:14:03.921921] [Steps   28200] [G 0.0018853]
Steps 28201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 13:23:17.747900] [Steps   28400] [G 0.0014423]
Steps 28401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 13:32:28.657451] [Steps   28600] [G 0.0031050]
Steps 28601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 13:41:43.007858] [Steps   28800] [G 0.0015254]
Steps 28801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 13:50:56.265963] [Steps   29000] [G 0.0017118]
Steps 29001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 14:00:08.768461] [Steps   29200] [G 0.0017907]
Steps 29201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 14:09:21.795093] [Steps   29400] [G 0.0020014]
Steps 29401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 14:18:32.739793] [Steps   29600] [G 0.0024727]
Steps 29601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 14:27:42.816857] [Steps   29800] [G 0.0017378]
Steps 29801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 14:36:55.066054] [Steps   30000] [G 0.0016866]
Steps 30001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 14:46:07.367047] [Steps   30200] [G 0.0019204]
Steps 30201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 14:55:19.704755] [Steps   30400] [G 0.0018030]
Steps 30401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 15:04:33.180708] [Steps   30600] [G 0.0022740]
Steps 30601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 15:13:46.413295] [Steps   30800] [G 0.0026878]
Steps 30801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 15:22:58.836845] [Steps   31000] [G 0.0019443]
Steps 31001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 15:32:11.467200] [Steps   31200] [G 0.0015114]
Steps 31201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 15:41:23.175387] [Steps   31400] [G 0.0016921]
Steps 31401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 15:50:35.166535] [Steps   31600] [G 0.0027237]
Steps 31601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 15:59:46.548994] [Steps   31800] [G 0.0020400]
Steps 31801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 16:08:59.188751] [Steps   32000] [G 0.0019960]
Steps 32001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 16:18:14.705226] [Steps   32200] [G 0.0014748]
Steps 32201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 16:27:27.867260] [Steps   32400] [G 0.0019003]
Steps 32401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 16:36:39.940724] [Steps   32600] [G 0.0020202]
Steps 32601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 16:45:52.802110] [Steps   32800] [G 0.0012010]
Steps 32801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 16:55:04.134177] [Steps   33000] [G 0.0026120]
Steps 33001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 17:04:15.159718] [Steps   33200] [G 0.0013695]
Steps 33201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 17:13:28.958790] [Steps   33400] [G 0.0023821]
Steps 33401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 17:22:42.474065] [Steps   33600] [G 0.0011837]
Steps 33601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 17:31:53.072659] [Steps   33800] [G 0.0014334]
Steps 33801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 17:41:05.894008] [Steps   34000] [G 0.0012781]
Steps 34001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 17:50:17.886575] [Steps   34200] [G 0.0012227]
Steps 34201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 17:59:27.719393] [Steps   34400] [G 0.0014665]
Steps 34401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 18:08:39.099290] [Steps   34600] [G 0.0021128]
Steps 34601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 18:17:50.131865] [Steps   34800] [G 0.0015569]
Steps 34801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 18:26:58.621184] [Steps   35000] [G 0.0014236]
Steps 35001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[2023-05-16 18:36:10.887825] [Steps   35200] [G 0.0025069]
Steps 35201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/6177)
[ WARN:0@97311.352] global loadsave.cpp:244 findDecoder imread_('data/unlabeled_data/train/2/bathroom-design_145.jpg'): can't open/read file: check file path/integrity
[ WARN:0@97354.711] global loadsave.cpp:244 findDecoder imread_('data/unlabeled_data/train/2/bathroom-design_145.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train2_vitGenerator.py", line 383, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 373, in worker
    pair_loader=pair_loader,val_pair_loader=val_pair_loader,logger=logger)
  File "train2_vitGenerator.py", line 183, in train
    images,target_images,illus=next(pair_loader)
  File "/home/xsx/dino/utils.py", line 956, in cycle3
    for images, targets,illu in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1199, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 2.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 224, in __getitem__
    train_image=self.transform(cv2.cvtColor(train_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fdaeb9e0190>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-16 19:40:00.779831] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-16 19:40:00.781350] # Params - G: 43378727
[2023-05-16 19:40:00.781377] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-16 19:40:00.781401] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9454)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-16 19:52:09.372076] [Steps     200] [G 0.0306994]
Steps 201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9454)
[2023-05-16 20:04:13.074794] [Steps     400] [G 0.0186681]
Steps 401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/9454)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f82cbbe0c90>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-16 20:15:49.606185] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-16 20:15:49.607653] # Params - G: 43378727
[2023-05-16 20:15:49.607681] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-16 20:15:49.607703] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/3315)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f34ef46de10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-16 20:17:39.284732] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-16 20:17:39.286180] # Params - G: 43378727
[2023-05-16 20:17:39.286207] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-16 20:17:39.286235] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/1490)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fcb67ce4b10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-16 20:20:11.582456] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-16 20:20:11.586782] # Params - G: 43378727
[2023-05-16 20:20:11.586855] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-16 20:20:11.586914] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-16 21:22:57.275706] [Steps    1000] [G 0.0165108]
Steps 1001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-16 22:25:20.351395] [Steps    2000] [G 0.0158937]
Steps 2001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-16 23:27:25.609110] [Steps    3000] [G 0.0085665]
Steps 3001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-17 00:29:30.237009] [Steps    4000] [G 0.0089046]
Steps 4001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-17 01:31:40.862607] [Steps    5000] [G 0.0084333]
Steps 5001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-17 02:33:48.093847] [Steps    6000] [G 0.0097503]
Steps 6001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-17 03:36:09.413439] [Steps    7000] [G 0.0063052]
Steps 7001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-17 04:38:22.659197] [Steps    8000] [G 0.0062937]
Steps 8001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-17 05:41:23.588356] [Steps    9000] [G 0.0062964]
Steps 9001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-17 06:43:42.051784] [Steps   10000] [G 0.0047514]
Steps 10001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[2023-05-17 07:46:02.096972] [Steps   11000] [G 0.0058930]
Steps 11001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7ff838a29b50>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)

Traceback (most recent call last):
  File "train2_vitGenerator.py", line 384, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 320, in worker
    generator = generator.cuda()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 491, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 409, in _apply
    param_applied = fn(param)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 491, in <lambda>
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
[2023-05-17 08:48:08.659112] [Steps   12000] [G 0.0052943]
Steps 12001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5391)
[ WARN:0@44900.336] global loadsave.cpp:244 findDecoder imread_('data/unlabeled_data/val/2/bathroom-design_145.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train2_vitGenerator.py", line 384, in <module>
    worker(P)
  File "train2_vitGenerator.py", line 374, in worker
    pair_loader=pair_loader,val_pair_loader=val_pair_loader,logger=logger)
  File "train2_vitGenerator.py", line 162, in train
    val_images,val_target_images,val_illus=next(val_pair_loader)
  File "/home/xsx/dino/utils.py", line 956, in cycle3
    for images, targets,illu in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1179, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 310, in __getitem__
    train_image=self.transform(cv2.cvtColor(train_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f004aa6cc10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-17 09:21:31.289900] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-17 09:21:31.291380] # Params - G: 43378727
[2023-05-17 09:21:31.291407] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-17 09:21:31.291428] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/837)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f57fc8d4d90>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-17 09:22:08.773247] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-17 09:22:08.774664] # Params - G: 43378727
[2023-05-17 09:22:08.774690] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-17 09:22:08.774712] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/7366)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f1aca48ec10>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-17 09:25:17.118266] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-17 09:25:17.119735] # Params - G: 43378727
[2023-05-17 09:25:17.119761] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-17 09:25:17.119784] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/8377)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f3c68a6dcd0>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-17 09:27:20.435443] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=True)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-17 09:27:20.436888] # Params - G: 43378727
[2023-05-17 09:27:20.436914] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-17 09:27:20.436934] Use G moving average: 0.9999778195362122
Steps 1 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-17 11:11:01.976948] [Steps    1000] [G 0.0166829]
Steps 1001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 12:54:49.657306] [Steps    2000] [G 0.0126652]
Steps 2001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 14:38:39.558482] [Steps    3000] [G 0.0094805]
Steps 3001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 16:22:31.216393] [Steps    4000] [G 0.0116143]
Steps 4001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 18:06:33.633760] [Steps    5000] [G 0.0079243]
Steps 5001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 19:50:31.648824] [Steps    6000] [G 0.0060846]
Steps 6001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7fa567a95d90>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.param_free_norm.weight', 'blocks.0.norm1.param_free_norm.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.param_free_norm.weight', 'blocks.0.norm2.param_free_norm.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.param_free_norm.weight', 'blocks.1.norm1.param_free_norm.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.param_free_norm.weight', 'blocks.1.norm2.param_free_norm.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.param_free_norm.weight', 'blocks.2.norm1.param_free_norm.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.param_free_norm.weight', 'blocks.2.norm2.param_free_norm.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.param_free_norm.weight', 'blocks.3.norm1.param_free_norm.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.param_free_norm.weight', 'blocks.3.norm2.param_free_norm.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.param_free_norm.weight', 'blocks.4.norm1.param_free_norm.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.param_free_norm.weight', 'blocks.4.norm2.param_free_norm.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.param_free_norm.weight', 'blocks.5.norm1.param_free_norm.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.param_free_norm.weight', 'blocks.5.norm2.param_free_norm.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.param_free_norm.weight', 'blocks.6.norm1.param_free_norm.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.param_free_norm.weight', 'blocks.6.norm2.param_free_norm.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.param_free_norm.weight', 'norm.param_free_norm.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
generator的train1参数加载结果为<All keys matched successfully>
g_ema的train1参数加载结果为<All keys matched successfully>
[2023-05-17 20:29:56.294482] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-17 20:29:56.294536] Use G moving average: 0.9999778195362122
Steps 6001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f05fcf8ad10>, (128, 128, 3))
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
generator的train1参数加载结果为<All keys matched successfully>
g_ema的train1参数加载结果为<All keys matched successfully>
[2023-05-17 20:30:38.069688] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-17 20:30:38.069732] Use G moving average: 0.9999778195362122
Steps 6001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-17 20:36:25.205724] [Steps    6200] [G 0.0058028]
Steps 6201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 20:42:05.539064] [Steps    6400] [G 0.0044378]
Steps 6401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 20:47:46.242063] [Steps    6600] [G 0.0082662]
Steps 6601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 20:53:24.384639] [Steps    6800] [G 0.0059018]
Steps 6801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 20:59:04.971965] [Steps    7000] [G 0.0079782]
Steps 7001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:04:45.810920] [Steps    7200] [G 0.0062916]
Steps 7201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:10:24.628219] [Steps    7400] [G 0.0079392]
Steps 7401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:16:04.867973] [Steps    7600] [G 0.0088700]
Steps 7601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:21:45.794738] [Steps    7800] [G 0.0044381]
Steps 7801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:27:24.373007] [Steps    8000] [G 0.0047417]
Steps 8001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:33:05.102981] [Steps    8200] [G 0.0057704]
Steps 8201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:38:45.673142] [Steps    8400] [G 0.0080435]
Steps 8401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:44:24.377709] [Steps    8600] [G 0.0069041]
Steps 8601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:50:04.959177] [Steps    8800] [G 0.0059908]
Steps 8801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 21:55:45.576019] [Steps    9000] [G 0.0059067]
Steps 9001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:01:24.616076] [Steps    9200] [G 0.0066525]
Steps 9201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:07:05.139295] [Steps    9400] [G 0.0058285]
Steps 9401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:12:45.364459] [Steps    9600] [G 0.0037803]
Steps 9601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:18:23.802094] [Steps    9800] [G 0.0033968]
Steps 9801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:24:04.203055] [Steps   10000] [G 0.0046977]
Steps 10001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:29:46.030227] [Steps   10200] [G 0.0050081]
Steps 10201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:35:25.725921] [Steps   10400] [G 0.0058957]
Steps 10401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:41:06.085657] [Steps   10600] [G 0.0052809]
Steps 10601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:46:46.009223] [Steps   10800] [G 0.0041416]
Steps 10801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:52:23.667891] [Steps   11000] [G 0.0060309]
Steps 11001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 22:58:03.430786] [Steps   11200] [G 0.0057425]
Steps 11201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:03:43.561482] [Steps   11400] [G 0.0048420]
Steps 11401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:09:23.726395] [Steps   11600] [G 0.0051997]
Steps 11601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:15:05.606571] [Steps   11800] [G 0.0058246]
Steps 11801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:20:47.079476] [Steps   12000] [G 0.0048535]
Steps 12001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:26:26.985521] [Steps   12200] [G 0.0048044]
Steps 12201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:32:08.645472] [Steps   12400] [G 0.0046783]
Steps 12401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:37:49.950196] [Steps   12600] [G 0.0054990]
Steps 12601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:43:29.697673] [Steps   12800] [G 0.0067094]
Steps 12801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:49:11.267021] [Steps   13000] [G 0.0036486]
Steps 13001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-17 23:54:52.793094] [Steps   13200] [G 0.0052295]
Steps 13201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:00:31.801689] [Steps   13400] [G 0.0049769]
Steps 13401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:06:12.767109] [Steps   13600] [G 0.0035046]
Steps 13601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:11:53.928716] [Steps   13800] [G 0.0044213]
Steps 13801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:17:32.758099] [Steps   14000] [G 0.0052277]
Steps 14001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:23:13.092260] [Steps   14200] [G 0.0040701]
Steps 14201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:28:53.393753] [Steps   14400] [G 0.0039138]
Steps 14401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:34:32.435584] [Steps   14600] [G 0.0056377]
Steps 14601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:40:13.440074] [Steps   14800] [G 0.0032440]
Steps 14801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:45:53.876289] [Steps   15000] [G 0.0042741]
Steps 15001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:51:32.496526] [Steps   15200] [G 0.0032252]
Steps 15201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 00:57:12.977917] [Steps   15400] [G 0.0035805]
Steps 15401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:02:53.623013] [Steps   15600] [G 0.0037283]
Steps 15601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:08:32.519346] [Steps   15800] [G 0.0035686]
Steps 15801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:14:13.155475] [Steps   16000] [G 0.0038375]
Steps 16001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:19:53.110164] [Steps   16200] [G 0.0038660]
Steps 16201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:25:31.196954] [Steps   16400] [G 0.0036972]
Steps 16401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:31:11.029471] [Steps   16600] [G 0.0031665]
Steps 16601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:36:50.940377] [Steps   16800] [G 0.0043103]
Steps 16801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:42:29.842726] [Steps   17000] [G 0.0030163]
Steps 17001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:48:10.604864] [Steps   17200] [G 0.0042235]
Steps 17201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:53:51.226992] [Steps   17400] [G 0.0036398]
Steps 17401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 01:59:30.648557] [Steps   17600] [G 0.0048377]
Steps 17601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:05:11.614996] [Steps   17800] [G 0.0035993]
Steps 17801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:10:52.716196] [Steps   18000] [G 0.0035887]
Steps 18001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:16:32.043432] [Steps   18200] [G 0.0025420]
Steps 18201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:22:13.168899] [Steps   18400] [G 0.0031243]
Steps 18401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:27:54.615147] [Steps   18600] [G 0.0045699]
Steps 18601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:33:34.104315] [Steps   18800] [G 0.0034641]
Steps 18801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:39:15.606547] [Steps   19000] [G 0.0028564]
Steps 19001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:44:57.066735] [Steps   19200] [G 0.0034198]
Steps 19201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:50:36.110812] [Steps   19400] [G 0.0026039]
Steps 19401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 02:56:17.075878] [Steps   19600] [G 0.0034790]
Steps 19601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:01:58.000394] [Steps   19800] [G 0.0028423]
Steps 19801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:07:36.651245] [Steps   20000] [G 0.0030091]
Steps 20001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:13:17.678101] [Steps   20200] [G 0.0034170]
Steps 20201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:18:58.148910] [Steps   20400] [G 0.0021354]
Steps 20401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:24:37.030836] [Steps   20600] [G 0.0034023]
Steps 20601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:30:17.423001] [Steps   20800] [G 0.0024940]
Steps 20801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:35:58.256399] [Steps   21000] [G 0.0022093]
Steps 21001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:41:37.012051] [Steps   21200] [G 0.0034749]
Steps 21201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:47:17.672877] [Steps   21400] [G 0.0030750]
Steps 21401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:52:58.550235] [Steps   21600] [G 0.0041759]
Steps 21601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 03:58:38.170691] [Steps   21800] [G 0.0031002]
Steps 21801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:04:20.074805] [Steps   22000] [G 0.0028050]
Steps 22001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:10:01.212958] [Steps   22200] [G 0.0030856]
Steps 22201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:15:40.443899] [Steps   22400] [G 0.0038450]
Steps 22401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:21:21.917836] [Steps   22600] [G 0.0026441]
Steps 22601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:27:02.527827] [Steps   22800] [G 0.0023272]
Steps 22801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:32:40.167254] [Steps   23000] [G 0.0022144]
Steps 23001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:38:20.592612] [Steps   23200] [G 0.0022651]
Steps 23201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:44:01.413361] [Steps   23400] [G 0.0021370]
Steps 23401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:49:40.400503] [Steps   23600] [G 0.0029543]
Steps 23601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 04:55:21.745931] [Steps   23800] [G 0.0028060]
Steps 23801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:01:03.377829] [Steps   24000] [G 0.0033123]
Steps 24001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:06:43.361298] [Steps   24200] [G 0.0024100]
Steps 24201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:12:23.919470] [Steps   24400] [G 0.0024262]
Steps 24401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:18:04.814522] [Steps   24600] [G 0.0022609]
Steps 24601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:23:43.659814] [Steps   24800] [G 0.0032713]
Steps 24801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:29:24.897148] [Steps   25000] [G 0.0021869]
Steps 25001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:35:05.071676] [Steps   25200] [G 0.0025925]
Steps 25201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:40:43.711268] [Steps   25400] [G 0.0019681]
Steps 25401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:46:24.345438] [Steps   25600] [G 0.0023698]
Steps 25601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:52:04.984592] [Steps   25800] [G 0.0017835]
Steps 25801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 05:57:42.667071] [Steps   26000] [G 0.0022098]
Steps 26001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:03:23.720843] [Steps   26200] [G 0.0033064]
Steps 26201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:09:05.029058] [Steps   26400] [G 0.0016774]
Steps 26401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:14:43.897778] [Steps   26600] [G 0.0020470]
Steps 26601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:20:23.889744] [Steps   26800] [G 0.0020010]
Steps 26801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:26:04.147057] [Steps   27000] [G 0.0021485]
Steps 27001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:31:43.045212] [Steps   27200] [G 0.0018743]
Steps 27201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:37:23.613683] [Steps   27400] [G 0.0025841]
Steps 27401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:43:04.486438] [Steps   27600] [G 0.0017095]
Steps 27601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:48:43.604608] [Steps   27800] [G 0.0018883]
Steps 27801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 06:54:23.011949] [Steps   28000] [G 0.0018613]
Steps 28001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:00:03.593849] [Steps   28200] [G 0.0020427]
Steps 28201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:05:43.031387] [Steps   28400] [G 0.0020582]
Steps 28401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:11:23.240324] [Steps   28600] [G 0.0016367]
Steps 28601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:17:03.724477] [Steps   28800] [G 0.0017506]
Steps 28801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:22:42.758297] [Steps   29000] [G 0.0016814]
Steps 29001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:28:23.228394] [Steps   29200] [G 0.0013628]
Steps 29201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:34:03.745602] [Steps   29400] [G 0.0019012]
Steps 29401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:39:41.501668] [Steps   29600] [G 0.0017836]
Steps 29601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:45:21.895118] [Steps   29800] [G 0.0020233]
Steps 29801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:51:02.527514] [Steps   30000] [G 0.0013619]
Steps 30001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 07:56:42.057609] [Steps   30200] [G 0.0026247]
Steps 30201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 08:02:23.037030] [Steps   30400] [G 0.0013498]
Steps 30401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 08:08:04.337270] [Steps   30600] [G 0.0020588]
Steps 30601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 08:13:43.295708] [Steps   30800] [G 0.0014194]
Steps 30801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f88c2852c50>, (128, 128, 3))
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
generator的train1参数加载结果为<All keys matched successfully>
g_ema的train1参数加载结果为<All keys matched successfully>
[2023-05-18 08:27:56.760311] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-18 08:27:56.760356] Use G moving average: 0.9999778195362122
Steps 30801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-18 08:33:45.595562] [Steps   31000] [G 0.0017456]
Steps 31001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 08:39:27.842945] [Steps   31200] [G 0.0012849]
Steps 31201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 08:45:09.546607] [Steps   31400] [G 0.0014865]
Steps 31401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 08:50:49.323021] [Steps   31600] [G 0.0018005]
Steps 31601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 08:56:31.808923] [Steps   31800] [G 0.0016288]
Steps 31801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 09:02:14.189375] [Steps   32000] [G 0.0019780]
Steps 32001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f7177175cd0>, (128, 128, 3))
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
generator的train1参数加载结果为<All keys matched successfully>
g_ema的train1参数加载结果为<All keys matched successfully>
[2023-05-18 09:03:03.267077] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-18 09:03:03.267121] Use G moving average: 0.9999778195362122
Steps 32001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-18 09:31:13.001043] [Steps   33000] [G 0.0013769]
Steps 33001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 09:59:14.351238] [Steps   34000] [G 0.0016901]
Steps 34001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 10:27:17.536495] [Steps   35000] [G 0.0015261]
Steps 35001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 10:55:20.412486] [Steps   36000] [G 0.0012863]
Steps 36001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 11:23:21.991364] [Steps   37000] [G 0.0015116]
Steps 37001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 11:51:24.741994] [Steps   38000] [G 0.0021599]
Steps 38001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 12:19:28.019165] [Steps   39000] [G 0.0016029]
Steps 39001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 12:47:31.480104] [Steps   40000] [G 0.0013594]
Steps 40001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 13:15:37.160668] [Steps   41000] [G 0.0018612]
Steps 41001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 13:43:38.495044] [Steps   42000] [G 0.0010365]
Steps 42001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 14:11:43.041481] [Steps   43000] [G 0.0017532]
Steps 43001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 14:39:46.940645] [Steps   44000] [G 0.0008045]
Steps 44001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 15:07:50.120600] [Steps   45000] [G 0.0009912]
Steps 45001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 15:35:55.518499] [Steps   46000] [G 0.0007798]
Steps 46001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 16:04:01.861073] [Steps   47000] [G 0.0016730]
Steps 47001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 16:32:06.701413] [Steps   48000] [G 0.0010801]
Steps 48001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 17:00:14.127167] [Steps   49000] [G 0.0020869]
Steps 49001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-18 17:28:18.034023] [Steps   50000] [G 0.0006427]
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7ff9f55d9cd0>, (128, 128, 3))
Since no pretrained weights have been provided, we load the reference pretrained DINO weights.
generator的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
g_ema的预训练参数加载结果为_IncompatibleKeys(missing_keys=['style.1.weight', 'style.1.bias', 'blocks.0.norm1.mlp_gamma.weight', 'blocks.0.norm1.mlp_gamma.bias', 'blocks.0.norm1.mlp_beta.weight', 'blocks.0.norm1.mlp_beta.bias', 'blocks.0.norm2.mlp_gamma.weight', 'blocks.0.norm2.mlp_gamma.bias', 'blocks.0.norm2.mlp_beta.weight', 'blocks.0.norm2.mlp_beta.bias', 'blocks.0.skip.weight', 'blocks.0.skip.bias', 'blocks.1.norm1.mlp_gamma.weight', 'blocks.1.norm1.mlp_gamma.bias', 'blocks.1.norm1.mlp_beta.weight', 'blocks.1.norm1.mlp_beta.bias', 'blocks.1.norm2.mlp_gamma.weight', 'blocks.1.norm2.mlp_gamma.bias', 'blocks.1.norm2.mlp_beta.weight', 'blocks.1.norm2.mlp_beta.bias', 'blocks.1.skip.weight', 'blocks.1.skip.bias', 'blocks.2.norm1.mlp_gamma.weight', 'blocks.2.norm1.mlp_gamma.bias', 'blocks.2.norm1.mlp_beta.weight', 'blocks.2.norm1.mlp_beta.bias', 'blocks.2.norm2.mlp_gamma.weight', 'blocks.2.norm2.mlp_gamma.bias', 'blocks.2.norm2.mlp_beta.weight', 'blocks.2.norm2.mlp_beta.bias', 'blocks.2.skip.weight', 'blocks.2.skip.bias', 'blocks.3.norm1.mlp_gamma.weight', 'blocks.3.norm1.mlp_gamma.bias', 'blocks.3.norm1.mlp_beta.weight', 'blocks.3.norm1.mlp_beta.bias', 'blocks.3.norm2.mlp_gamma.weight', 'blocks.3.norm2.mlp_gamma.bias', 'blocks.3.norm2.mlp_beta.weight', 'blocks.3.norm2.mlp_beta.bias', 'blocks.3.skip.weight', 'blocks.3.skip.bias', 'blocks.4.norm1.mlp_gamma.weight', 'blocks.4.norm1.mlp_gamma.bias', 'blocks.4.norm1.mlp_beta.weight', 'blocks.4.norm1.mlp_beta.bias', 'blocks.4.norm2.mlp_gamma.weight', 'blocks.4.norm2.mlp_gamma.bias', 'blocks.4.norm2.mlp_beta.weight', 'blocks.4.norm2.mlp_beta.bias', 'blocks.4.skip.weight', 'blocks.4.skip.bias', 'blocks.5.norm1.mlp_gamma.weight', 'blocks.5.norm1.mlp_gamma.bias', 'blocks.5.norm1.mlp_beta.weight', 'blocks.5.norm1.mlp_beta.bias', 'blocks.5.norm2.mlp_gamma.weight', 'blocks.5.norm2.mlp_gamma.bias', 'blocks.5.norm2.mlp_beta.weight', 'blocks.5.norm2.mlp_beta.bias', 'blocks.5.skip.weight', 'blocks.5.skip.bias', 'blocks.6.norm1.mlp_gamma.weight', 'blocks.6.norm1.mlp_gamma.bias', 'blocks.6.norm1.mlp_beta.weight', 'blocks.6.norm1.mlp_beta.bias', 'blocks.6.norm2.mlp_gamma.weight', 'blocks.6.norm2.mlp_gamma.bias', 'blocks.6.norm2.mlp_beta.weight', 'blocks.6.norm2.mlp_beta.bias', 'blocks.6.skip.weight', 'blocks.6.skip.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgb.bias', 'to_rgb.conv.weight', 'to_rgb.conv.modulation.weight', 'to_rgb.conv.modulation.bias', 'norm.mlp_gamma.weight', 'norm.mlp_gamma.bias', 'norm.mlp_beta.weight', 'norm.mlp_beta.bias', 'mid.weight', 'mid.bias'], unexpected_keys=['blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'norm.weight', 'norm.bias'])
[2023-05-21 10:52:03.364781] VisionTransformer(
  (style): Sequential(
    (0): PixelNorm()
    (1): EqualLinear(384, 384)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (activation): SinActivation()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (1): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (2): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (3): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (4): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (5): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
    (6): Block(
      (norm1): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): SelfModulatedLayerNorm(
        (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
        (mlp_gamma): EqualLinear(384, 384)
        (mlp_beta): EqualLinear(384, 384)
      )
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (skip): Linear(in_features=197, out_features=256, bias=True)
    )
  )
  (convs): ModuleList(
    (0): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (1): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (2): StyleLayer(
      (conv): ModulatedConv2d(768, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (3): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (4): StyleLayer(
      (conv): ModulatedConv2d(480, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (5): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (6): StyleLayer(
      (conv): ModulatedConv2d(408, 384, 3, upsample=True)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (7): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (8): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (9): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (10): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (11): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (12): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (13): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (14): StyleLayer(
      (conv): ModulatedConv2d(390, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
    (15): StyleLayer(
      (conv): ModulatedConv2d(384, 384, 3, upsample=False)
      (noise): NoiseInjection()
      (activate): FusedLeakyReLU()
    )
  )
  (to_rgb): ToRGB(
    (conv): ModulatedConv2d(385, 3, 1, upsample=False)
  )
  (norm): SelfModulatedLayerNorm(
    (param_free_norm): LayerNorm((384,), eps=0.001, elementwise_affine=False)
    (mlp_gamma): EqualLinear(384, 384)
    (mlp_beta): EqualLinear(384, 384)
  )
  (mid): Linear(in_features=197, out_features=64, bias=True)
  (head): Identity()
)
[2023-05-21 10:52:03.367406] # Params - G: 43367207
[2023-05-21 10:52:03.367455] {'dataset': 'unlabeled_data1_LAB_presudo', 'batch_size': 32, 'fid_size': 10000, 'loss': 'nonsat', 'max_steps': 50000, 'warmup': 3000, 'n_critic': 1, 'lr': 0.0001, 'lr_d': 0.0001, 'beta': (0.0, 0.99), 'lbd': 1.0, 'lbd2': 1.0}
[2023-05-21 10:52:03.367496] Use G moving average: 0.9999778195362122

/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
当前ResBlock的in_channel为256,当前ResBlock的out_channel为512
当前ResBlock的in_channel为512,当前ResBlock的out_channel为512
当前ResBlock的in_channel为512,当前ResBlock的out_channel为512
当前ResBlock的in_channel为512,当前ResBlock的out_channel为512
当前ResBlock的in_channel为512,当前ResBlock的out_channel为512
当前ResBlock的in_channel为256,当前ResBlock的out_channel为512
当前ResBlock的in_channel为512,当前ResBlock的out_channel为512
当前ResBlock的in_channel为512,当前ResBlock的out_channel为512
当前ResBlock的in_channel为512,当前ResBlock的out_channel为512
当前ResBlock的in_channel为512,当前ResBlock的out_channel为512
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 596, in worker
    opt = torch.load(f"{P.resume}/optim_stage2_90000.pt")
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 579, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357/optim_stage2_90000.pt'
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 596, in worker
    opt = torch.load(f"{P.resume}/optim_stage2_90000.pt")
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 579, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357/optim_stage2_90000.pt'
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 596, in worker
    opt = torch.load(f"{P.resume}/optim_stage2_90000.pt")
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 579, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357/optim_stage2_90000.pt'
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:49:05.950114] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:49:05.950166] Use G moving average: 0.9999778195362122
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:49:47.703879] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:49:47.703937] Use G moving average: 0.9999778195362122
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
平衡一致性损失为0.028833163902163506
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 316, in train
    ds_loss, ds_aux = train_fn["train3_D_match"](P, discriminator_single, opt,real_images,ugen_images)
  File "/home/xsx/dino/training/gan/aug_both.py", line 215, in loss_D_my_match_fn
    d_loss,aux=loss_D_match_fn(P, D, options, l_img_input, u_img_input)
  File "/home/xsx/dino/training/gan/aug_both.py", line 62, in loss_D_match_fn
    d_all = D(P.augment_fn(all_images))#用d_all来存储discriminator的输出值
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/models/gan/base.py", line 147, in forward
    features = self.penultimate(inputs)#
  File "/home/xsx/dino/mydiscriminator.py", line 295, in penultimate
    out = self.layers(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/models/gan/stylegan2/op/fused_act.py", line 109, in forward
    return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)
  File "/home/xsx/dino/models/gan/stylegan2/op/fused_act.py", line 113, in fused_leaky_relu
    return scale * F.leaky_relu(input + bias.view((1, -1) + (1,) * (len(input.shape) - 2)),
RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.77 GiB total capacity; 9.75 GiB already allocated; 77.50 MiB free; 10.53 GiB reserved in total by PyTorch)

=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:51:05.469214] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:51:05.469262] Use G moving average: 0.9999889097066088
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 316, in train
    ds_loss, ds_aux = train_fn["train3_D_match"](P, discriminator_single, opt,real_images,ugen_images)
  File "/home/xsx/dino/training/gan/aug_both.py", line 215, in loss_D_my_match_fn
    d_loss,aux=loss_D_match_fn(P, D, options, l_img_input, u_img_input)
  File "/home/xsx/dino/training/gan/aug_both.py", line 83, in loss_D_match_fn
    lbd=options['lbd'], lbd2=options['lbd2'])#options.lbd=1,options.lbd2=1
  File "/home/xsx/dino/penalty.py", line 69, in compute_penalty
    return call_with_accepted_args(fn, **kwargs)
  File "/home/xsx/dino/utils.py", line 1056, in call_with_accepted_args
    return fn(**kwargs)
  File "/home/xsx/dino/penalty.py", line 51, in balanced_consistency
    d_aug_all = D(P.augment_fn(all_images))#但是这里的数据增强不是一样的吗
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/models/gan/base.py", line 147, in forward
    features = self.penultimate(inputs)#
  File "/home/xsx/dino/mydiscriminator.py", line 295, in penultimate
    out = self.layers(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/mydiscriminator.py", line 131, in forward
    out = self.conv2(out)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/models/gan/stylegan2/layers.py", line 124, in forward
    out = upfirdn2d(input, self.kernel, pad=self.pad)
  File "/home/xsx/dino/models/gan/stylegan2/op/upfirdn2d.py", line 214, in upfirdn2d
    out = upfirdn2d_native(input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1])
  File "/home/xsx/dino/models/gan/stylegan2/op/upfirdn2d.py", line 227, in upfirdn2d_native
    out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/functional.py", line 3997, in _pad
    return _VF.constant_pad_nd(input, pad, value)
RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 11.77 GiB total capacity; 9.90 GiB already allocated; 121.50 MiB free; 10.48 GiB reserved in total by PyTorch)

=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:52:25.868246] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:52:25.868295] Use G moving average: 0.99999445483793
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 386, in train
    writer.add_scalars('stage2_G',{'g_critic_loss': losses['G_critic_loss'][-1], 'g_l_mse_loss': losses['G_l_mse_loss'][-1],'g_ch_loss':losses['G_ch_loss']}, step)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/tensorboardX/writer.py", line 505, in add_scalars
    fw.add_summary(scalar(main_tag, scalar_value),
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/tensorboardX/summary.py", line 152, in scalar
    assert scalar.squeeze().ndim == 0, 'scalar should be 0D'
AssertionError: scalar should be 0D
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:54:30.280456] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 11:54:30.280519] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 213, in train
    val_images,val_images128,val_target_images,val_illus=next(val_pair_loader)
  File "/home/xsx/dino/utils.py", line 956, in cycle3
    for images, targets,illu in dataloader:
ValueError: too many values to unpack (expected 3)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 13:38:30.538065] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 13:38:30.538122] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.37s/it]100%|██████████| 1/1 [00:05<00:00,  5.37s/it][2023-05-22 14:48:07.765342] [Steps   50200][G 2.5858793][G_critic 1.3927556] [G_l_mse 0.0119312] [G_ch_loss 0.00000000000000][DP 1.3405708][DP_real 0.1215388][DP_gen 0.0104900][DP_penalty 0.0540897][DS 1.3986812][DS_real 0.0608250][DS_gen 0.0736399][DS_penalty 0.0506951]
Steps 50201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.28s/it]100%|██████████| 1/1 [00:01<00:00,  1.28s/it][2023-05-22 14:48:09.828628] [Steps   50200][fid_score 20.0663410]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.66it/s]100%|██████████| 1/1 [00:00<00:00,  1.66it/s]
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 220, in train
    val_fid_value=my_fid_score(path_base='base_stats.npz', G=generator, size=val_images.size(0), batch_size=val_images.size(0), model=None, dims=192)
  File "/home/xsx/dino/fid_score.py", line 274, in my_fid_score
    fid_value = calculate_frechet_distance(m1, s1, m2, s2)
  File "/home/xsx/dino/fid_score.py", line 92, in calculate_frechet_distance
    'Training and test covariances have different dimensions'
AssertionError: Training and test covariances have different dimensions
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 16:56:47.591538] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 16:56:47.591601] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:01<00:01,  1.90s/it]100%|██████████| 2/2 [00:02<00:00,  1.27s/it]100%|██████████| 2/2 [00:02<00:00,  1.36s/it][2023-05-22 18:15:46.676384] [Steps   50200][G 2.4735467][G_critic 1.4581189] [G_l_mse 0.0101543] [G_ch_loss 0.00000000000000][DP 1.3618622][DP_real 0.0792674][DP_gen 0.0224098][DP_penalty 0.0256918][DS 1.3558277][DS_real 0.1202200][DS_gen 0.0472734][DS_penalty 0.0509261]
Steps 50201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.17s/it]100%|██████████| 1/1 [00:01<00:00,  1.17s/it][2023-05-22 18:15:48.650321] [Steps   50200][fid_score 10.8722034]

0it [00:00, ?it/s]0it [00:00, ?it/s]
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 220, in train
    val_fid_value=my_fid_score(path_base='base_stats.npz', G=generator, size=val_images.size(0), batch_size=32, model=None, dims=192)
  File "/home/xsx/dino/fid_score.py", line 272, in my_fid_score
    m2, s2 = compute_stats_from_my_G(G, model, size=size, batch_size=batch_size,pair_loader=pair_loader)
  File "/home/xsx/dino/fid_score.py", line 230, in compute_stats_from_my_G
    predictions = torch.cat(predictions, dim=0)#batchsize对最终结果没有影响，反正都是要cat在一起
RuntimeError: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].

CPU: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/build/aten/src/ATen/RegisterCPU.cpp:5925 [kernel]
CUDA: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/build/aten/src/ATen/RegisterCUDA.cpp:7100 [kernel]
QuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:641 [kernel]
BackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1614378098133/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Named: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
AutogradOther: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
AutogradCPU: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
AutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
AutogradXLA: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
AutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
UNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
AutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
AutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
AutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]
Tracer: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/torch/csrc/autograd/generated/TraceType_2.cpp:10525 [kernel]
Autocast: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/aten/src/ATen/autocast_mode.cpp:254 [kernel]
Batched: registered at /opt/conda/conda-bld/pytorch_1614378098133/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]
VmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1614378098133/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]

=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 19:37:26.861133] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 19:37:26.861184] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[ WARN:0@393.864] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/art_1.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 274, in train
    ltrain_images, ltarget_images,lillus,lgan_images = next(ltrain_ltarget_pair_loader)
  File "/home/xsx/dino/utils.py", line 968, in cycle4
    for images, targets,illu,real_images in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1199, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 281, in __getitem__
    train_image=self.transform(cv2.cvtColor(train_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 20:31:28.057112] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 20:31:28.057158] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[ WARN:0@8.386] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005220.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.388] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005492.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.397] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005224.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.412] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005199.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.458] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005200.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.487] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005500.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.507] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005203.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 274, in train
    ltrain_images, ltarget_images,lillus,lgan_images = next(ltrain_ltarget_pair_loader)
  File "/home/xsx/dino/utils.py", line 968, in cycle4
    for images, targets,illu,real_images in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1199, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 281, in __getitem__
    train_image=self.transform(cv2.cvtColor(train_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 20:33:33.588862] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 20:33:33.588909] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[ WARN:0@8.396] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005220.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.396] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005492.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.412] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005224.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.413] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005199.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.476] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005200.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.489] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005500.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.527] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005203.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 274, in train
    ltrain_images, ltarget_images,lillus,lgan_images = next(ltrain_ltarget_pair_loader)
  File "/home/xsx/dino/utils.py", line 968, in cycle4
    for images, targets,illu,real_images in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1199, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 281, in __getitem__
    train_image=self.transform(cv2.cvtColor(train_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 23:59:39.374812] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-22 23:59:39.374859] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[ WARN:0@8.246] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005220.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.246] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005492.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.263] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005224.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.293] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005199.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.319] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005200.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.361] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005203.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.380] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005500.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 274, in train
    ltrain_images, ltarget_images,lillus,lgan_images = next(ltrain_ltarget_pair_loader)
  File "/home/xsx/dino/utils.py", line 968, in cycle4
    for images, targets,illu,real_images in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1199, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 281, in __getitem__
    train_image=self.transform(cv2.cvtColor(train_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 00:01:51.022067] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 00:01:51.022117] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[ WARN:0@8.228] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005492.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.228] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005220.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.240] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005224.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.275] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005199.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.322] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005200.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.355] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005203.jpg'): can't open/read file: check file path/integrity
[ WARN:0@8.361] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005500.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 274, in train
    ltrain_images, ltarget_images,lillus,lgan_images = next(ltrain_ltarget_pair_loader)
  File "/home/xsx/dino/utils.py", line 968, in cycle4
    for images, targets,illu,real_images in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1199, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 281, in __getitem__
    train_image=self.transform(cv2.cvtColor(train_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 00:08:05.533935] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 00:08:05.533983] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[ WARN:0@9.043] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005220.jpg'): can't open/read file: check file path/integrity
[ WARN:0@9.045] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005492.jpg'): can't open/read file: check file path/integrity
[ WARN:0@9.056] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005224.jpg'): can't open/read file: check file path/integrity
[ WARN:0@9.091] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005199.jpg'): can't open/read file: check file path/integrity
[ WARN:0@9.116] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005200.jpg'): can't open/read file: check file path/integrity
[ WARN:0@9.140] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/train/0/ILSVRC2012_val_00005203.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 274, in train
    ltrain_images, ltarget_images,lillus,lgan_images = next(ltrain_ltarget_pair_loader)
  File "/home/xsx/dino/utils.py", line 968, in cycle4
    for images, targets,illu,real_images in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1199, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 281, in __getitem__
    train_image=self.transform(cv2.cvtColor(train_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 00:22:12.905209] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 00:22:12.905266] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[ WARN:0@4745.026] global loadsave.cpp:244 findDecoder imread_('data/labeled_data/val_target/0/ILSVRC2012_val_00005054.jpg'): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "train_stylegan2.py", line 646, in <module>
    worker(P)
  File "train_stylegan2.py", line 636, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 213, in train
    val_images,val_images128,val_target_images,val_illus=next(val_pair_loader)
  File "/home/xsx/dino/utils.py", line 968, in cycle4
    for images, targets,illu,real_images in dataloader:
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1199, in _next_data
    return self._process_data(data)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1225, in _process_data
    data.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
cv2.error: Caught error in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/xsx/dino/datasets.py", line 340, in __getitem__
    target_image=self.target_transform(cv2.cvtColor(target_image_cv2,cv2.COLOR_BGR2RGB))
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'


=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 07:55:55.734857] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 07:55:55.734905] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 08:41:29.583749] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 08:41:29.583799] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
图片类型为torch.float32和torch.float32
[2023-05-23 08:42:31.522016] [Steps   50002][G 1.9281678][G_critic 1.4997054] [G_l_mse 0.0042846] [G_ch_loss 0.00000695157269][DP 1.3808265][DP_real 0.0015465][DP_gen -0.0137426][DP_penalty 0.0243551][DS 1.3535882][DS_real 0.0502579][DS_gen -0.0188047][DS_penalty 0.0187109]
Steps 50003 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 08:47:53.067811] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 08:47:53.067880] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
1.0
1.2225065231323242
[2023-05-23 08:48:55.362929] [Steps   50002][G 1.9108542][G_critic 1.5071980] [G_l_mse 0.0040366] [G_ch_loss 0.00000000000000][DP 1.3638878][DP_real -0.0014137][DP_gen -0.0514536][DP_penalty 0.0189493][DS 1.3222880][DS_real 0.0380647][DS_gen -0.0977136][DS_penalty 0.0169592]
Steps 50003 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
1.0
1.1758700609207153
[2023-05-23 08:49:48.860771] [Steps   50004][G 2.3528175][G_critic 1.3837205] [G_l_mse 0.0096910] [G_ch_loss 0.00000000000000][DP 1.3493248][DP_real 0.0779429][DP_gen -0.0030439][DP_penalty 0.0227611][DS 1.3959584][DS_real -0.0887079][DS_gen -0.0779289][DS_penalty 0.0253909]
Steps 50005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 08:51:24.418850] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 08:51:24.418893] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 08:57:23.634754] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 08:57:23.634817] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-23 08:58:05.098356] [Steps   50002][stage2_val_SSIM 0.8449783][stage2_val_PSNR 21.4555298] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00734130945057]
[2023-05-23 08:58:26.206293] [Steps   50002][G 1.8341867][G_critic 1.4553566] [G_l_mse 0.0037883] [G_ch_loss 0.00000000000000][DP 1.3700519][DP_real 0.0396613][DP_gen 0.0011402][DP_penalty 0.0249898][DS 1.3127614][DS_real 0.0332578][DS_gen -0.1213193][DS_penalty 0.0072873]
Steps 50003 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 08:58:59.144459] [Steps   50004][stage2_val_SSIM 0.8437936][stage2_val_PSNR 21.3596404] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00749463262036]
[2023-05-23 08:59:20.574632] [Steps   50004][G 2.2918386][G_critic 1.3811333] [G_l_mse 0.0091071] [G_ch_loss 0.00000000000000][DP 1.3261099][DP_real 0.1000055][DP_gen -0.0273203][DP_penalty 0.0164783][DS 1.3810012][DS_real -0.0614606][DS_gen -0.0808850][DS_penalty 0.0213730]
Steps 50005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 08:59:53.001137] [Steps   50006][stage2_val_SSIM 0.8466480][stage2_val_PSNR 21.5328148] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00715520884842]
[2023-05-23 09:00:13.897159] [Steps   50006][G 2.1426418][G_critic 1.4137974] [G_l_mse 0.0072884] [G_ch_loss 0.00000000000000][DP 1.3051305][DP_real 0.1790950][DP_gen 0.0006906][DP_penalty 0.0384356][DS 1.3597786][DS_real 0.0541792][DS_gen -0.0026043][DS_penalty 0.0175801]
Steps 50007 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 09:00:45.897408] [Steps   50008][stage2_val_SSIM 0.8453632][stage2_val_PSNR 21.5107530] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00713634165004]
[2023-05-23 09:01:06.662390] [Steps   50008][G 2.0105226][G_critic 1.3609041] [G_l_mse 0.0064962] [G_ch_loss 0.00000000000000][DP 1.3510667][DP_real 0.0717478][DP_gen -0.0028523][DP_penalty 0.0162189][DS 1.3672845][DS_real 0.0959772][DS_gen 0.0508382][DS_penalty 0.0245445]
Steps 50009 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 09:01:38.882402] [Steps   50010][stage2_val_SSIM 0.8479788][stage2_val_PSNR 21.5140759] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00709055876359]
[2023-05-23 09:01:59.965222] [Steps   50010][G 2.1293316][G_critic 1.4537064] [G_l_mse 0.0067558] [G_ch_loss 0.00004700154022][DP 1.3266736][DP_real 0.0350601][DP_gen -0.0889147][DP_penalty 0.0081856][DS 1.3511111][DS_real -0.0052102][DS_gen -0.0802476][DS_penalty 0.0146333]
Steps 50011 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 09:02:32.205602] [Steps   50012][stage2_val_SSIM 0.8472465][stage2_val_PSNR 21.4589369] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00698336772621]
[2023-05-23 09:02:53.416939] [Steps   50012][G 2.2841105][G_critic 1.4997833] [G_l_mse 0.0078433] [G_ch_loss 0.00000000000000][DP 1.3099396][DP_real 0.1130264][DP_gen -0.0488438][DP_penalty 0.0212468][DS 1.3485005][DS_real 0.0664061][DS_gen -0.0177926][DS_penalty 0.0190122]
Steps 50013 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 09:03:25.667844] [Steps   50014][stage2_val_SSIM 0.8480559][stage2_val_PSNR 21.3069290] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00710636423901]
[2023-05-23 09:03:46.626942] [Steps   50014][G 2.0386052][G_critic 1.3694778] [G_l_mse 0.0066913] [G_ch_loss 0.00000000000000][DP 1.3601940][DP_real 0.0378135][DP_gen -0.0191562][DP_penalty 0.0225391][DS 1.3618720][DS_real 0.0844648][DS_gen 0.0256105][DS_penalty 0.0202935]
Steps 50015 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 09:04:19.023847] [Steps   50016][stage2_val_SSIM 0.8458681][stage2_val_PSNR 21.4716181] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00729388743639]
[2023-05-23 09:04:39.785564] [Steps   50016][G 2.3325505][G_critic 1.5319476] [G_l_mse 0.0079899] [G_ch_loss 0.00161132751964][DP 1.2803321][DP_real 0.1099126][DP_gen -0.1174887][DP_penalty 0.0347120][DS 1.3517952][DS_real 0.0476102][DS_gen -0.0311413][DS_penalty 0.0589659]
Steps 50017 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 09:05:12.115041] [Steps   50018][stage2_val_SSIM 0.8479419][stage2_val_PSNR 21.4527368] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00717816920951]
[2023-05-23 09:05:32.905666] [Steps   50018][G 1.9317356][G_critic 1.4544222] [G_l_mse 0.0047731] [G_ch_loss 0.00000000000000][DP 1.3554189][DP_real 0.1036492][DP_gen 0.0270409][DP_penalty 0.0175817][DS 1.3582436][DS_real 0.0994488][DS_gen 0.0346569][DS_penalty 0.0249809]
Steps 50019 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 09:06:37.384339] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 09:06:37.384418] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-23 09:07:17.298445] [Steps   50002][stage2_val_SSIM 0.8348651][stage2_val_PSNR 21.2339672] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00746592693031]
[2023-05-23 09:07:38.262451] [Steps   50002][G 1.8597829][G_critic 1.4633592] [G_l_mse 0.0039642] [G_ch_loss 0.00000000000000][DP 1.3668168][DP_real 0.0232259][DP_gen -0.0213836][DP_penalty 0.0214633][DS 1.3128371][DS_real 0.0273715][DS_gen -0.1284764][DS_penalty 0.0138530]
Steps 50003 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 09:08:12.177587] [Steps   50004][stage2_val_SSIM 0.8336797][stage2_val_PSNR 21.1683444] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00759203219786]
[2023-05-23 09:08:33.185145] [Steps   50004][G 2.2612281][G_critic 1.3830053] [G_l_mse 0.0087822] [G_ch_loss 0.00000000000000][DP 1.3127611][DP_real 0.1223323][DP_gen -0.0343390][DP_penalty 0.0213444][DS 1.3655128][DS_real -0.0568265][DS_gen -0.1067795][DS_penalty 0.0089225]
Steps 50005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-23 09:09:05.186578] [Steps   50006][stage2_val_SSIM 0.8338952][stage2_val_PSNR 21.0680152] [stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00734342914075]
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 09:10:44.674247] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 09:10:44.674307] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 81, in <module>
    model = InceptionV3([block_idx]).cuda()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 491, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 409, in _apply
    param_applied = fn(param)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 491, in <lambda>
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
Traceback (most recent call last):
  File "train_stylegan2.py", line 685, in <module>
    worker(P)
  File "train_stylegan2.py", line 564, in worker
    state_G = torch.load(f"{P.resume}/gen_stage3.pt")
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 592, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 851, in _load
    result = unpickler.load()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 843, in persistent_load
    load_tensor(data_type, size, key, _maybe_decode_ascii(location))
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 832, in load_tensor
    loaded_storages[key] = restore_location(storage, location)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 175, in default_restore_location
    result = fn(storage, location)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/serialization.py", line 157, in _cuda_deserialize
    return obj.cuda(device)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 80, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/cuda/__init__.py", line 484, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 10:20:38.670274] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 10:20:38.670338] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Current run is terminating due to exception: mat1 and mat2 shapes cannot be multiplied (18816x128 and 4x2)
Engine run is terminating due to exception: mat1 and mat2 shapes cannot be multiplied (18816x128 and 4x2)
Traceback (most recent call last):
  File "train_stylegan2.py", line 685, in <module>
    worker(P)
  File "train_stylegan2.py", line 675, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 260, in train
    state = default_evaluator.run([[val_gen_images,val_target_images]])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 892, in run
    return self._internal_run()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 935, in _internal_run
    return next(self._internal_run_generator)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 993, in _internal_run_as_gen
    self._handle_exception(e)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 638, in _handle_exception
    raise e
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 959, in _internal_run_as_gen
    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 1087, in _run_once_on_dataset_as_gen
    self._handle_exception(e)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 638, in _handle_exception
    raise e
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 1069, in _run_once_on_dataset_as_gen
    self._fire_event(Events.ITERATION_COMPLETED)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 425, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/metrics/metric.py", line 310, in iteration_completed
    self.update(output)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/metrics/metric.py", line 607, in wrapper
    func(self, *args, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/metrics/gan/fid.py", line 236, in update
    train_features = self._extract_features(train)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/metrics/gan/utils.py", line 101, in _extract_features
    outputs = self._feature_extractor(inputs).to(self._device, dtype=torch.float64)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (18816x128 and 4x2)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 10:26:50.711977] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 10:26:50.712043] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Current run is terminating due to exception: mat1 and mat2 shapes cannot be multiplied (18816x128 and 4x2)
Engine run is terminating due to exception: mat1 and mat2 shapes cannot be multiplied (18816x128 and 4x2)
Traceback (most recent call last):
  File "train_stylegan2.py", line 685, in <module>
    worker(P)
  File "train_stylegan2.py", line 675, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 260, in train
    state = default_evaluator.run([[val_gen_images,val_target_images]])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 892, in run
    return self._internal_run()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 935, in _internal_run
    return next(self._internal_run_generator)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 993, in _internal_run_as_gen
    self._handle_exception(e)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 638, in _handle_exception
    raise e
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 959, in _internal_run_as_gen
    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 1087, in _run_once_on_dataset_as_gen
    self._handle_exception(e)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 638, in _handle_exception
    raise e
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 1069, in _run_once_on_dataset_as_gen
    self._fire_event(Events.ITERATION_COMPLETED)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/engine/engine.py", line 425, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/metrics/metric.py", line 310, in iteration_completed
    self.update(output)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/metrics/metric.py", line 607, in wrapper
    func(self, *args, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/metrics/gan/fid.py", line 236, in update
    train_features = self._extract_features(train)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/ignite/metrics/gan/utils.py", line 101, in _extract_features
    outputs = self._feature_extractor(inputs).to(self._device, dtype=torch.float64)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (18816x128 and 4x2)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 10:29:44.502958] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 10:29:44.503003] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-23 10:30:24.954084] [Steps   50002][stage2_val_SSIM 0.8423223][stage2_val_PSNR 21.3114691][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00767950434238]
[2023-05-23 10:30:46.067359] [Steps   50002][G 1.8267984][G_critic 1.4666014] [G_l_mse 0.0036020] [G_ch_loss 0.00000000000000][DP 1.3685874][DP_real 0.0128087][DP_gen -0.0296312][DP_penalty 0.0236819][DS 1.2962661][DS_real 0.0516478][DS_gen -0.1390681][DS_penalty 0.0153398]
Steps 50003 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 10:35:34.814440] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 10:35:34.814486] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-23 17:07:08.604231] [Steps   51000][stage2_val_SSIM 0.8315949][stage2_val_PSNR 20.7578300][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.00976493488997]
[2023-05-23 17:07:34.321077] [Steps   51000][G 1.9438624][G_critic 1.4446394] [G_l_mse 0.0049814] [G_ch_loss 0.00108244293369][DP 1.2767000][DP_real 0.1847001][DP_gen -0.0627556][DP_penalty 0.0655310][DS 1.3654200][DS_real 0.0539046][DS_gen 0.0052921][DS_penalty 0.0142849]
Steps 51001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 19:38:19.659513] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 19:38:19.659585] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 51001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 686, in <module>
    worker(P)
  File "train_stylegan2.py", line 676, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 367, in train
    loss.backward()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 11.77 GiB total capacity; 9.85 GiB already allocated; 13.50 MiB free; 10.59 GiB reserved in total by PyTorch)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 20:05:54.464389] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-23 20:05:54.464439] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 51001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-24 02:35:51.869818] [Steps   52000][stage2_val_SSIM 0.8243460][stage2_val_PSNR 20.3276659][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01100752782077]
[2023-05-24 02:36:16.495245] [Steps   52000][G 1.8858188][G_critic 1.4472062] [G_l_mse 0.0043802] [G_ch_loss 0.00059534923639][DP 1.2359079][DP_real 0.2403385][DP_gen -0.1151448][DP_penalty 0.1188940][DS 1.3598425][DS_real 0.0609490][DS_gen -0.0011355][DS_penalty 0.0115554]
Steps 52001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-24 09:08:15.202291] [Steps   53000][stage2_val_SSIM 0.8256527][stage2_val_PSNR 20.2365741][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01173392403871]
[2023-05-24 09:08:38.648766] [Steps   53000][G 1.7795066][G_critic 1.4280536] [G_l_mse 0.0035138] [G_ch_loss 0.00007244526932][DP 1.2862233][DP_real 0.1586226][DP_gen -0.0724252][DP_penalty 0.0531930][DS 1.3714556][DS_real 0.0587925][DS_gen 0.0167087][DS_penalty 0.0106210]
Steps 53001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
Traceback (most recent call last):
  File "train_stylegan2.py", line 686, in <module>
    # ltarget_pair_loader=ltarget_pair_loader,
  File "train_stylegan2.py", line 676, in worker
    generator.sample_latent = generator.module.sample_latent
  File "train_stylegan2.py", line 367, in train
    #    u_img_input.requires_grad = True
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 11.77 GiB total capacity; 9.90 GiB already allocated; 89.50 MiB free; 10.52 GiB reserved in total by PyTorch)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:25:40.238679] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:25:40.238728] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 703, in <module>
    worker(P)
  File "train_stylegan2.py", line 693, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 442, in train
    writer.add_scalars('stage2_G_D',{'g_loss': g_loss.item(), 'd_loss': loss.item()}, step)
NameError: name 'loss' is not defined
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:28:27.213801] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:28:27.213855] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 684, in <module>
    worker(P)
  File "train_stylegan2.py", line 674, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 444, in train
    writer.add_scalars('stage2_DP',{'dp_loss': losses['DP_loss'][-1], 'dp_penalty_loss': losses['DP_penalty'][-1]}, step)
IndexError: list index out of range
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:30:58.313386] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:30:58.313433] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:32:48.384180] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:32:48.384234] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-24 09:33:25.008427] [Steps   53002][stage2_val_SSIM 0.8274070][stage2_val_PSNR 20.3173347][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01137053873390]
[2023-05-24 09:33:41.781552] [Steps   53002][G 1.8172314][G_critic 1.5772145] [G_l_mse 0.0024002] [G_ch_loss 0.00000000000000][DP 1.2460351][DP_real 0.2023924][DP_gen -0.0996705][DP_penalty 0.0235327][DS 1.3349230][DS_real -0.0096721][DS_gen -0.1342851][DS_penalty 0.0191813]
Steps 53003 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:36:03.101264] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:36:03.101321] Use G moving average: 0.9999937616948332
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53003 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:39:27.019135] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:39:27.019195] Use G moving average: 0.9999937616948332
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53003 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-24 09:39:59.663361] [Steps   53004][stage2_val_SSIM 0.8271179][stage2_val_PSNR 20.1999449][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01136745791882]
[2023-05-24 09:40:16.420691] [Steps   53004][G 1.7632198][G_critic 1.5218672] [G_l_mse 0.0024121] [G_ch_loss 0.00014385976829][DP 1.2104027][DP_real 0.1756232][DP_gen -0.2075089][DP_penalty 0.0510207][DS 1.3306558][DS_real -0.0082085][DS_gen -0.1474598][DS_penalty 0.0174498]
Steps 53005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-24 09:40:44.060741] [Steps   53006][stage2_val_SSIM 0.8279605][stage2_val_PSNR 20.2469017][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01142912916839]
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:47:43.856571] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:47:43.856620] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 686, in <module>
    worker(P)
  File "train_stylegan2.py", line 676, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 429, in train
    gs_loss ,gs_aux= train_fn["train3_G_match"](P, discriminator_single, opt,ugen_images,target_images,ugen_images)
  File "/home/xsx/dino/training/gan/aug_both.py", line 226, in loss_G_my_match_fn
    g_critic_loss=loss_G_match_fn(P, D,options,u_img_input)
  File "/home/xsx/dino/training/gan/aug_both.py", line 95, in loss_G_match_fn
    d_gen = D(P.augment_fn(u_img_input))
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/models/gan/base.py", line 147, in forward
    features = self.penultimate(inputs)#
  File "/home/xsx/dino/mydiscriminator.py", line 295, in penultimate
    out = self.layers(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/models/gan/stylegan2/op/fused_act.py", line 109, in forward
    return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)
  File "/home/xsx/dino/models/gan/stylegan2/op/fused_act.py", line 114, in fused_leaky_relu
    negative_slope=negative_slope)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/functional.py", line 1378, in leaky_relu
    result = torch._C._nn.leaky_relu(input, negative_slope)
RuntimeError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 11.77 GiB total capacity; 10.48 GiB already allocated; 35.50 MiB free; 10.57 GiB reserved in total by PyTorch)

=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:50:47.434017] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 09:50:47.434062] Use G moving average: 0.9999937616948332
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 10:20:40.767049] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 10:20:40.767103] Use G moving average: 0.9999937616948332
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 10:41:53.410630] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 10:41:53.410684] Use G moving average: 0.9999937616948332
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 693, in <module>
    worker(P)
  File "train_stylegan2.py", line 683, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 432, in train
    gp_loss ,gp_aux= train_fn["train3_G_match"](P, discriminator_pair, opt,lgen_images,ltarget_images,u_img_input)#,ugen_images)
  File "/home/xsx/dino/training/gan/aug_both.py", line 226, in loss_G_my_match_fn
    g_critic_loss=loss_G_match_fn(P, D,options,u_img_input)
  File "/home/xsx/dino/training/gan/aug_both.py", line 95, in loss_G_match_fn
    d_gen = D(P.augment_fn(u_img_input))
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/models/gan/base.py", line 147, in forward
    features = self.penultimate(inputs)#
  File "/home/xsx/dino/mydiscriminator.py", line 295, in penultimate
    out = self.layers(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/mydiscriminator.py", line 133, in forward
    skip = self.skip(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/xsx/dino/models/gan/stylegan2/layers.py", line 124, in forward
    out = upfirdn2d(input, self.kernel, pad=self.pad)
  File "/home/xsx/dino/models/gan/stylegan2/op/upfirdn2d.py", line 214, in upfirdn2d
    out = upfirdn2d_native(input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1])
  File "/home/xsx/dino/models/gan/stylegan2/op/upfirdn2d.py", line 238, in upfirdn2d_native
    [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.77 GiB total capacity; 10.45 GiB already allocated; 19.50 MiB free; 10.59 GiB reserved in total by PyTorch)

=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 10:44:34.175225] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-24 10:44:34.175271] Use G moving average: 0.9999937616948332
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 53005 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-24 17:06:39.881024] [Steps   54000][stage2_val_SSIM 0.8236363][stage2_val_PSNR 20.1626752][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01222285348922]
[2023-05-24 17:07:02.864364] [Steps   54000][G 1.7495580][G_critic 1.4853328] [G_l_mse 0.0026423] [G_ch_loss 0.00000000000000][DP 1.2318976][DP_real 0.2653921][DP_gen -0.0798926][DP_penalty 0.0306888][DS 1.3739736][DS_real 0.0196438][DS_gen -0.0180631][DS_penalty 0.0172793]
Steps 54001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-24 23:31:33.470689] [Steps   55000][stage2_val_SSIM 0.8271971][stage2_val_PSNR 20.3601313][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01145298779011]
[2023-05-24 23:31:56.449377] [Steps   55000][G 1.7969468][G_critic 1.5191452] [G_l_mse 0.0027768] [G_ch_loss 0.00011814009486][DP 1.2826724][DP_real 0.0484076][DP_gen -0.1858684][DP_penalty 0.0601888][DS 1.3061836][DS_real 0.0524323][DS_gen -0.1365267][DS_penalty 0.0095202]
Steps 55001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-25 05:55:09.722205] [Steps   56000][stage2_val_SSIM 0.8211742][stage2_val_PSNR 20.1517165][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01227646134794]
[2023-05-25 05:55:32.698632] [Steps   56000][G 2.0451958][G_critic 1.5612136] [G_l_mse 0.0048396] [G_ch_loss 0.00002484996912][DP 1.1854599][DP_real 0.3592797][DP_gen -0.1026669][DP_penalty 0.0508680][DS 1.3529708][DS_real 0.1459187][DS_gen 0.0607712][DS_penalty 0.0207030]
Steps 56001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-25 12:19:20.403151] [Steps   57000][stage2_val_SSIM 0.8277637][stage2_val_PSNR 20.2831783][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01140263956040]
[2023-05-25 12:19:44.068526] [Steps   57000][G 1.8788722][G_critic 1.4748387] [G_l_mse 0.0040403] [G_ch_loss 0.00000000000000][DP 1.1791027][DP_real 0.3028134][DP_gen -0.1636459][DP_penalty 0.0242895][DS 1.3022215][DS_real 0.0049557][DS_gen -0.1855028][DS_penalty 0.0474004]
Steps 57001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
Traceback (most recent call last):
  File "train_stylegan2.py", line 693, in <module>
  File "train_stylegan2.py", line 683, in worker
    
  File "train_stylegan2.py", line 405, in train
    opt_DP.step()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 11.77 GiB total capacity; 9.89 GiB already allocated; 73.50 MiB free; 10.53 GiB reserved in total by PyTorch)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
usage: train_stylegan2.py [-h] [--mode MODE] [--penalty PENALTY] [--aug AUG]
                          [--use_warmup] [--workers N] [--temp TEMP]
                          [--lbd_a LBD_A] [--no_lazy]
                          [--d_reg_every D_REG_EVERY] [--lbd_r1 LBD_R1]
                          [--style_mix STYLE_MIX] [--halflife_k HALFLIFE_K]
                          [--ema_start_k EMA_START_K]
                          [--halflife_lr HALFLIFE_LR] [--use_nerf_proj]
                          [--no_fid] [--no_gif] [--n_eval_avg N_EVAL_AVG]
                          [--print_every PRINT_EVERY]
                          [--evaluate_every EVALUATE_EVERY]
                          [--save_every SAVE_EVERY] [--comment COMMENT]
                          [--resume RESUME] [--finetune FINETUNE]
                          gin_config architecture
train_stylegan2.py: error: argument --print_every: invalid int value: ''
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-25 14:32:26.362861] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-25 14:32:26.362930] Use G moving average: 0.9999937616948332
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 57001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Traceback (most recent call last):
  File "train_stylegan2.py", line 692, in <module>
    worker(P)
  File "train_stylegan2.py", line 682, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 404, in train
    loss.backward()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 11.77 GiB total capacity; 9.73 GiB already allocated; 45.50 MiB free; 10.56 GiB reserved in total by PyTorch)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-25 14:45:47.638341] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-25 14:45:47.638417] Use G moving average: 0.99999445483793
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 57001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-25 21:18:22.242380] [Steps   58000][stage2_val_SSIM 0.8267176][stage2_val_PSNR 20.1190837][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01175932306796]
[2023-05-25 21:18:46.168829] [Steps   58000][G 2.0793414][G_critic 1.6218309] [G_l_mse 0.0045751] [G_ch_loss 0.00000000000000][DP 1.0746603][DP_real 0.3299022][DP_gen -0.3849675][DP_penalty 0.0709535][DS 1.3580706][DS_real -0.0393170][DS_gen -0.1653780][DS_penalty 0.0773956]
Steps 58001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
Traceback (most recent call last):
  File "train_stylegan2.py", line 692, in <module>
    worker(P)
  File "train_stylegan2.py", line 682, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 404, in train
    loss.backward()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 11.77 GiB total capacity; 9.87 GiB already allocated; 83.50 MiB free; 10.52 GiB reserved in total by PyTorch)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-26 07:49:56.924889] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-26 07:49:56.924960] Use G moving average: 0.99999445483793
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 58001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-26 07:51:49.629089] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-26 07:51:49.629141] Use G moving average: 0.99999445483793
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 58001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-26 14:23:39.433583] [Steps   59000][stage2_val_SSIM 0.8240393][stage2_val_PSNR 20.0286006][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01184823270887]
[2023-05-26 14:24:03.518035] [Steps   59000][G 2.1018577][G_critic 1.6438585] [G_l_mse 0.0045800] [G_ch_loss 0.00000000000000][DP 1.0573864][DP_real 0.3561148][DP_gen -0.4067101][DP_penalty 0.0803633][DS 1.3620285][DS_real -0.0459457][DS_gen -0.1572911][DS_penalty 0.0696814]
Steps 59001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-26 20:48:04.264349] [Steps   60000][stage2_val_SSIM 0.8245081][stage2_val_PSNR 20.2565894][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01164123974741]
[2023-05-26 20:48:28.223415] [Steps   60000][G 2.0646014][G_critic 1.6758710] [G_l_mse 0.0038873] [G_ch_loss 0.00000000000000][DP 1.0772023][DP_real 0.3428093][DP_gen -0.3747485][DP_penalty 0.0476822][DS 1.3148365][DS_real 0.1615285][DS_gen 0.0044317][DS_penalty 0.0075312]
Steps 60001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-27 03:15:38.457039] [Steps   61000][stage2_val_SSIM 0.8195628][stage2_val_PSNR 20.1180004][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01201539393514]
[2023-05-27 03:16:01.176797] [Steps   61000][G 2.0454471][G_critic 1.6211062] [G_l_mse 0.0042416] [G_ch_loss 0.00018435725360][DP 1.1442955][DP_real 0.1681172][DP_gen -0.4210401][DP_penalty 0.2036078][DS 1.3796589][DS_real -0.0636060][DS_gen -0.0909407][DS_penalty 0.0454042]
Steps 61001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-27 09:37:06.409341] [Steps   62000][stage2_val_SSIM 0.8310882][stage2_val_PSNR 20.2409375][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01075121853501]
[2023-05-27 09:37:29.153347] [Steps   62000][G 2.2923751][G_critic 1.7110403] [G_l_mse 0.0058133] [G_ch_loss 0.00000000000000][DP 1.2962023][DP_real 0.1377599][DP_gen -0.0711551][DP_penalty 0.1384204][DS 1.3379236][DS_real 0.0928317][DS_gen -0.0288145][DS_penalty 0.0336942]
Steps 62001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-27 15:57:50.968931] [Steps   63000][stage2_val_SSIM 0.8233272][stage2_val_PSNR 20.1353169][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01219112426043]
[2023-05-27 15:58:13.803571] [Steps   63000][G 2.1717286][G_critic 1.5802791] [G_l_mse 0.0059145] [G_ch_loss 0.00000000000000][DP 1.1828281][DP_real 0.0805772][DP_gen -0.4065075][DP_penalty 0.1163668][DS 1.3023832][DS_real 0.1055982][DS_gen -0.0851892][DS_penalty 0.0096886]
Steps 63001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-27 22:36:17.517947] [Steps   64000][stage2_val_SSIM 0.8256524][stage2_val_PSNR 20.0588317][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01167871616781]
[2023-05-27 22:36:41.321599] [Steps   64000][G 2.3432782][G_critic 1.8460618] [G_l_mse 0.0049722] [G_ch_loss 0.00000000000000][DP 1.0638435][DP_real 0.1350921][DP_gen -0.6675045][DP_penalty 0.0391262][DS 1.3674808][DS_real 0.0276459][DS_gen -0.0312066][DS_penalty 0.0324628]
Steps 64001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-28 04:58:45.702581] [Steps   65000][stage2_val_SSIM 0.8243762][stage2_val_PSNR 20.1937945][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01211512461305]
[2023-05-28 04:59:08.409284] [Steps   65000][G 1.8891132][G_critic 1.5407326] [G_l_mse 0.0034838] [G_ch_loss 0.00000000000000][DP 1.1893423][DP_real 0.2570475][DP_gen -0.2243949][DP_penalty 0.0921485][DS 1.3153491][DS_real 0.0132270][DS_gen -0.1658680][DS_penalty 0.0256995]
Steps 65001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-28 11:20:00.719900] [Steps   66000][stage2_val_SSIM 0.8291013][stage2_val_PSNR 20.3025978][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01145471539348]
[2023-05-28 11:20:23.529306] [Steps   66000][G 1.8996673][G_critic 1.5875945] [G_l_mse 0.0031190] [G_ch_loss 0.00017444438708][DP 1.1646900][DP_real 0.2455195][DP_gen -0.2609278][DP_penalty 0.1285354][DS 1.3235397][DS_real 0.1145765][DS_gen -0.0327743][DS_penalty 0.0121465]
Steps 66001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-28 17:42:31.392632] [Steps   67000][stage2_val_SSIM 0.8319245][stage2_val_PSNR 20.1849803][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01172483339906]
[2023-05-28 17:42:54.683730] [Steps   67000][G 2.0632796][G_critic 1.7345489] [G_l_mse 0.0032858] [G_ch_loss 0.00014882127289][DP 1.1013429][DP_real 0.4948473][DP_gen -0.1905379][DP_penalty 0.1452550][DS 1.2786406][DS_real 0.0816950][DS_gen -0.1809557][DS_penalty 0.0158767]
Steps 67001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-29 00:15:27.686129] [Steps   68000][stage2_val_SSIM 0.8313064][stage2_val_PSNR 20.1922610][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01173312310129]
[2023-05-29 00:15:51.951458] [Steps   68000][G 1.7508945][G_critic 1.4618577] [G_l_mse 0.0028581] [G_ch_loss 0.00322265550494][DP 1.1624625][DP_real 0.3840621][DP_gen -0.1335831][DP_penalty 0.1142696][DS 1.3878707][DS_real 0.0750363][DS_gen 0.0508493][DS_penalty 0.0161111]
Steps 68001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-29 19:09:51.859852] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-29 19:09:51.859917] Use G moving average: 0.99999445483793
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 68001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-30 01:35:49.506852] [Steps   69000][stage2_val_SSIM 0.8278199][stage2_val_PSNR 20.0370213][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01232382003218]
[2023-05-30 01:36:12.202055] [Steps   69000][G 2.1538191][G_critic 1.7363350] [G_l_mse 0.0041748] [G_ch_loss 0.00000000000000][DP 1.0059142][DP_real 0.4166677][DP_gen -0.4905338][DP_penalty 0.1178743][DS 1.3537716][DS_real -0.0271772][DS_gen -0.1607955][DS_penalty 0.0452427]
Steps 69001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-30 08:01:23.747292] [Steps   70000][stage2_val_SSIM 0.8285114][stage2_val_PSNR 20.0927281][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01214102935046]
[2023-05-30 08:01:46.514724] [Steps   70000][G 2.0674679][G_critic 1.6771794] [G_l_mse 0.0039029] [G_ch_loss 0.00000000000000][DP 1.0248125][DP_real 0.4208509][DP_gen -0.4201381][DP_penalty 0.0567126][DS 1.2861216][DS_real 0.2074033][DS_gen -0.0105731][DS_penalty 0.0104583]
Steps 70001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
Traceback (most recent call last):
  File "train_stylegan2.py", line 692, in <module>
    worker(P)
  File "train_stylegan2.py", line 682, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 404, in train
    loss.backward()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 11.77 GiB total capacity; 9.87 GiB already allocated; 99.50 MiB free; 10.51 GiB reserved in total by PyTorch)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-30 08:20:06.529193] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-30 08:20:06.529382] Use G moving average: 0.99999445483793
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 70001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-30 14:44:39.142552] [Steps   71000][stage2_val_SSIM 0.8268968][stage2_val_PSNR 20.0663233][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01229480281472]
[2023-05-30 14:45:03.261854] [Steps   71000][G 2.1864572][G_critic 1.7579262] [G_l_mse 0.0042853] [G_ch_loss 0.00000000000000][DP 0.9942529][DP_real 0.4315727][DP_gen -0.5131595][DP_penalty 0.1344882][DS 1.3668675][DS_real -0.0357409][DS_gen -0.1399920][DS_penalty 0.0452865]
Steps 71001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-30 15:20:36.585650] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-30 15:20:36.585720] Use G moving average: 0.99999445483793
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 71001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.99s/it]100%|██████████| 1/1 [00:02<00:00,  2.99s/it][2023-05-30 16:40:56.874391] [Steps   71200][stage2_val_SSIM 0.8278059][stage2_val_PSNR 20.3397831][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01168150175363]
[2023-05-30 16:41:21.033782] [Steps   71200][G 1.9672378][G_critic 1.6891378] [G_l_mse 0.0027644] [G_ch_loss 0.00166015618015][DP 1.0735257][DP_real 0.3480016][DP_gen -0.4115487][DP_penalty 0.0732095][DS 1.3132621][DS_real 0.0965608][DS_gen -0.0612776][DS_penalty 0.0176528]
Steps 71201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)

Traceback (most recent call last):
  File "train_stylegan2.py", line 696, in <module>
    worker(P)
  File "train_stylegan2.py", line 686, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 504, in train
    logger.log('[Steps %7d][fid_score %.7f]' %(step, metrics['fid_score'][-1]))
IndexError: list index out of range
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-30 17:24:43.039162] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-30 17:24:43.039290] Use G moving average: 0.99999445483793
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 71001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.82s/it]100%|██████████| 1/1 [00:02<00:00,  2.82s/it][2023-05-30 18:41:13.964501] [Steps   71200][stage2_val_SSIM 0.8177435][stage2_val_PSNR 19.8236751][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01244103815407]
[2023-05-30 18:41:36.709895] [Steps   71200][G 2.0058258][G_critic 1.6733941] [G_l_mse 0.0033073] [G_ch_loss 0.00169974577148][DP 1.0336847][DP_real 0.3862266][DP_gen -0.4582783][DP_penalty 0.0679630][DS 1.3136852][DS_real 0.1079522][DS_gen -0.0484947][DS_penalty 0.0155733]
Steps 71201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-30 18:41:36.721302] [Steps   71200][fid_score 5.6730991]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.86s/it]100%|██████████| 1/1 [00:02<00:00,  2.86s/it][2023-05-30 19:57:58.529355] [Steps   71400][stage2_val_SSIM 0.8346519][stage2_val_PSNR 20.5582234][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01096659060568]
[2023-05-30 19:58:21.320824] [Steps   71400][G 1.9576013][G_critic 1.6107038] [G_l_mse 0.0034658] [G_ch_loss 0.00032182261930][DP 1.1680197][DP_real 0.2912278][DP_gen -0.2164027][DP_penalty 0.0634081][DS 1.2775255][DS_real 0.1118265][DS_gen -0.1437777][DS_penalty 0.0191566]
Steps 71401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-30 19:58:21.331568] [Steps   71400][fid_score 5.0225622]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.83s/it]100%|██████████| 1/1 [00:02<00:00,  2.83s/it][2023-05-30 21:14:34.761451] [Steps   71600][stage2_val_SSIM 0.8311746][stage2_val_PSNR 20.3145391][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01179023925215]
[2023-05-30 21:14:57.394899] [Steps   71600][G 1.8989201][G_critic 1.6213123] [G_l_mse 0.0027761] [G_ch_loss 0.00000000000000][DP 1.1027454][DP_real 0.2848150][DP_gen -0.3723607][DP_penalty 0.0709961][DS 1.3243265][DS_real 0.0475384][DS_gen -0.0971925][DS_penalty 0.0218272]
Steps 71601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-30 21:14:57.405749] [Steps   71600][fid_score 5.1590193]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.89s/it]100%|██████████| 1/1 [00:02<00:00,  2.89s/it][2023-05-30 22:31:50.315231] [Steps   71800][stage2_val_SSIM 0.8283595][stage2_val_PSNR 20.3918857][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01262182369828]
[2023-05-30 22:32:13.106646] [Steps   71800][G 1.9314021][G_critic 1.6092523] [G_l_mse 0.0032215] [G_ch_loss 0.00000000000000][DP 1.2039870][DP_real 0.5706093][DP_gen 0.0574357][DP_penalty 0.1116597][DS 1.3297789][DS_real -0.0725015][DS_gen -0.2230646][DS_penalty 0.0282899]
Steps 71801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-30 22:32:13.118100] [Steps   71800][fid_score 4.8567739]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.79s/it]100%|██████████| 1/1 [00:02<00:00,  2.80s/it][2023-05-30 23:49:09.174074] [Steps   72000][stage2_val_SSIM 0.8259176][stage2_val_PSNR 19.9766982][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01257272996008]
[2023-05-30 23:49:33.052733] [Steps   72000][G 2.2926521][G_critic 1.8099399] [G_l_mse 0.0047136] [G_ch_loss 0.01135253533721][DP 1.0410434][DP_real 0.2754674][DP_gen -0.5252806][DP_penalty 0.0323606][DS 1.3640623][DS_real -0.0082609][DS_gen -0.1021095][DS_penalty 0.0620351]
Steps 72001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-30 23:49:33.063766] [Steps   72000][fid_score 4.9528719]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.83s/it]100%|██████████| 1/1 [00:02<00:00,  2.83s/it][2023-05-31 01:06:28.154722] [Steps   72200][stage2_val_SSIM 0.8298833][stage2_val_PSNR 20.1737331][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01211004145443]
[2023-05-31 01:06:50.277122] [Steps   72200][G 2.5386028][G_critic 1.7742544] [G_l_mse 0.0076435] [G_ch_loss 0.00000000000000][DP 1.0772581][DP_real 0.1769949][DP_gen -0.5928446][DP_penalty 0.0576864][DS 1.3029935][DS_real 0.1209919][DS_gen -0.0631186][DS_penalty 0.0087734]
Steps 72201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 01:06:50.288538] [Steps   72200][fid_score 5.3246429]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.81s/it]100%|██████████| 1/1 [00:02<00:00,  2.81s/it][2023-05-31 02:24:07.579139] [Steps   72400][stage2_val_SSIM 0.8298570][stage2_val_PSNR 20.0148756][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01194638293236]
[2023-05-31 02:24:31.593702] [Steps   72400][G 2.1392112][G_critic 1.7401467] [G_l_mse 0.0039906] [G_ch_loss 0.00000000000000][DP 1.1338671][DP_real 0.2944668][DP_gen -0.3383411][DP_penalty 0.0806706][DS 1.2355707][DS_real 0.2441719][DS_gen -0.1132417][DS_penalty 0.0393651]
Steps 72401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 02:24:31.604687] [Steps   72400][fid_score 6.1222288]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.79s/it]100%|██████████| 1/1 [00:02<00:00,  2.79s/it][2023-05-31 03:41:24.678973] [Steps   72600][stage2_val_SSIM 0.8234843][stage2_val_PSNR 19.9077835][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01242056488991]
[2023-05-31 03:41:47.489604] [Steps   72600][G 2.0130525][G_critic 1.6193703] [G_l_mse 0.0028661] [G_ch_loss 0.10706785321236][DP 1.0817213][DP_real 0.2881317][DP_gen -0.4125259][DP_penalty 0.0951254][DS 1.3386585][DS_real 0.1076666][DS_gen 0.0050084][DS_penalty 0.0245510]
Steps 72601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 03:41:47.498501] [Steps   72600][fid_score 6.0379716]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.91s/it]100%|██████████| 1/1 [00:02<00:00,  2.91s/it][2023-05-31 04:58:32.433711] [Steps   72800][stage2_val_SSIM 0.8271428][stage2_val_PSNR 20.2306000][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01214423496276]
[2023-05-31 04:58:54.865420] [Steps   72800][G 2.1652722][G_critic 1.8915332] [G_l_mse 0.0027374] [G_ch_loss 0.00000000000000][DP 0.9899938][DP_real 0.3582871][DP_gen -0.5981861][DP_penalty 0.0747322][DS 1.2535930][DS_real 0.1057842][DS_gen -0.1840843][DS_penalty 0.0716371]
Steps 72801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 04:58:54.875867] [Steps   72800][fid_score 5.4402378]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.77s/it]100%|██████████| 1/1 [00:02<00:00,  2.77s/it][2023-05-31 06:15:38.343209] [Steps   73000][stage2_val_SSIM 0.8299022][stage2_val_PSNR 19.9894917][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01210020296276]
[2023-05-31 06:16:00.964399] [Steps   73000][G 1.9103212][G_critic 1.5672006] [G_l_mse 0.0034312] [G_ch_loss 0.00000000000000][DP 1.0427405][DP_real 0.4162600][DP_gen -0.4100736][DP_penalty 0.1351780][DS 1.3351315][DS_real 0.1000382][DS_gen -0.0165179][DS_penalty 0.0135867]
Steps 73001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 06:16:00.974490] [Steps   73000][fid_score 4.7916038]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.87s/it]100%|██████████| 1/1 [00:02<00:00,  2.87s/it][2023-05-31 07:32:55.102656] [Steps   73200][stage2_val_SSIM 0.8298378][stage2_val_PSNR 20.3789443][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01170910988003]
[2023-05-31 07:33:17.717727] [Steps   73200][G 2.2447691][G_critic 1.8177482] [G_l_mse 0.0042702] [G_ch_loss 0.00000000000000][DP 1.0568371][DP_real 0.4984485][DP_gen -0.3173760][DP_penalty 0.3135732][DS 1.2168231][DS_real 0.0393368][DS_gen -0.3517174][DS_penalty 0.0234633]
Steps 73201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 07:33:17.729690] [Steps   73200][fid_score 5.5340962]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.86s/it]100%|██████████| 1/1 [00:02<00:00,  2.86s/it][2023-05-31 08:50:38.136353] [Steps   73400][stage2_val_SSIM 0.8307281][stage2_val_PSNR 20.0871698][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01232338976115]
[2023-05-31 08:51:01.141791] [Steps   73400][G 1.9778154][G_critic 1.6294969] [G_l_mse 0.0034779] [G_ch_loss 0.00053067010595][DP 1.0471780][DP_real 0.4385635][DP_gen -0.3433552][DP_penalty 0.0493286][DS 1.3527254][DS_real 0.1932411][DS_gen 0.0880604][DS_penalty 0.0163623]
Steps 73401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 08:51:01.153883] [Steps   73400][fid_score 5.1037636]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.90s/it]100%|██████████| 1/1 [00:02<00:00,  2.90s/it][2023-05-31 10:07:36.324844] [Steps   73600][stage2_val_SSIM 0.8231251][stage2_val_PSNR 20.2858138][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01183611527085]
[2023-05-31 10:07:59.125503] [Steps   73600][G 1.9846392][G_critic 1.6635716] [G_l_mse 0.0032107] [G_ch_loss 0.00000000000000][DP 1.0791140][DP_real 0.4913203][DP_gen -0.2369653][DP_penalty 0.0595871][DS 1.3286972][DS_real 0.0697483][DS_gen -0.0563049][DS_penalty 0.0398133]
Steps 73601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 10:07:59.136762] [Steps   73600][fid_score 5.8435815]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.84s/it]100%|██████████| 1/1 [00:02<00:00,  2.84s/it][2023-05-31 11:24:25.723788] [Steps   73800][stage2_val_SSIM 0.8314481][stage2_val_PSNR 20.2371344][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01163087785244]
[2023-05-31 11:24:48.441224] [Steps   73800][G 2.3181169][G_critic 1.8514968] [G_l_mse 0.0046662] [G_ch_loss 0.00000000000000][DP 0.9579463][DP_real 0.5123414][DP_gen -0.5587176][DP_penalty 0.1266379][DS 1.3538668][DS_real 0.0405971][DS_gen -0.0363051][DS_penalty 0.0164834]
Steps 73801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 11:24:48.450724] [Steps   73800][fid_score 5.7479663]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.82s/it]100%|██████████| 1/1 [00:02<00:00,  2.82s/it][2023-05-31 12:41:25.869148] [Steps   74000][stage2_val_SSIM 0.8283944][stage2_val_PSNR 20.1735218][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01248974725604]
[2023-05-31 12:41:48.464549] [Steps   74000][G 2.0519114][G_critic 1.6813496] [G_l_mse 0.0037056] [G_ch_loss 0.00000000000000][DP 1.1407949][DP_real 0.2302320][DP_gen -0.3514428][DP_penalty 0.1275365][DS 1.3187053][DS_real 0.1543584][DS_gen -0.0058019][DS_penalty 0.0187831]
Steps 74001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 12:41:48.474223] [Steps   74000][fid_score 5.0543743]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.82s/it]100%|██████████| 1/1 [00:02<00:00,  2.82s/it][2023-05-31 13:58:12.841417] [Steps   74200][stage2_val_SSIM 0.8300769][stage2_val_PSNR 19.9541917][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01253511290997]
[2023-05-31 13:58:35.549630] [Steps   74200][G 2.1182504][G_critic 1.7788335] [G_l_mse 0.0033942] [G_ch_loss 0.00000000000000][DP 1.0116241][DP_real 0.5292940][DP_gen -0.3572033][DP_penalty 0.0365023][DS 1.2619348][DS_real 0.1289218][DS_gen -0.1636768][DS_penalty 0.0398339]
Steps 74201 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 13:58:35.558984] [Steps   74200][fid_score 4.9487053]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.83s/it]100%|██████████| 1/1 [00:02<00:00,  2.83s/it][2023-05-31 15:15:10.012808] [Steps   74400][stage2_val_SSIM 0.8223052][stage2_val_PSNR 19.8062227][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01289862487465]
[2023-05-31 15:15:33.011829] [Steps   74400][G 2.0670927][G_critic 1.7276893] [G_l_mse 0.0033940] [G_ch_loss 0.00000000000000][DP 1.0610354][DP_real 0.3442169][DP_gen -0.4781496][DP_penalty 0.1029830][DS 1.3035614][DS_real -0.0511193][DS_gen -0.2500191][DS_penalty 0.0093612]
Steps 74401 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 15:15:33.022034] [Steps   74400][fid_score 7.3249339]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.80s/it]100%|██████████| 1/1 [00:02<00:00,  2.80s/it][2023-05-31 16:33:54.202569] [Steps   74600][stage2_val_SSIM 0.8283930][stage2_val_PSNR 19.9329804][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01240906678140]
[2023-05-31 16:34:16.913433] [Steps   74600][G 1.9998803][G_critic 1.6565177] [G_l_mse 0.0034336] [G_ch_loss 0.00000000000000][DP 1.0643594][DP_real 0.3456279][DP_gen -0.4176736][DP_penalty 0.0397857][DS 1.3446982][DS_real 0.0808434][DS_gen -0.0240150][DS_penalty 0.0419653]
Steps 74601 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 16:34:16.924232] [Steps   74600][fid_score 5.3079460]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.82s/it]100%|██████████| 1/1 [00:02<00:00,  2.82s/it][2023-05-31 17:50:50.528591] [Steps   74800][stage2_val_SSIM 0.8376278][stage2_val_PSNR 20.3805475][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01137417461723]
[2023-05-31 17:51:13.085321] [Steps   74800][G 1.8172321][G_critic 1.5136776] [G_l_mse 0.0030355] [G_ch_loss 0.00000000000000][DP 1.1271718][DP_real 0.3408064][DP_gen -0.2628861][DP_penalty 0.0847923][DS 1.3007092][DS_real 0.1475705][DS_gen -0.0374306][DS_penalty 0.0175721]
Steps 74801 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 17:51:13.098817] [Steps   74800][fid_score 4.8819625]

  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.87s/it]100%|██████████| 1/1 [00:02<00:00,  2.87s/it][2023-05-31 19:07:56.757940] [Steps   75000][stage2_val_SSIM 0.8307641][stage2_val_PSNR 19.9186556][stage2_val_ch_loss 0.0000000] [stage2_val_target_loss 0.01233148947358]
[2023-05-31 19:08:19.680954] [Steps   75000][G 2.2889059][G_critic 1.8843992] [G_l_mse 0.0040435] [G_ch_loss 0.00015818426618][DP 1.1987126][DP_real 0.0398513][DP_gen -0.4465100][DP_penalty 0.0870926][DS 1.2357882][DS_real 0.0323067][DS_gen -0.3566761][DS_penalty 0.0707651]
Steps 75001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 19:08:19.691466] [Steps   75000][fid_score 4.8817303]

get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f6c883d8c10>, (128, 128, 3))
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
generator的train1参数加载结果为<All keys matched successfully>
g_ema的train1参数加载结果为<All keys matched successfully>
[2023-05-31 20:03:13.635374] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-05-31 20:03:13.635428] Use G moving average: 0.9999778195362122
Steps 50001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-05-31 20:31:42.666740] [Steps   51000] [G 0.0007612]
Steps 51001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 20:59:38.187723] [Steps   52000] [G 0.0008964]
Steps 52001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 21:27:34.293915] [Steps   53000] [G 0.0006430]
Steps 53001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 21:55:31.241656] [Steps   54000] [G 0.0005614]
Steps 54001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 22:23:30.377800] [Steps   55000] [G 0.0007931]
Steps 55001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 22:51:28.851236] [Steps   56000] [G 0.0017292]
Steps 56001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 23:19:23.608711] [Steps   57000] [G 0.0010236]
Steps 57001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-05-31 23:47:19.585774] [Steps   58000] [G 0.0006874]
Steps 58001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 00:15:17.357517] [Steps   59000] [G 0.0012848]
Steps 59001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 00:43:11.932203] [Steps   60000] [G 0.0004226]
Steps 60001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 01:11:08.563289] [Steps   61000] [G 0.0011994]
Steps 61001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 01:39:06.746449] [Steps   62000] [G 0.0003832]
Steps 62001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 02:07:04.450645] [Steps   63000] [G 0.0005213]
Steps 63001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 02:35:00.907397] [Steps   64000] [G 0.0002937]
Steps 64001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 03:03:01.029909] [Steps   65000] [G 0.0012105]
Steps 65001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 03:30:57.069527] [Steps   66000] [G 0.0005782]
Steps 66001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 03:58:53.923203] [Steps   67000] [G 0.0015345]
Steps 67001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 04:26:52.056753] [Steps   68000] [G 0.0003561]
Steps 68001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 04:54:48.989125] [Steps   69000] [G 0.0006375]
Steps 69001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 05:22:43.184359] [Steps   70000] [G 0.0004410]
Steps 70001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 05:50:42.126494] [Steps   71000] [G 0.0003052]
Steps 71001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 06:18:36.714134] [Steps   72000] [G 0.0005229]
Steps 72001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 06:46:30.896509] [Steps   73000] [G 0.0003381]
Steps 73001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 07:14:28.634114] [Steps   74000] [G 0.0004595]
Steps 74001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 07:42:24.814611] [Steps   75000] [G 0.0003950]
Steps 75001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 08:10:21.573651] [Steps   76000] [G 0.0003497]
Steps 76001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
get_dataset(dataset=options['dataset'])为(<datasets.MyColorTransferImageDataset1 object at 0x7f3a4d486d50>, (128, 128, 3))
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
generator的train1参数加载结果为<All keys matched successfully>
g_ema的train1参数加载结果为<All keys matched successfully>
[2023-06-01 08:31:18.373381] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-01 08:31:18.373616] Use G moving average: 0.9999778195362122
Steps 76001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
lr为0.0001
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
[2023-06-01 08:59:25.969818] [Steps   77000] [G 0.0003685]
Steps 77001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 09:27:27.598663] [Steps   78000] [G 0.0006170]
Steps 78001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 09:55:35.100209] [Steps   79000] [G 0.0003574]
Steps 79001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 10:23:37.565467] [Steps   80000] [G 0.0002744]
Steps 80001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 10:51:44.434096] [Steps   81000] [G 0.0005027]
Steps 81001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 11:19:48.482315] [Steps   82000] [G 0.0015922]
Steps 82001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 11:47:51.409747] [Steps   83000] [G 0.0007775]
Steps 83001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 12:15:54.045758] [Steps   84000] [G 0.0004420]
Steps 84001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 12:43:57.164067] [Steps   85000] [G 0.0011578]
Steps 85001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 13:11:57.650036] [Steps   86000] [G 0.0002209]
Steps 86001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 13:39:58.761369] [Steps   87000] [G 0.0010094]
Steps 87001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 14:08:02.458824] [Steps   88000] [G 0.0002909]
Steps 88001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 14:36:04.725566] [Steps   89000] [G 0.0003871]
Steps 89001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 15:04:07.797201] [Steps   90000] [G 0.0001851]
Steps 90001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 15:32:13.227234] [Steps   91000] [G 0.0010983]
Steps 91001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 16:00:14.660206] [Steps   92000] [G 0.0004135]
Steps 92001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 16:28:20.055581] [Steps   93000] [G 0.0013940]
Steps 93001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 16:56:27.878731] [Steps   94000] [G 0.0002768]
Steps 94001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 17:24:32.290751] [Steps   95000] [G 0.0005239]
Steps 95001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 17:52:37.375447] [Steps   96000] [G 0.0003686]
Steps 96001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 18:20:45.525728] [Steps   97000] [G 0.0002133]
Steps 97001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 18:48:50.811061] [Steps   98000] [G 0.0004365]
Steps 98001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 19:16:53.078933] [Steps   99000] [G 0.0002629]
Steps 99001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
[2023-06-01 19:44:56.663897] [Steps  100000] [G 0.0004104]
Steps 100001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-05 13:54:29.504974] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-05 13:54:29.513459] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 75001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
读到的x形状为:torch.Size([4, 3, 224, 224])
读到的input为:torch.Size([4, 384])
读到的x形状为:torch.Size([4, 3, 224, 224])
读到的input为:torch.Size([4, 384])
读到的x形状为:torch.Size([4, 3, 224, 224])
读到的input为:torch.Size([4, 384])
Traceback (most recent call last):
  File "train_stylegan2.py", line 677, in <module>
    worker(P)
  File "train_stylegan2.py", line 667, in worker
    val_pair_loader=val_pair_loader,logger=logger)
  File "train_stylegan2.py", line 412, in train
    loss.backward()
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.77 GiB total capacity; 10.44 GiB already allocated; 10.75 MiB free; 10.59 GiB reserved in total by PyTorch)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-05 13:56:13.419735] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-05 13:56:13.419801] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 75001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-05 13:59:51.729595] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-05 13:59:51.729646] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 100001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
=> Loading checkpoint from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-05 14:00:27.585417] Checkpoint loaded from 'logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357'
[2023-06-05 14:00:27.585466] Use G moving average: 0.9999916822684258
G_lr为0.0001
DP_lr为0.0001
DS_lr为0.0001
Steps 100001 (logs/gan_dp/c10_style64/vitgan/aug_both_diffaug_bcr_R0.1_H1000_NoLazy_NoWarmup/5357)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/xsx/anaconda3/envs/xsx_dino2/lib/python3.7/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
